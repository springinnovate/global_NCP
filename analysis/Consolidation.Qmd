---
title: "Hotspot Extraction Pipeline"
subtitle: "v0.5.0"
author: "Jerónimo Rodríguez Escobar"
date: last-modified
date-format: "YYYY-MM-DD HH:mm zzz"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
  pdf:
    toc: true
    number-sections: true
    geometry: margin=1in
editor: source
---

```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
library(tidyr)
library(forcats)
library(scales)
library(RColorBrewer)
library(htmltools)
library(leaflet)
library(devtools)
library(reticulate)
library(exactextractr)
library(httpgd)
load_all()

# set venv for Python
use_virtualenv("/home/jeronimo/venvs/coastal_snap_env", required = TRUE)

# Centralize data roots (set GLOBAL_NCP_DATA in ~/.Renviron)
data_root <- data_dir()
interim_dir <- data_interim()
processed_dir <- data_processed()

processed_tables_dir <- file.path(processed_dir, "tables")
processed_hotspots_dir <- file.path(processed_dir, "hotspots")
processed_coastal_dir <- file.path(processed_dir, "coastal")

latest_interim <- function(pattern) {
  files <- list.files(interim_dir, pattern = pattern, full.names = TRUE)
  if (!length(files)) {
    stop("No interim files match pattern: ", pattern)
  }
  files[which.max(file.mtime(files))]
}

# Legacy locations (kept for tracking):
# inpath <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data"
# inpath <- "/home/jeronimo/OneDrive/PROJECTS/Global_NCP/Final_outputs"
inpath <- processed_dir
```

::: callout-warning
**TODO / cleanup (run heavy chunks only when inputs are ready)**

- Paths are centralized via `data_dir()`; keep `GLOBAL_NCP_DATA` set.
- Keep expensive consolidation chunks `eval: false` until the latest `10k_grid_synth_{serv,benef,coastal}_*.gpkg` files are in `interim/`.
- Use this file as the living pipeline record; promote stable text to README/methods.
- Maintain the AOO equal-area grid as canonical; use the 4326 clean copy for raster extraction, then wrap dateline for display/output hygiene.
:::

# Workflow Synthesis: Global Hotspots of Ecosystem Service Change

## Notes to lift into README / Methods

- Inputs: IUCN AOO 10 km equal-area grid (land-only, with admin/biome attributes), ES rasters (1992/2020), coastal protection points (rasterized), and beneficiary rasters.
- Data roots: raw data in `/home/jeronimo/data/global_ncp/raw`; synth outputs in `/home/jeronimo/data/global_ncp/interim`; consolidated outputs and hotspots in `/home/jeronimo/data/global_ncp/processed`.
- Steps: rasterize coastal points → extract zonal stats (mean/sum per service/year via `summary_pipeline_landgrid.py` in Docker) → wrap dateline + move `10k_grid_synth_*` to `interim/` → consolidate (services+beneficiaries+coastal) → compute abs/pct change → hotspot extraction → KS analysis.
- Coastal protection: summarize Rt and Rt_ratio per grid cell (optional Rt_serv_ch for comparison).
- Outputs: `10k_grid_synth_*` in interim; consolidated `10k_change_calc.gpkg` in processed (intermediate `10k_grid_ES_change_benef.gpkg` is optional); hotspot GPKGs and tables.
- TODO: add a `keep_fields` option to the summary pipeline YAMLs so we can preserve only needed grid attributes (instead of all).

## Data Inputs

Global 10 km grid covering terrestrial areas worldwide (polygon-based template).

### Canonical 10 km grid (IUCN AOO, land-only)

We do **not** generate the grid from scratch. We use the published IUCN
Area of Occupancy (AOO) 10×10 km equal-area grid and subset it to land
by intersecting with the correspondence polygons. We keep the full grid
cells that touch land (no clipping) and append attributes used for
subregional aggregation (country/continent/region/biome).

```{r prep-grid-aoo-land, eval=FALSE}
library(sf)
library(dplyr)

grid_raw <- st_read(data_vectors("AOOGrid_10x10kmShp/AOOGrid_10x10km.shp"))
land_polys <- st_read(
  data_vectors("cartographic_ee_ee_r264_correspondence.gpkg"),
  layer = "ee_r264_correspondence"
)

if (st_crs(grid_raw) != st_crs(land_polys)) {
  land_polys <- st_transform(land_polys, st_crs(grid_raw))
}

# Keep grid cells that intersect any land polygon
grid_land <- st_filter(grid_raw, land_polys, .predicate = st_intersects)

# Attach attributes for aggregation (spatial join or fid-based join)
land_attrs <- land_polys |>
  dplyr::select(
    iso3, nev_name, continent, region_un, region_wb,
    income_grp, WWF_biome
  )
grid_land <- st_join(grid_land, land_attrs, join = st_intersects, left = TRUE)

st_write(
  grid_land,
  data_vectors("AOOGrid_10x10km_land.gpkg"),
  delete_layer = TRUE
)
```

```{python}
#| label: make-grid-valid-shapely
#| eval: false
#| message: false
#| warning: false

# Optional: create a Shapely-validated grid copy to avoid GEOS errors in Python.
# Requires geopandas/shapely in the Quarto Python environment.
import os
import geopandas as gpd

data_root = os.environ.get("GLOBAL_NCP_DATA", "")
grid_in = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land.gpkg")
grid_out = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land_shpvalid.gpkg")

gdf = gpd.read_file(grid_in)
gdf["geometry"] = gdf.geometry.make_valid()
gdf = gdf[gdf.geometry.notna() & gdf.is_valid]

gdf.to_file(grid_out, driver="GPKG")
print(f"wrote {len(gdf):,} features -> {grid_out}")
```

```{python}
#| label: grid-4326-clean
#| eval: false
#| message: false
#| warning: false

# Create a 4326 version for raster extraction (avoids to_crs errors at runtime).
# This keeps the same cells, only expressed in EPSG:4326.
import geopandas as gpd
from shapely.ops import transform
from pyproj import Transformer

grid_in = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land_shpvalid.gpkg")
grid_out = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land_4326_clean.gpkg")

gdf = gpd.read_file(grid_in)
transformer = Transformer.from_crs(gdf.crs, "EPSG:4326", always_xy=True)

bad = 0
new_geoms = []
for geom in gdf.geometry:
    try:
        g = transform(transformer.transform, geom)
        if g.is_empty or not g.is_valid:
            bad += 1
            new_geoms.append(None)
        else:
            new_geoms.append(g)
    except Exception:
        bad += 1
        new_geoms.append(None)

gdf["geometry"] = new_geoms
gdf = gdf[gdf.geometry.notna()]
gdf = gdf.set_crs("EPSG:4326", allow_override=True)
gdf.to_file(grid_out, driver="GPKG")
print(f"dropped {bad:,} invalid geometries; wrote -> {grid_out}")
```

**Canonical grids used downstream**

- **Equal-area canonical grid**: `AOOGrid_10x10km_land.gpkg`
- **Extraction grid (EPSG:4326)**: `AOOGrid_10x10km_land_4326_clean.gpkg`

The 4326 file is only for raster extraction; the cell geometry is the same
10×10 km equal-area grid expressed in a different CRS.

Coastal protection model outputs (1992 and 2020) originally as points spaced \~500 m along shorelines.

Global raster datasets for nitrogen retention/export/ratio, sediment retention/export/USLE, pollination, and nature access (1992 and 2020).

Country polygon dataset for spatial aggregation and attribution.

## Data Processing Pipeline

### Zonal Statistics for Gridded ES Services

Geosharding-based pipeline to extract zonal statistics (mean or sum, depending on the service) from modeled global ES raster layers for two years (1992-2020) to the 10 km grid (or any poygon vector file). Values are labeled and appended as new columns in an output vector dataset (gpkg)

### Coastal Protection Aggregation

This dataset comes originally as a vector included in this analysis three key variables for each year: *convert into math notation*

-   Rt: Actual Risk Level
-   Rt_nohab: Potential Risk Level if no ecosystem (expected to remain constant over time, but necessary to calculate the next ones)
-   Rt_nohab - Rt = Rt_service: This difference is the actual value of the service provided (the potential-the observed Risk) = Provided service
-   (Rt_nohab-Rt)/Rt: Coastal risk reduction ratio

Computed mean and max per grid cell for each coastal protection metric.

### Bi-temporal Change Calculation

Applied compute_change() function to derive absolute and percentage changes for all variables, using consistent naming conventions (\_abs_chg, \_pct_chg). Percentage change uses the earliest year as the baseline, so zero baselines yield Inf/NaN and should be handled downstream.

### Reshaping Data for Hotspot Detection

Converted the dataset to long format (plt_long) with columns: fid, c_fid, service, and pct_chg.

### Hotspot Identification

Defined loss/damage services and gain services.

Identified hotspots as grid cells in the bottom 5% (losses) or top 5% (gains) per service. The developed function can extract any threshold, and the inverses. It is also posssible to set the variables for which an increase is an improvement (goods) to which an increase is a deterioration (damage), and set groups of variables, aka *combos* to count the number of variables for which a cell is a hotspot. ( Binary columns explicitly indicating whether a cell is a hotpot for each variable analized is included, as well as columns indicating the type of hotpot (damage/service loss).

### Hotspot Grouping into Service Combos

Created combos for aggregated hotspot counts:

combo_1 = c("Nature_Access","Pollination","N_export", "Sed_export", "C_Risk"), combo_2 = c("Nature_Access","Pollination", "N_Ret_Ratio", "Sed_Ret_Ratio", "C_Risk_Red_Ratio")

Calculated counts per combo and total hotspot count.

### Final Integration and Output

Generated binary columns (0/1) per service for hotspot presence.

Joined hotspot attributes back to the 10 km grid sf object.

Produced final geopackage file with:

Per-service hotspot flags.

Total hotspot count.

Counts for each combo

A column listing all hotspot services per grid cell.

### Outputs

Final gpkg with the summary stats for each year for all variables plus socio economic data. Final gpkg object containing all hotspot-related attributes for each 10 km grid cell.

Map outputs prepared in Qis (one for the hotspots for each variable) and for the *combos*

Flexible structure allows filtering for losses only, per-service mapping, or aggregated combo-based analyses.

# Data processing

## Consolidate Synthesis Data

There is an issue with the naming of the variables that has been giving me a lot of trouble. I will need to clean and review this, remove remaining unnecesary data. Doing this based on the index is definitively not a good think, each change/new dataset shifts the whole thing and it is hard to follow effectively.

Assemble final gridded Change/beneficiary data set

```{r}
#| eval: false
#| include: false
# Legacy locations (keep for tracking):
# inpath <- "/home/jeronimo/OneDrive/PROJECTS/Global_NCP/Final_outputs"
# inpath <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data"
source(here::here("R", "paths.R"))
interim_dir <- data_interim()
processed_dir <- data_processed()
processed_coastal_dir <- file.path(processed_dir, "coastal")

latest_interim <- function(pattern) {
  files <- list.files(interim_dir, pattern = pattern, full.names = TRUE)
  if (!length(files)) stop("No interim files match pattern: ", pattern)
  files[which.max(file.mtime(files))]
}

inpath <- processed_dir
# Optional: export name indexes to CSV
write_index <- FALSE
write_base_synth <- TRUE

# New workflow: means + sums are in the same synth output.
serv_path  <- latest_interim("^10k_grid_services(.*)?\\.gpkg$")
ben_path   <- latest_interim("^10k_grid_benef(.*)?\\.gpkg$")
coast_path <- latest_interim("^10k_grid_coastal(.*)?\\.gpkg$")


S <- st_read(serv_path)
B <- st_read(ben_path)
CP <- st_read(coast_path)

if (!"fid" %in% names(S)) S$fid <- seq_len(nrow(S))
if (!"fid" %in% names(B)) B$fid <- seq_len(nrow(B))
if (!"fid" %in% names(CP)) CP$fid <- seq_len(nrow(CP))

S_tbl <- st_drop_geometry(S)
B_tbl <- st_drop_geometry(B)
CP_tbl <- st_drop_geometry(CP)

sf_f <- S |>
  left_join(B_tbl, by = "fid") |>
  left_join(CP_tbl, by = "fid")

# Optional: index fields for audit/debugging
if (isTRUE(write_index)) {
  write.csv(cbind(names(B_tbl), names(B_tbl)), file = file.path(processed_tables_dir, "benef.csv"))
  write.csv(cbind(names(S), names(S)), file = file.path(processed_tables_dir, "serv.csv"))
  write.csv(cbind(names(CP_tbl), names(CP_tbl)), file = file.path(processed_tables_dir, "cp.csv"))
}

# `sf_f` already contains services + beneficiaries + coastal summaries.
# Define name replacements per object. Had to do it manually, there is just too much variability to automatize, did it by hand, but this is the kind of thing an LLM helps to write easily. Still, there must be a smarter, more flexible way to do it (maybe the best approach is to keep a consistent naming convention for the modeled rasters, this has been a PIA since i started this work)

# Update 2. Include corrected Access Data
rename_list <- list(
  c(
    usle_1992_mean = "global_usle_marine_mod_ESA_1992_mean",
    usle_2020_mean = "global_usle_marine_mod_ESA_2020_mean",
    nature_access_1992_mean = "nature_access_lspop2019_ESA1992_mean",
    nature_access_2020_mean = "nature_access_lspop2019_ESA2020_mean",
    n_ret_ratio_1992_mean = "N_ret_ratio_1992_mean",
    n_ret_ratio_2020_mean = "N_ret_ratio_2020_mean",
    sed_ret_ratio_1992_mean = "Sed_ret_ratio_1992_mean",
    sed_ret_ratio_2020_mean = "Sed_ret_ratio_2020_mean",
    coastal_protection_Rt_1992_mean = "coastal_protection_Rt_1992_mean",
    coastal_protection_Rt_2020_mean = "coastal_protection_Rt_2020_mean",
    coastal_protection_Rt_nohab_all_1992_mean = "coastal_protection_Rt_nohab_all_1992_mean",
    coastal_protection_Rt_nohab_all_2020_mean = "coastal_protection_Rt_nohab_all_2020_mean",
    coastal_protection_Rt_ratio_1992_mean = "coastal_protection_Rt_ratio_1992_mean",
    coastal_protection_Rt_ratio_2020_mean = "coastal_protection_Rt_ratio_2020_mean"
  ),
  c(
    n_export_1992_sum = "global_n_export_tnc_esa1992_sum",
    n_export_2020_sum = "global_n_export_tnc_esa2020_sum",
    n_retention_1992_sum = "global_n_retention_ESAmar_1992_fertilizer_sum",
    n_retention_2020_sum = "global_n_retention_ESAmar_2020_fertilizer_sum",
    sed_export_1992_sum = "global_sed_export_marine_mod_ESA_1992_sum",
    sed_export_2020_sum = "global_sed_export_marine_mod_ESA_2020_sum",
    pollination_1992_sum = "realized_polllination_on_ag_ESA1992_sum",
    pollination_2020_sum = "realized_polllination_on_ag_ESA2020_sum"
  )
)
rename_map <- c(rename_list[[1]], rename_list[[2]])
sf_f <- dplyr::rename(sf_f, !!!rename_map)

# Optional: save the base synthesis (raw 1992/2020 + beneficiaries + coastal)
if (isTRUE(write_base_synth)) {
  st_write(sf_f, file.path(processed_dir, "10k_grid_synth_all.gpkg"), append = FALSE)
}

 # Services-only synth lives in interim (10k_grid_services.gpkg) if needed for QA.

```

### next
```{r compute-bitemporal-change}
#| eval: false
#| include: false
# Assumes `sf_f` is already created in the consolidation chunk above.
# If not, load the latest base synthesis from processed.
# This keeps the change calculation resumable without rerunning consolidation.
if (!exists("sf_f")) {
  synth_path <- file.path(processed_dir, "10k_grid_synth_all.gpkg")
  sf_f <- st_read(synth_path, quiet = TRUE)
}
# Calculate bi-temporal change in % and absolute terms.
# pct_chg uses the earliest year as baseline: ((t1 - t0) / t0) * 100.
# Division by zero yields Inf/NaN; handle downstream if needed.
sf_f <- compute_change(
  sf_f,
  suffix = c("_sum", "_mean"),
  change_type = "both",
  drop_columns = FALSE
)

# NaN are divisions by 0, NA is when values are NA
write_full_change <- TRUE

# Expanded change synthesis (raw + change) for QA/validation
if (isTRUE(write_full_change)) {
  st_write(
    sf_f,
    file.path(processed_dir, "10k_grid_ES_change_benef.gpkg"),
    layer = "10k_grid_ES_change_benef",
    delete_dsn = TRUE
  )
}


```

```{r filter-canonical-change}
#| eval: false
#| include: false
# Create a trimmed, canonical change dataset for downstream use.
# Keep the full QA dataset in 10k_grid_ES_change_benef.gpkg.

if (!"fid" %in% names(sf_f)) {
  sf_f$fid <- seq_len(nrow(sf_f))
}

keep_fixed <- c(
  "fid", "iso3", "continent", "region_un", "region_wb",
  "income_grp", "subregion", "WWF_biome", "BIOME"
)

benef_pattern <- "^(GHS_|GlobPOP_|hdi_|rast_gdpTot_|rast_adm1_gini_|fields_mehrabi_)"

# Drop raw/base-year service columns; we only keep change metrics here.
year_cols <- grep("1992|2020", names(sf_f), value = TRUE)
benef_cols <- grep(benef_pattern, names(sf_f), value = TRUE)
drop_year_cols <- setdiff(year_cols, benef_cols)
if (length(drop_year_cols)) {
  sf_f <- sf_f |> dplyr::select(-any_of(drop_year_cols))
}

# Canonical services (mean vs sum encoded in rename_list).
svc_cols <- names(c(rename_list[[1]], rename_list[[2]]))
svc_raw <- unname(c(rename_list[[1]], rename_list[[2]]))
svc_base <- unique(stringr::str_replace(svc_cols, "_(1992|2020)_", "_"))
svc_raw_base <- unique(stringr::str_replace(svc_raw, "_?(1992|2020)_?", "_"))
svc_raw_base <- stringr::str_replace_all(svc_raw_base, "__+", "_")
svc_raw_base <- stringr::str_replace(svc_raw_base, "_$", "")

# Normalize raw service prefixes to canonical bases before filtering.
svc_map <- stats::setNames(svc_base, svc_raw_base)
for (i in seq_along(svc_map)) {
  raw_base <- names(svc_map)[i]
  canon_base <- unname(svc_map[i])
  sf_f <- dplyr::rename_with(
    sf_f,
    ~ sub(paste0("^", raw_base, "_"), paste0(canon_base, "_"), .x)
  )
}

# Start with all change columns; we can narrow to canonical services.
chg_cols <- grep("_(abs|pct)_chg$", names(sf_f), value = TRUE)
if (!length(chg_cols)) {
  message("No change columns found in sf_f; reloading full dataset.")
  sf_f <- st_read(file.path(processed_dir, "10k_grid_ES_change_benef.gpkg"), quiet = TRUE)
  if (!"fid" %in% names(sf_f)) {
    sf_f$fid <- seq_len(nrow(sf_f))
  }
  chg_cols <- grep("_(abs|pct)_chg$", names(sf_f), value = TRUE)
}

coastal_keep <- c(
  "coastal_protection_Rt",
  "coastal_protection_Rt_nohab_all",
  "coastal_protection_Rt_ratio"
)
coastal_mean_change <- grep(
  paste0("^(", paste(coastal_keep, collapse = "|"), ")_mean_(abs|pct)_chg$"),
  names(sf_f),
  value = TRUE
)
canonical_change <- unique(c(
  paste0(svc_base, "_abs_chg"),
  paste0(svc_base, "_pct_chg"),
  coastal_mean_change
))
chg_cols <- intersect(chg_cols, canonical_change)

keep_cols <- unique(c(
  keep_fixed,
  grep(benef_pattern, names(sf_f), value = TRUE),
  chg_cols
))
keep_cols <- setdiff(keep_cols, c("RasterVal", "id_2"))

svc_meta <- tibble::tibble(
  col = svc_cols,
  stat = stringr::str_match(svc_cols, "_(mean|sum)$")[, 2],
  base = stringr::str_replace(svc_cols, "_(1992|2020)_", "_")
) |>
  dplyr::distinct(base, stat)
message("Service stats kept (canonical):")
print(svc_meta)
message("Change columns kept (n = ", length(chg_cols), "):")
print(head(chg_cols, 25))

sf_canon <- sf_f |> dplyr::select(any_of(keep_cols))

grid_attrs <- st_read(
  file.path(data_interim(), "..", "vector_basedata", "AOOGrid_10x10km_land_4326_clean.gpkg"),
  quiet = TRUE
) |>
  st_drop_geometry() |>
  dplyr::select(any_of(keep_fixed))

if (!"fid" %in% names(grid_attrs)) {
  grid_attrs$fid <- seq_len(nrow(grid_attrs))
}

if (!all(keep_fixed %in% names(sf_canon))) {
  sf_canon <- sf_canon |>
    left_join(grid_attrs, by = "fid")
}

# Re-apply selection to ensure only intended columns remain after the join
sf_canon <- sf_canon |> dplyr::select(any_of(keep_cols))

# Clean service change names only (drop _mean/_sum for canonical services).
rename_change <- function(nm) {
  stringr::str_replace(nm, "_(mean|sum)_(abs|pct)_chg$", "_\\2_chg")
}
svc_change <- c(
  paste0(svc_base, "_abs_chg"),
  paste0(svc_base, "_pct_chg")
)
svc_change <- intersect(svc_change, names(sf_canon))
if (length(svc_change)) {
  sf_canon <- dplyr::rename_with(sf_canon, rename_change, .cols = all_of(svc_change))
}

st_write(
  sf_canon,
  file.path(processed_dir, "10k_change_calc.gpkg"),
  layer = "10k_change_calc",
  delete_dsn = TRUE
)
```

Note: coastal protection shows up twice in this document. The earlier `c_prot` block is only a field inventory; the actual join and aggregation happen in the sections below.

## Include Coastal Protection (final)

This whole part need to be fixed at some point. The key part is to get the coastal with the right fid so we can join and add this correclty wqhen we have more data. Right now keep this hewre

```{r include coastal points}
#| eval: false
#| include: false
# Add ratios to the joined coastal points (1992/2020) before rasterization.

cp_joined_path <- file.path(data_interim(), "c_protection_1992_2020_joined.gpkg")
cp_points <- st_read(cp_joined_path, quiet = TRUE)

missing_cols <- setdiff(
  c("Rt_1992", "Rt_nohab_all_1992", "Rt_2020", "Rt_nohab_all_2020"),
  names(cp_points)
)
if (length(missing_cols)) {
  stop("Missing coastal columns: ", paste(missing_cols, collapse = ", "))
}

cp_points <- cp_points |>
  dplyr::mutate(
    Rt_service_1992 = .data$Rt_nohab_all_1992 - .data$Rt_1992,
    Rt_service_2020 = .data$Rt_nohab_all_2020 - .data$Rt_2020,
    Rt_ratio_1992 = dplyr::if_else(
      .data$Rt_nohab_all_1992 != 0,
      .data$Rt_service_1992 / .data$Rt_nohab_all_1992,
      NA_real_
    ),
    Rt_ratio_2020 = dplyr::if_else(
      .data$Rt_nohab_all_2020 != 0,
      .data$Rt_service_2020 / .data$Rt_nohab_all_2020,
      NA_real_
    )
  )

st_write(cp_points, cp_joined_path, append = FALSE)
```

### Coastal protection pipeline (current)

Coastal protection now follows a three-step sequence. Keep this order to avoid
stale joins and to ensure the ratios are computed before rasterization:

1. Join 1992/2020 points → `interim/c_protection_1992_2020_joined.gpkg`
   (via `Python_scripts/coastal_protection_join.py`).
2. Add Rt_service and Rt_ratio columns (see chunk above).
3. Rasterize the joined points (with ratios) and run the summary pipeline to
   produce the coastal synthesis GPKG.

```{r coastal-protection-runbook}
#| eval: false
#| include: false

# 1) Join points (creates interim/c_protection_1992_2020_joined.gpkg)
# python Python_scripts/coastal_protection_join.py

# 2) Add ratios (run the chunk above)

# 3) Rasterize the updated points
# COASTAL_INCLUDE_CH=1 python Python_scripts/rasterize_coastal.py

# 4) Summarize to the 10 km grid (Docker)
# python summary_pipeline_landgrid.py --data-root /data analysis_configs/c_protection_synth.yaml

# 5) Wrap dateline + move to interim (see README section)
# OUT_DIR=/home/jeronimo/data/global_ncp/interim
# TS=$(date +%Y%m%d_%H%M%S)
# COAST_SRC=/home/jeronimo/projects/global_NCP/summary_pipeline_workspace/<coastal_file>.gpkg
# ogr2ogr -wrapdateline -datelineoffset 180 \
#   "$OUT_DIR/10k_grid_synth_coastal_${TS}.gpkg" "$COAST_SRC"
```

## Add Updated Access Data

No need to run again, already updated the Access Data.

```{r add access}
#| eval: false
#| include: false

# Deprecated: access corrections are handled in the current services_slim outputs.
# Keeping this chunk as historical reference only.

```

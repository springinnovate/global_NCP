---
title: "Hotspot Extraction"
subtitle: "v{{< meta analysis_version >}}"
author: "Jeronimo Rodríguez"
date: today
date-format: iso
analysis_version: "v0.4.1"  
format:
  html:
    toc: true
    number-sections: true
    code-fold: show
editor: visual

params:
  analysis_version: "v0.4.1"

---

```{r setup, message=FALSE, warning=FALSE}
#| label: setup
#| message: false
#| warning: false

# Core tidy + spatial
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(forcats)
library(ggplot2)
library(scales)
library(sf)
library(terra)
library(exactextractr)

# Plot helpers
library(viridisLite)
library(ggnewscale)
library(ragg)

# Dev/project helpers
library(devtools)   # for load_all()

# Optional viewers (enable only if you use them)
# library(httpgd)
# library(leaflet)
# library(htmltools)

# ---- Global options / performance ---------------------------------------
options(dplyr.summarise.inform = FALSE)
sf::sf_use_s2(TRUE)
Sys.setenv(GDAL_NUM_THREADS = "ALL_CPUS", PROJ_NETWORK = "ON")
terraOptions(tempdir = file.path(tempdir(), "terra_tmp"))

set.seed(1)

# ---- Project wiring ------------------------------------------------------
# Load package-style functions from R/ (if this is a package-ish repo)
devtools::load_all(quiet = TRUE)

# Paths helper (set GLOBAL_NCP_DATA in ~/.Renviron first)
source("../R/paths.R")

# ---- Python (enable when needed) ----------------------------------------
# library(reticulate)
# use_virtualenv("/home/jeronimo/venvs/coastal_snap_env", required = TRUE)

```



```{r}
# Prints version/time/git/data-root info so renders are traceable (no heavy compute).

#| label: run-metadata
#| echo: false
#| message: false
#| warning: false

analysis_version <- tryCatch(params$analysis_version, error = function(e) NULL)
if (is.null(analysis_version) || is.na(analysis_version)) {
  analysis_version <- Sys.getenv("ANALYSIS_VERSION", unset = "dev")
}

# Git info (robust to non-git folders)
git_branch <- tryCatch(
  system2("git", c("rev-parse", "--abbrev-ref", "HEAD"), stdout = TRUE),
  error = function(e) NA_character_
)
git_commit <- tryCatch(
  system2("git", c("rev-parse", "--short", "HEAD"), stdout = TRUE),
  error = function(e) NA_character_
)

# Data root from your paths helper, with fallbacks
data_root <- tryCatch(data_dir(), error = function(e) NULL)
if (is.null(data_root)) {
  data_root <- Sys.getenv("GLOBAL_NCP_DATA", unset = "")
}

cat(paste0(
  "::: callout-note\n",
  "**Run metadata**\n\n",
  "- Analysis version: ", analysis_version, "\n",
  "- Rendered: ", format(Sys.time(), "%Y-%m-%d %H:%M %Z"), "\n",
  "- Git: ", if (!is.na(git_branch)) git_branch else "NA", " @ ",
                 if (!is.na(git_commit)) git_commit else "NA", "\n",
  "- Data root: ", if (nzchar(data_root)) data_root else "unset", "\n",
  ":::"
))
```

## Regional attributes for sub-regional analysis (countries & biomes)

We enrich the 10 km grid (`sf_f`) with country and WWF biome IDs so we can aggregate and compare change by subregions (World Bank region, income group, continent, UN region, and biome). We use a point-on-surface join to avoid sliver/overlap issues, keep only the needed fields, and write a single enriched GPKG for downstream grouping, hotspot extraction, and plotting.

::: callout-tip
**What this step does**

- Reads country and biome layers from `vectors/`.
- Joins them to the 10 km grid via point-on-surface.
- Writes `processed/10k_change_calc.gpkg` for downstream analysis (pivoting, hotspots, plots).
**Inputs:** `sf_f` grid; `vectors/cartographic_ee_ee_r264_correspondence.gpkg`; `vectors/Biome.gpkg`  
**Output:** `processed/10k_change_calc.gpkg` (grid + regional attributes)

**Why point-on-surface?** It’s robust for odd cell shapes and avoids polygon–polygon sliver issues.

**When to bump the version?** If the joined attributes change schema/meaning (e.g., new grouping columns), bump **MINOR**; if file name/structure changes in a breaking way, bump **MAJOR**.
:::


```{r}
#| label: attach-region-attrs
#| eval: false
#| echo: true
# Purpose:
# Attach country & biome attributes to the 10 km grid (sf_f) so we can group by
# income_grp, region_wb, BIOME, etc. This writes a processed GPKG for downstream use.

# Preconditions:
# - sf_f (grid polygons) in memory with at least: fid, c_fid, geometry
# - paths.R available and data_dir() points to your data root (GLOBAL_NCP_DATA)

stopifnot(inherits(sf_f, "sf"))
stopifnot(all(c("fid","c_fid") %in% names(sf_f)))

# Inputs
ct_path  <- file.path(data_dir(), "vectors", "cartographic_ee_ee_r264_correspondence.gpkg")
bio_path <- file.path(data_dir(), "vectors", "Biome.gpkg")
stopifnot(file.exists(ct_path), file.exists(bio_path))

ct <- sf::st_read(ct_path, quiet = TRUE) |>
  dplyr::select(id, ee_r264_name, iso3, continent, income_grp, region_un, region_wb, subregion)

biomes <- sf::st_read(bio_path, quiet = TRUE) |>
  dplyr::select(BIOME, WWF_biome)

# CRS harmonization (note the != instead of ! ==)
crs_grid <- sf::st_crs(sf_f)
if (sf::st_crs(ct)     != crs_grid) ct     <- sf::st_transform(ct, crs_grid)
if (sf::st_crs(biomes) != crs_grid) biomes <- sf::st_transform(biomes, crs_grid)

# Lightweight template (keeps geometry)
sf_template <- sf_f |>
  dplyr::select(fid, c_fid)

# Representative point per cell; more robust than centroids
pts <- sf::st_point_on_surface(sf_template)

# Spatial joins
pts_ct    <- sf::st_join(pts, ct,     left = TRUE)
pts_biome <- sf::st_join(pts, biomes, left = TRUE)

# Drop geometry and merge attrs by fid
pts_ctdf <- sf::st_drop_geometry(pts_ct)
pts_bmdf <- sf::st_drop_geometry(pts_biome)

out_attrs <- pts_ctdf |>
  dplyr::left_join(pts_bmdf, by = "fid", suffix = c("", "_biome"))

# Enforce uniqueness by fid (defensive)
if (anyDuplicated(out_attrs$fid)) {
  out_attrs <- out_attrs |>
    dplyr::arrange(fid) |>
    dplyr::distinct(fid, .keep_all = TRUE)
}

# Bring attributes back to the grid
sf_f_joined <- sf_f |>
  dplyr::left_join(out_attrs, by = "fid")

stopifnot(nrow(sf_f_joined) == nrow(sf_f), inherits(sf_f_joined, "sf"))

# Output (idempotent write)
out_gpkg <- file.path(data_dir(), "processed", "10k_change_calc.gpkg")
dir.create(dirname(out_gpkg), recursive = TRUE, showWarnings = FALSE)
if (file.exists(out_gpkg)) file.remove(out_gpkg)
sf::st_write(sf_f_joined, out_gpkg, quiet = TRUE)

# TODO (run later): quick QA tables
# dplyr::count(sf_f_joined, is.na(iso3))
# dplyr::count(sf_f_joined, is.na(BIOME))
# dplyr::count(sf_f_joined, income_grp, sort = TRUE)

```


## Prepare data for analysis: reshape to “long” service × cell

Start by turning the wide grid table (one row per 10 km cell with many *_abs_chg / *_pct_chg columns) into an **analysis-ready long format**: one row per **cell × service** with two value columns: `abs_chg` and `pct_chg`. This makes it easy to rank, filter, and facet by service in later hotspot steps.

**What this chunk does**
1) Reads the processed grid from `processed/10k_change_calc.gpkg` and ensures a unique `fid`.  
2) Splits “ID/grouping” columns from change columns.  
3) Pivots *_abs_chg / *_pct_chg to long, then back to wide as `abs_chg` / `pct_chg` per service.  
4) Cleans obvious issues (drops `Inf`/`NA` where appropriate) so plots and tests won’t choke.  
5) Applies a human-readable service label mapping (e.g., `n_export → N_export`).  
6) Sets a canonical facet order (`svc_order`) so plots are consistent across the report.

**Inputs:** `processed/10k_change_calc.gpkg` (contains `fid`, `c_fid`, service change fields, and sub-regional tags like `region_wb`, `income_grp`, `BIOME`, etc.).  
**Output:** `plt_long` (tidy tibble) with columns  
`fid, c_fid, service, abs_chg, pct_chg, <grouping/socio vars>`.

::: callout-tip
**Why long format?**  
Ranking, percentile cuts, ECDFs, and faceted plots are all simpler and faster when each **service** is a row attribute rather than a separate column.
:::



```{r pivot}
#| eval: true
#| include: true
# ---- Produce plt_long once (skip if already present) ---------------- ----

if (!exists("plt_long", inherits = TRUE)) {

gpkg <- file.path(data_dir(), "processed", "10k_change_calc.gpkg")
stopifnot(file.exists(gpkg))

sf_f <- sf::st_read(gpkg, quiet = TRUE)

# Ensure fid exists (idempotent)

if (!"fid" %in% names(sf_f)) {
sf_f$fid <- seq_len(nrow(sf_f))
}

plt <- sf::st_drop_geometry(sf_f)

# IDs + change columns

id_cols  <- c("fid", "c_fid")
chg_cols <- grep("_(abs|pct)_chg$", names(plt), value = TRUE)

# Socio/grouping columns = everything else

socio_vars <- setdiff(names(plt), c(id_cols, chg_cols))

# Long -> Wide (abs_chg / pct_chg)

plt_long <- plt |>
tidyr::pivot_longer(
cols = tidyselect::all_of(chg_cols),
names_to = c("service", "chg_type"),
names_pattern = "^(.*)_(abs|pct)_chg$",
values_to = "chg_value"
) |>
dplyr::mutate(service = stringr::str_remove(service, "_mean$")) |> 
tidyr::pivot_wider(
names_from  = chg_type,
values_from = chg_value,
names_vary  = "slowest"
) |> # clean column names
dplyr::rename(abs_chg = abs, pct_chg = pct) |>
dplyr::select(fid, c_fid, service, abs_chg, pct_chg, dplyr::any_of(socio_vars)) |>
dplyr::filter(!is.na(c_fid)) |> # remove cells outside any country.
dplyr::filter(!(is.infinite(abs_chg) | is.infinite(pct_chg))) |>
dplyr::filter(!is.na(abs_chg) | !is.na(pct_chg))


# TODO: externalize label mapping (and loss/gain tags) to a config:
# analysis_configs/service_meta.csv with columns:
# raw,label,direction,pref_metric
# Load once and join, so we don’t hard-code names here.
service_lookup <- c(
sed_export     = "Sed_export",
n_export       = "N_export",
n_retention    = "N_retention",
nature_access  = "Nature_Access",
pollination    = "Pollination",
usle           = "USLE",
n_ret_ratio    = "N_Ret_Ratio",
sed_ret_ratio  = "Sed_Ret_Ratio",
Rt_ratio       = "C_Risk_Red_Ratio",
Rt             = "C_Risk",
Rt_service     = "C_Prot_service",
Rt_nohab       = "Rt_nohab"
)
plt_long <- plt_long |>
dplyr::mutate(service = dplyr::recode(service, !!!service_lookup, .default = service))


}# Canonical ordering (don’t drop anything here)

# Canonical facet order: your 8, then any extras
svc_order <- c(
  "C_Risk","N_export","Sed_export",
  "C_Risk_Red_Ratio","N_Ret_Ratio","Sed_Ret_Ratio",
  "Pollination","Nature_Access"
)

extras <- setdiff(unique(plt_long$service), svc_order)
plt_long <- plt_long |>
  mutate(service = factor(service, levels = c(svc_order, sort(extras))))

# after creating plt_long from sf_f ...
grid_sf <- sf_f %>% dplyr::select(fid, c_fid)  # geometry is "sticky" and kept
rm(plt, sf_f); gc()                             # free the big attribute table + full sf

```


### Sanity checks on `plt_long`

Before ranking/thresholding, a lightweight QA helps catch silent problems (e.g., an empty service, unexpected sparsity, or leftover `Inf`s):

- Confirm `plt_long` exists and peek the structure.
- Count rows per `service` to spot outliers in coverage (e.g., a service that only exists in a few countries).

#### Pivot Data

```{r sanity-check pivot}
# ---- Quick sanity --------------------------------------------------------

stopifnot(exists("plt_long"))

# Peek a few rows & structure

dplyr::glimpse(head(plt_long, 5), width = 80)

# Counts per service (helps spot weird sparsity)

plt_long |>
dplyr::count(service, name = "rows") |>
dplyr::arrange(dplyr::desc(rows)) |>
print(n = 50)
```
## Hotspot extraction and exports (global + subregional)

We identify per-service hotspots using a 5% percentile rule and **direction vectors**:
- `loss_services` (e.g., Nature_Access, Pollination, N/Sed_Ret_Ratio, C_Risk_Red_Ratio) → we flag the **lowest** values;
- `gain_services` (Sed_export, N_export, C_Risk) → we flag the **highest** values.

We run this **once globally** and then **once per subregion** (World Bank region, income group, continent, UN region, WWF biome). For each run and for each metric (absolute and percent change) we write a compact **GPKG** containing only the hotspot cells, plus a CSV index:

- Output root: `processed/hotspots/`
  - `abs/global/hotspots_global_abs.gpkg`
  - `pct/global/hotspots_global_pct.gpkg`
  - `abs/<group_col>/hotspots_<group_col>_<group_val>_abs.gpkg`
  - `pct/<group_col>/hotspots_<group_col>_<group_val>_pct.gpkg`
- Index: `processed/hotspots/_hotspots_index.csv` (columns: scope, group_col, group_val, metric, n_hot, gpkg).

These files are meant for QGIS/QA and downstream stats (e.g., KS) without recomputing hotspots.

## Hotspot rules & export configuration

The analysis uses a single, central configuration so the hotspot rules are consistent everywhere:

Thresholding: we flag hotspots using the top/bottom tails of the distribution per service. Here we use a percentile cutoff (e.g., 5%) rather than a fixed count.

Direction of concern: services in loss are “worse when they go down” (we keep the lowest tail); services in gain are “worse when they go up” (we keep the highest tail).

Combos (optional): grouped service sets that we count per cell for quick composite summaries.

Export switches: choose whether to write GPKGs and/or the CSV index.


```{r}
#| label: hotspots_config
#| include: true
#| eval: true

#| label: hotspots_config
HOTS_CFG <- list(
  analysis_name   = "global_NCP_hotspots",
  pct_cutoff      = 0.05,
  threshold_mode  = "percent",
  rule_mode       = "vectors",
  loss = c("Nature_Access","Pollination","N_Ret_Ratio","Sed_Ret_Ratio","C_Risk_Red_Ratio"),
  gain = c("Sed_export","N_export","C_Risk"),
  combos = list(
    deg_combo = c("Nature_Access","Pollination","N_export","Sed_export","C_Risk"),
    rec_combo = c("Nature_Access","Pollination","N_Ret_Ratio","Sed_Ret_Ratio","C_Risk_Red_Ratio")
  ),
# centralize the grouping columns here
  groupings = c("income_grp","region_wb","continent","region_un","WWF_biome"),
  # IO
  write_layers = TRUE,
  write_index  = TRUE,
  out_dir      = file.path(data_dir(), "processed", "hotspots")
)
```

```{r}
#| label: show_hotspots_config
#| echo: false
loss_txt <- paste(HOTS_CFG$loss, collapse = ", ")
gain_txt <- paste(HOTS_CFG$gain, collapse = ", ")
grp_txt <- paste(HOTS_CFG$groupings, collapse = ", ")
combo_txt <- if (length(HOTS_CFG$combos))
  paste(paste0("**", names(HOTS_CFG$combos), "**: ",
               vapply(HOTS_CFG$combos, \(v) paste(v, collapse=", "), "")),
        collapse = "<br>") else "None"

cat(paste0(
"::: callout-note\n",
"**Hotspot configuration**\n\n",
"- Cutoff: ", HOTS_CFG$pct_cutoff * 100, "% (", HOTS_CFG$threshold_mode, ")\n",
"- Rule mode: ", HOTS_CFG$rule_mode, "\n",
"- Loss services: ", loss_txt, "\n",
"- Gain services: ", gain_txt, "\n",
"- Combos: ", combo_txt, "\n",
"…\n- Groupings: ", grp_txt, "\n…",
"- Write layers: ", HOTS_CFG$write_layers, " | Write index: ", HOTS_CFG$write_index, "\n",
"- Output dir: `", HOTS_CFG$out_dir, "`\n",
":::"
))
```

#  Validate Config Hotspot 

```{r}
#| label: validate_hotspots_config
#| message: false
#| warning: false

stopifnot(exists("plt_long"))
svc_all <- unique(plt_long$service)

# No service should be in both loss and gain
if (length(intersect(HOTS_CFG$loss, HOTS_CFG$gain)) > 0) {
  stop("A service appears in BOTH `loss` and `gain`. Fix HOTS_CFG.")
}

miss_loss <- setdiff(HOTS_CFG$loss, svc_all)
miss_gain <- setdiff(HOTS_CFG$gain, svc_all)
if (length(miss_loss) > 0 || length(miss_gain) > 0) {
  warning("Services in HOTS_CFG not found in `plt_long$service`:\n",
          if (length(miss_loss)) paste0("  - missing loss: ", paste(miss_loss, collapse=", "), "\n"),
          if (length(miss_gain)) paste0("  - missing gain: ", paste(miss_gain, collapse=", "), "\n"))
}
```


```{r}
#| label: hotspots_export
#| message: false
#| warning: false

# -------------------------------------------------------------------
# Purpose
# - Compute hotspot cells once (global + by subregion) for abs/pct change
# - Write labeled hotspot layers to GPKG for mapping/QA
# - Keep a compact CSV index of everything written
# Preconditions: plt_long exists; HOTS_CFG is defined (loss/gain/combos/etc.)
# -------------------------------------------------------------------

stopifnot(exists("plt_long"))

# ---- Geometry: prefer an in-memory slim grid, else build it --------------
if (exists("grid_sf") && inherits(grid_sf, "sf")) {
  geom_sf <- dplyr::select(grid_sf, fid, c_fid)
} else {
  gpkg_grid <- file.path(data_dir(), "processed", "10k_change_calc.gpkg")
  stopifnot(file.exists(gpkg_grid))
  geom_sf <- sf::st_read(gpkg_grid, quiet = TRUE) |>
    dplyr::select(dplyr::any_of(c("fid","c_fid")), dplyr::everything())
  if (!"fid" %in% names(geom_sf)) {
    geom_sf$fid <- seq_len(nrow(geom_sf))
  }
  geom_sf <- dplyr::select(geom_sf, fid, c_fid)  # keep it slim
}
stopifnot("fid" %in% names(geom_sf), !any(duplicated(geom_sf$fid)))

# ---- Helper: safe slug for filenames -------------------------------------
slug <- function(x) {
  x <- gsub("[^A-Za-z0-9]+", "_", x)
  x <- gsub("_+", "_", x)
  sub("^_|_$", "", x)
}

# ---- Core runner (uses central HOTS_CFG) ----------------------------------
run_one_hotset <- function(df, value_col, scope,
                           group_col = NA_character_, group_val = NA_character_,
                           sf_obj = geom_sf,
                           cfg = HOTS_CFG) {
  # Optional subsetting by a specific group value
  if (!is.na(group_col) && !is.na(group_val)) {
    df <- df[df[[group_col]] %in% group_val, , drop = FALSE]
  }
  if (nrow(df) == 0L) {
    return(tibble::tibble(scope, group_col, group_val = as.character(group_val),
                          metric = if (identical(value_col, "abs_chg")) "abs" else "pct",
                          n_hot = 0L, gpkg = NA_character_))
  }

  # Safety: geometry must have all fids present in df
  stopifnot("fid" %in% names(df), "fid" %in% names(sf_obj))
  if (!all(df$fid %in% sf_obj$fid)) {
    missing <- setdiff(unique(df$fid), sf_obj$fid)
    stop(sprintf("Geometry is missing %d fid(s), e.g. %s",
                 length(missing), paste(head(missing, 5), collapse = ", ")))
  }

  # Single source of truth for rules/directions/combos
  hs <- extract_hotspots(
    df             = df,
    value_col      = value_col,
    pct_cutoff     = cfg$pct_cutoff,
    threshold_mode = cfg$threshold_mode,
    rule_mode      = cfg$rule_mode,
    loss_services  = cfg$loss,
    gain_services  = cfg$gain,
    combos         = cfg$combos,
    id_cols        = c("c_fid"),
    sf_obj         = sf_obj,
    write_sf_path  = NULL,
    clean_names    = TRUE
  )

  # Output layout
  out_root <- file.path(data_dir(), "processed", "hotspots")
  metric_stub <- if (identical(value_col, "abs_chg")) "abs" else "pct"
  folder <- if (is.na(group_col)) file.path(out_root, metric_stub, "global")
            else                   file.path(out_root, metric_stub, tolower(group_col))
  dir.create(folder, recursive = TRUE, showWarnings = FALSE)

  file_stub <- if (is.na(group_col)) {
    sprintf("hotspots_global_%s", metric_stub)
  } else {
    sprintf("hotspots_%s_%s_%s", tolower(group_col), slug(group_val), metric_stub)
  }
  out_gpkg <- file.path(folder, paste0(file_stub, ".gpkg"))

  # Write only hotspot features
  if (!is.null(hs$hotspots_sf) && nrow(hs$hotspots_sf) > 0) {
    sf::st_write(hs$hotspots_sf, out_gpkg, quiet = TRUE, delete_dsn = TRUE)
    n_hot <- nrow(hs$hotspots_sf)
  } else {
    out_gpkg <- NA_character__; n_hot <- 0L
  }

  tibble::tibble(
    scope      = scope,
    group_col  = ifelse(is.na(group_col), NA_character_, group_col),
    group_val  = ifelse(is.na(group_val), NA_character_, as.character(group_val)),
    metric     = metric_stub,
    n_hot      = n_hot,
    gpkg       = out_gpkg
  )
}

# ---- Execute: global + subregional runs -----------------------------------

# Pull groupings from config, with a safe fallback
groupings <- if (!is.null(HOTS_CFG$groupings)) {
  HOTS_CFG$groupings
} else {
  c("income_grp","region_wb","continent","region_un","WWF_biome")
}

# Run both metrics
metrics <- c("abs_chg","pct_chg")
index_rows <- lapply(metrics, function(m) run_one_hotset(plt_long, value_col = m, scope = "global"))

# Subregional runs
for (gc in groupings) {
  if (!gc %in% names(plt_long)) next
  vals_chr <- as.character(stats::na.omit(unique(plt_long[[gc]])))
  if (!length(vals_chr)) next

  for (m in metrics) {
    index_rows <- append(index_rows, list(
      purrr::map_dfr(vals_chr, \(v) run_one_hotset(
        df        = plt_long,
        value_col = m,
        scope     = "by_group",
        group_col = gc,
        group_val = v
      ))
    ))
  }
}

hot_index <- dplyr::bind_rows(index_rows)

# Ensure the output dir exists before writing the CSV index
csv_dir <- file.path(data_dir(), "processed", "hotspots")
dir.create(csv_dir, recursive = TRUE, showWarnings = FALSE)
readr::write_csv(hot_index, file.path(csv_dir, "_hotspots_index.csv"))

# Small console summary
dplyr::glimpse(hot_index, width = 120)

```

## What we’ve done (checkpoint)

Wired the project with a metadata banner (version, git, data root) for reproducibility.

Enriched the 10 km grid with country/biome tags (documented chunk, eval: false) and standardized the working input at processed/10k_change_calc.gpkg.

Reshaped to analysis-ready long format (plt_long), cleaned basic issues, harmonized service labels, and set a canonical facet order.

Centralized hotspot rules in HOTS_CFG (loss/gain, combos, cutoff, groupings, IO).

Exported hotspots once (global + by subregion, for abs/pct change) to compact GPKGs under processed/hotspots/, and wrote a manifest:
processed/hotspots/_hotspots_index.csv.

Why this structure?
Heavy work (ranking/thresholding/joining) is done once. The manifest gives us traceability and fast loading for downstream steps (bar plots, violins, KS tests) without re-computation.


# Call The runner 
```{r}
# ---- 1) GLOBAL hotspots (no grouping) -----------------------------------
idx_global_abs <- run_one_hotset(plt_long, value_col = "abs_chg", scope = "global", cfg = HOTS_CFG)
idx_global_pct <- run_one_hotset(plt_long, value_col = "pct_chg", scope = "global", cfg = HOTS_CFG)

groupings <- c("income_grp","region_wb","continent","region_un","BIOME")
index_rows <- list(idx_global_abs, idx_global_pct)

for (gc in groupings) {
  vals <- sort(unique(plt_long[[gc]])); vals <- vals[!is.na(vals)]
  if (length(vals) == 0L) next

  idx_abs <- purrr::map_dfr(vals, ~ run_one_hotset(
    df        = plt_long,
    value_col = "abs_chg",
    scope     = "by_group",
    group_col = gc,
    group_val = .x,
    cfg       = HOTS_CFG
  ))
  idx_pct <- purrr::map_dfr(vals, ~ run_one_hotset(
    df        = plt_long,
    value_col = "pct_chg",
    scope     = "by_group",
    group_col = gc,
    group_val = .x,
    cfg       = HOTS_CFG
  ))

  index_rows <- append(index_rows, list(idx_abs, idx_pct))
}
hot_index <- dplyr::bind_rows(index_rows)
readr::write_csv(hot_index, file.path(data_dir(), "processed", "hotspots", "_hotspots_index.csv"))

# Small console summary

dplyr::glimpse(hot_index, width = 100)
```




Wrapper: extract hotpots per group 

```{r extract hotposts_group}

# group_cols is a character vector like c("income_grp") or c("region_wb")
extract_hotspots_by <- function(df_long, group_cols,
                                loss, gain,
                                value_col = "abs_chg",
                                pct_cutoff = 0.05,
                                threshold_mode = "percent") {

  stopifnot(all(group_cols %in% names(df_long)))

  df_long %>%
    select(fid, service, all_of(group_cols), all_of(value_col), c_fid) %>%
    rename(.value = {{ value_col }}) %>%  # standardize name for mapping
    nest(data = -all_of(group_cols)) %>%
    mutate(
      res = map(
        data,
        ~ extract_hotspots(
            df            = rename(.x, !!value_col := .value),
            value_col     = value_col,
            pct_cutoff    = pct_cutoff,
            threshold_mode= threshold_mode,
            rule_mode     = "vectors",
            loss_services = loss,
            gain_services = gain,
            id_cols       = c("c_fid")      # carry c_fid to summaries
          )
      )
    ) %>%
    transmute(
      !!!syms(group_cols),
      hotspots_df     = map(res, "hotspots_df"),
      non_hotspots_df = map(res, "non_hotspots_df"),
      summary_df      = map(res, "summary_df"),
      binary_matrix   = map(res, "binary_matrix")
    )
}

```




```{r run wrapper violins}
# -----------------------------------------------
# Faceted violins (VERTICAL) with fixed service order (3 cols)
# - ABS and PCT plotted separately
# - Free y scales per facet
# - Removes NA/0 and trims 0.1%–99.9% per service
# -----------------------------------------------
run_hotspot_violins_by <- function(group_col, out_stub,
                                   svc_order = c("C_Risk","N_export","Sed_export",
                                                 "C_Risk_Red_Ratio","N_Ret_Ratio","Sed_Ret_Ratio",
                                                 "Pollination","Nature_Access"),
                                   cut_q = 0.999,
                                   plot_n = 300000L,
                                   out_dir = "outputs/plots") {

  message("==> Violins by: ", group_col)
  dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)

  # Run hotspot wrapper once for ABS and once for PCT
  by_abs <- extract_hotspots_by(plt_long, group_col, loss, gain, value_col = "abs_chg")
  by_pct <- extract_hotspots_by(plt_long, group_col, loss, gain, value_col = "pct_chg")

  # Long value tables (fid × service × group)
  abs_vals <- by_abs %>%
    dplyr::transmute(!!group_col := .data[[group_col]],
                     vals = purrr::map(hotspots_df, ~ dplyr::select(.x, fid, service, abs_chg))) %>%
    tidyr::unnest(vals)

  pct_vals <- by_pct %>%
    dplyr::transmute(!!group_col := .data[[group_col]],
                     vals = purrr::map(hotspots_df, ~ dplyr::select(.x, fid, service, pct_chg))) %>%
    tidyr::unnest(vals)

  # Join for alignment; keep only desired services and sample for plotting speed
  vals <- dplyr::full_join(abs_vals, pct_vals, by = c(group_col, "fid", "service")) %>%
    dplyr::filter(service %in% svc_order) %>%
    dplyr::mutate(service = factor(service, levels = svc_order)) %>%
    dplyr::filter(!is.na(.data[[group_col]])) %>%
    { if (nrow(.) > plot_n) dplyr::slice_sample(., n = plot_n) else . } %>%
    dplyr::mutate(!!group_col := as.factor(.data[[group_col]]))

  # ---- ABS (vertical violins) ----
  abs_trim <- vals %>%
    dplyr::filter(!is.na(abs_chg), abs_chg != 0) %>%
    dplyr::group_by(service) %>%
    dplyr::mutate(
      lo = stats::quantile(abs_chg, 1 - cut_q, na.rm = TRUE),
      hi = stats::quantile(abs_chg, cut_q,     na.rm = TRUE)
    ) %>%
    dplyr::ungroup() %>%
    dplyr::filter(abs_chg >= lo, abs_chg <= hi)

  p_abs <- ggplot2::ggplot(abs_trim, ggplot2::aes(x = .data[[group_col]], y = abs_chg)) +
    ggplot2::geom_violin(trim = TRUE, scale = "width") +
    ggplot2::facet_wrap(~ service, scales = "free_y", ncol = 3) +
    ggplot2::labs(title = paste0("Absolute change in hotspots by ", group_col),
                  subtitle = "Violins only; NA/0 removed; per-service 99.9% trim",
                  x = NULL, y = "Absolute change (service units)") +
    ggplot2::theme_minimal(base_size = 11) +
    ggplot2::theme(axis.text.x = ggplot2::element_text(size = 8, angle = 45, hjust = 1))

  ggplot2::ggsave(file.path(out_dir, paste0(out_stub, "_abs_change_violins.png")),
                  p_abs, width = 12, height = 8, dpi = 300)

  # ---- PCT (vertical violins) ----
  pct_trim <- vals %>%
    tidyr::drop_na(pct_chg) %>%
    dplyr::filter(pct_chg != 0) %>%
    dplyr::group_by(service) %>%
    dplyr::mutate(
      lo = stats::quantile(pct_chg, 1 - cut_q, na.rm = TRUE),
      hi = stats::quantile(pct_chg, cut_q,     na.rm = TRUE)
    ) %>%
    dplyr::ungroup() %>%
    dplyr::filter(pct_chg >= lo, pct_chg <= hi)

  p_pct <- ggplot2::ggplot(pct_trim, ggplot2::aes(x = .data[[group_col]], y = pct_chg)) +
    ggplot2::geom_violin(trim = TRUE, scale = "width") +
    ggplot2::facet_wrap(~ service, scales = "free_y", ncol = 3) +
    ggplot2::labs(title = paste0("Percent change in hotspots by ", group_col),
                  subtitle = "Violins only; NA/0 removed; per-service 99.9% trim",
                  x = NULL, y = "Percent change (%)") +
    ggplot2::theme_minimal(base_size = 11) +
    ggplot2::theme(axis.text.x = ggplot2::element_text(size = 8, angle = 45, hjust = 1))

  ggplot2::ggsave(file.path(out_dir, paste0(out_stub, "_pct_change_violins.png")),
                  p_pct, width = 12, height = 8, dpi = 300)

  invisible(list(abs_plot = p_abs, pct_plot = p_pct))
}

# Facet order (row-wise fill with ncol = 3)
svc_order <- c(
  "C_Risk","N_export","Sed_export",
  "C_Risk_Red_Ratio","N_Ret_Ratio","Sed_Ret_Ratio",
  "Pollination","Nature_Access"
)

# Run
loss <- c("Nature_Access","Pollination","N_Ret_Ratio","Sed_Ret_Ratio","C_Risk_Red_Ratio")
gain <- c("Sed_export","N_export","C_Risk")

run_hotspot_violins_by("region_wb", "regionwb",  svc_order = svc_order)
run_hotspot_violins_by("WWF_biome", "biome",     svc_order = svc_order)
run_hotspot_violins_by("continent", "continent", svc_order = svc_order)
run_hotspot_violins_by("region_un",  "regionun",  svc_order = svc_order)
run_hotspot_violins_by("income_grp", "incomegrp", svc_order = svc_order)



```

```{r}
chk <- extract_hotspots( # or pick one group’s df to keep it small
  df = plt_long,
  value_col = "pct_chg",  # or "abs_chg"—direction logic is the same
  pct_cutoff = 0.05,
  threshold_mode = "percent",
  rule_mode = "vectors",
  loss_services = loss,
  gain_services = gain
)$hotspots_df

chk |>
  dplyr::filter(service %in% c("Nature_Access","Pollination")) |>
  dplyr::count(service, hotspot_flag)
# Expect hotspot_flag == "low" for these two
```


# Run Baprlots from Stored Functions 


```{r run baplots}
# Desired service order (facets)
svc_order <- c(
  "C_Risk","N_export","Sed_export",
  "C_Risk_Red_Ratio","N_Ret_Ratio","Sed_Ret_Ratio",
  "Pollination","Nature_Access"
)

groupings <- c(
  region_wb = "regionwb",
  WWF_biome = "biome",      # <- use the label column
  continent = "continent",
  region_un = "regionun",
  income_grp= "incomegrp"
)

# Generate 20 files: 5 groupings × 2 metrics × 2 (with/without Global)
invisible(lapply(names(groupings), function(gc) {
  stub <- groupings[[gc]]
  # pct, no global
  make_change_bars(gc, stub, svc_order, metric = "pct", include_global = FALSE, out_dir = "outputs/plots")
  # pct, with global
  make_change_bars(gc, stub, svc_order, metric = "pct", include_global = TRUE,  out_dir = "outputs/plots")
  # abs, no global
  make_change_bars(gc, stub, svc_order, metric = "abs", include_global = FALSE, out_dir = "outputs/plots")
  # abs, with global
  make_change_bars(gc, stub, svc_order, metric = "abs", include_global = TRUE,  out_dir = "outputs/plots")
}))


```

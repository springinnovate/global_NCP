---
title: "DaTa Preparation NBS"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: 72
---



A framework to assess the potential ecosystem service gains from restoration across five landscapes.

# Purpose

Estimate the potential value of ecosystem services provision gain  if the target surface is distributed randomly across the intervention areas.


The ecosystem data included here is derived from Chaplin-Kramer et. al (2022) [**Mapping the planet’s critical natural assets**](https://www.nature.com/articles/s41559-022-01934-5)


1. Coastal Protection. Unitless measure, refers to a vulnerability index. [**InVEST Coastal Vulnerability Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/coastal_vulnerability.html)

2. Nitrogen Export. Derived from the Nitrogen retention modeled using the [**InVEST Nutrient Delivery Ratio**](http://data.naturalcapitalproject.org/invest-releases/3.5.0/userguide/ndr.html). Expressed in kg/pixel/year

3. Sediment Retention. Derived using [**InVEST SDR: Sediment Delivery Ration**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/sdr.html). Values in ton/pixel/year

4. Pollination. Derived from [**InVEST SDR: Pollinator Abundance Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/croppollination.html). Units expressed on *equivalent people feed*.
 More information in [**Chaplin-Kramer, et al. 2022**](https://static-content.springer.com/esm/art%3A10.1038%2Fs41559-022-01934-5/MediaObjects/41559_2022_1934_MOESM1_ESM.pdf)

5. Nature Access represented as *the number of people within 1 hour travel of natural and semi-natural lands* (Chaplin-Kramer et al, 2022).
: 
# Goals:

- Identify High-Value Pixels: Focus restoration on areas where ecosystem service gains are maximized.
- Estimate Total Gains: If restoration is distributed randomly over a target area, what is the expected ecosystem service value?

- Represent uncertainty through confidence intervals and spatial distributions of selected pixels.

# Methods:

## Assumptions

- All ecosystem services are equally valued.
- Value is a constant function of service quantity.
- Uniform likelihood of restoration across the intervention area unless constrained by inputs (e.g., masks, distance).
- Independence of pixel values (no spatial autocorrelation considered in sampling)
- Sampling provides a robust method for estimating random distribution effects.

# Approach:

- Normalize ecosystem service rasters to a 0-1 scale, add them, and identify high-value areas.
- Randomly sample pixels to estimate service values under random distribution scenarios, iterating multiple times for robust confidence intervals.
-Address spatial distribution issues (e.g., clustering) later using additional weighting factors (e.g., downstream beneficiaries).


## Combined ES raster production:

Preliminar first step to deal with raster data sets representing different variables.

**Limitations:**
- Implicitly assumes equal importance of ecosystem services.
- May overemphasize areas with one dominant service, especially pollination, which is contingent to agricultural production.
- Fails to account for geographically weighted factors, plan to address.

## Random Sampling:

Suited methodology when no location-based prioritization is assumed.
Repeated  sampling process  assuming  Central Limit Theorem.
Summarize results with total estimated ES gains and confidence intervals.



# 1. Prepare Environemnt 

Load libraries, functions and prepare template data 

```{r align list of rasters, eval=TRUE, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats', 'foreign')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
align_rasters <- function(raster_list, template, resample_method = "bilinear") {
  lapply(raster_list, function(r) {
    if (!compareGeom(r, template, stopOnError = FALSE)) {
      message("Aligning raster: ", names(r))
      # Resample to align with the template
      resample(r, template, method = resample_method)
    } else {
      # Return raster as is if already aligned
      message("Raster already aligned: ", names(r))
      r
    }
  })
}

process_rasters <- function(nested_list, templates, resample_method = "bilinear") {
  lapply(seq_along(nested_list), function(i) {
    # Align all rasters in the sublist to the template
    aligned <- lapply(nested_list[[i]], function(r) {
      if (!compareGeom(r, templates[[i]], stopOnError = FALSE)) {
        message("Aligning raster: ", names(r))
        resample(r, templates[[i]], method = resample_method)
      } else {
        message("Raster already aligned: ", names(r))
        r
      }
    })
    # Stack all aligned rasters into a single multiband SpatRaster
    stacked <- do.call(c, aligned)
    
   tmp # Merge the stacked raster with the corresponding template raster
    merged <- merge(stacked, templates[[i]])
    
    # Sum the layers of the merged raster
    app(merged, sum)
  })
}
normalize_raster <- function(r) {
  min_val <- min(values(r), na.rm = TRUE)
  max_val <- max(values(r), na.rm = TRUE)
  (r - min_val) / (max_val - min_val)
}

process_intervention_area <- function(raster_list) {
  # Normalize each raster in the list
  normalized_rasters <- lapply(raster_list, normalize_raster)
  # Combine the normalized rasters by summing them
  combined_raster <- do.call(sum, normalized_rasters)
  return(combined_raster)
}
```

## 1.2 Load Templates

This loads and prepares template data used to align
```{r create tmp}
library(here)
# add backgrounds/templates to align 
path_lc <- here('ESA_LC') 
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "rec"))
lc <- lapply(tf,rast)
# create rcl matrix
rcl <- matrix(c(
  0, Inf, 0   # Any value greater than 0 becomes 1
), ncol = 3, byrow = TRUE)

#create background pixels, subsitute all by 0
tmp <- lapply(lc, function(r){
  r <- classify(r[[1]], rcl)
})

rm(lc)
```


# 2 Extract the data

## 2.1 List the Restoration Layers 
```{r select conservation data}
#Restoration:
tiffes <- file.path(here("cropped_raster_data" ), list.files(paste0(here("cropped_raster_data")),pattern= '.tif$'))
tiffes <- tiffes[c(1:5,51:55,71:75,81:90,106:110)]
```

## 2.2 Normalize and combine data layers

The combined raster is a proxy for the total ES provided(with some assumptions that need to be eventually reviewed and refined).

```{r apply mask AOI AP, eval=FALSE, include=FALSE}

# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  FilePath = tiffes,
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)

# Step 4: Extract the sorted file paths
tiffes <- file_info$FilePath

baseES <- lapply(tiffes, rast)
baseES <- lapply(baseES, function(r){
  r <- normalize_raster(r)
})

# set the number of different products to split the list 
num <- length(unique(product_names))
# split again to have nested lists
baseES <- split(baseES, rep(1:length(nam), each = num))

# align, add background and sum all the values
baseES <- process_rasters(baseES,tmp, resample_method = "bilinear")
map(1:length(baseES), function(x) writeRaster(baseES[[x]], paste0(here("restoration_combined"),'/', unique(country_names)[[x]], '_ES_sum.tif')))
```


## 2.3 Load Adjusted Griscom Restoration Data and apply mask

Use the adjusted Griscom restoration data (deals with pixel values encoding issues in the original data, had to get the metadadata for that). We apply this mask to isolate only the pixels that have been identified as Restoration Potential in Griscom's  Dataset on [Global priority areas for ecosystem
restoration](https://www.ksfire.org/woody_encroachment/documents/Global%20priority%20areas%20of%20ecosytem%20restoration.pdf).



```{r load Griscoms data, eval=FALSE, include=FALSE}
# Load Griscom Data
tiffes <- file.path(here("Restoration_Griscom"), list.files(paste0(here("Restoration_Griscom")),pattern= 'rec'))

pa_rc <- lapply(tiffes,rast)

baseES_m <- map2(
  baseES, 
  pa_rc, 
  function(rst, msk) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    mask(rst, msk, maskvalue=0)
  }
)
map(1:length(baseES), function(x) writeRaster(baseES_m[[x]], paste0(here("restoration_combined"),'/', unique(country_names)[[x]], '_ES_msk.tif'), overwrite=T))
```

![Intervention Areas & Combiend Restoration Potential](/home/jeronimo/OneDrive/WWF_nbs_op/output_maps/maps_interventions.png)


# 3 Calculate Restoration Potential output.


```{r load intevention data}
tiffes <- file.path(here("restoration_combined"), list.files(paste0(here("restoration_combined")),pattern= 'ES_msk'))
baseES_m <- lapply(tiffes, rast)
tiffes
```

## 3.1 Brazil Espiritu Santo
```{r brazil intervention , eval=FALSE, include=FALSE}
poly <- st_read(here("Interventions", "Brazil_int_areas", "Espirito_Santo_Albers.shp"))
pol_wgs <- st_transform(poly, crs=crs(baseES_m[[1]]))
serv_bra <- baseES_m[[1]] %>% crop(pol_wgs) %>% mask(pol_wgs)
serv_bra <- project(serv_bra, crs(poly), method='cubic')
rest_sp <- crop(pa_rc[[1]], pol_wgs)
rest_sp <- mask(rest_sp,pol_wgs)
writeRaster(serv_bra, here("Interventions", "Brazil_intervention", "esp_santo_rest.tif"),overwrite=TRUE)
writeRaster(rest_sp, here("Interventions", "Brazil_intervention", "SS_griscom.tif"), overwrite=TRUE)

```

### 3.1.1 Prepare Template Brazil Espiritu Santo
This just create a background of Zeroes to use as template for reprojecting (this is necessary because the access layer whic is on a different crs. Not ideal, but need to move forward)
```{r brazil intervention 1, eval=FALSE, include=FALSE}
poly <- st_read(here("Interventions", "Brazil_int_areas", "Espirito_Santo_Albers.shp"))
pol_wgs <- st_transform(poly, crs=crs(tmp[[1]]))
serv_bra <- tmp[[1]] %>% crop(pol_wgs) %>% mask(pol_wgs)
serv_bra <- project(serv_bra, crs(poly), res=30)
writeRaster(serv_bra, here("Interventions", "Brazil_intervention", "esp_santo_bkg.tif"),overwrite=TRUE)

```

## 3.2 Madagascar (Not Necessary To Mask for te Interventions)
```{r Madagascar intervention 1, eval=FALSE, include=FALSE}
poly <- st_read(here("Interventions", "Mex_intervention","mex_intervention2.geojson"))

pol_wgs <- st_transform(poly, crs=crs(baseES_m[[3]]))
serv_mdg <- baseES_m[[3]] 
serv_mdg <- project(serv_mdg, crs(pol_wgs), method='cubic')
rest_mdg <- crop(pa_rc[[3]], pol_wgs)
rest_sp <- mask(rest_sp,pol_wgs)
writeRaster(serv_mdg, here("Interventions", "Mdg_intervention", "mdg_int_rest.tif"))
writeRaster(rest_sp, here("Interventions", "Mdg_intervention", "mdg_griscom.tif"))

```

## 3.3  Yucatan
```{r yucatan intervention 1, eval=FALSE, include=FALSE}
poly <- st_read(here("Interventions", "Mex_intervention","mex_intervention2.geojson"))

pol_wgs <- st_transform(poly, crs=crs(baseES_m[[3]]))
serv_mex <- baseES_m[[3]] %>% crop(pol_wgs) %>% mask(pol_wgs)
serv_mex <- project(serv_mex, crs(poly_wgs), method='cubic')
rest_mex <- crop(pa_rc[[3]], pol_wgs)
rest_mex <- mask(rest_mex,pol_wgs)
writeRaster(serv_mex, here("Interventions", "Mex_intervention", "yucatan_int_rest.tif"))
writeRaster(rest_mex, here("Interventions", "Mex_intervention", "Mex_griscom.tif"), overwrite=TRUE)

```

### 3.3.1 Prepare Template Yucatan
This just create a background of Zeroes to use as template for reprojecting (this is necessary because the access layer whic is on a different crs. Not ideal, but need to move forward)
```{r brazil intervention, eval=FALSE, include=FALSE}
poly <- st_read(here("Interventions", "Mex_intervention","mex_intervention2.geojson"))
pol_wgs <- st_transform(poly, crs=crs(tmp[[3]]))
serv_bra <- tmp[[3]] %>% crop(pol_wgs) %>% mask(pol_wgs)
serv_bra <- project(serv_bra, crs(poly), res=30)
writeRaster(serv_bra, here("Interventions", "Mex_intervention", "yucatan_bkg.tif"),overwrite=TRUE)

```
## 3.3  Peru
```{r Madre De Dios intervention, eval=FALSE, include=FALSE}
pol <- st_read(here("Interventions","Peru","Intervenciones", "commondata","ganaderia","GANADERIA.shp"))
pol <- st_make_valid(pol)
st_write(pol, here("Interventions", "ganaderia.shp"))

#get msk non zero values from the EC mask
msk_p <- classify(baseES[[4]],rcl)
pol_wgs <- st_transform(pol, crs=crs(baseES_m[[4]]))
serv_per_gan <- baseES[[4]] %>% crop(pol_wgs) %>% mask(pol_wgs)

rest_per <- project(pa_rc[[4]], serv_per)
rest_per <- classify(rest_per,rcl)
serv_per_gan <- project(serv_per_gan, serv_per, method='bilinear')
msk_g <- classify(serv_per_gan,rcl) 
msk_g <- subst(msk_g, from=0, to =1)
per1 <- project(per1,serv_per)

mask_all <- merge(msk_g,rest_per)
mask_all <- classify(mask_all,rcl)
maks <- list(rest_per, msk_g, mask_all)

mask_all_wgs <- project(mask_all, baseES[[4]])

mks_peru <- mask(baseES[[4]],mask_all_wgs, maskvalue=0)

areas <- lapply(maks,lsm_c_ca)
save(areas, file=here("Interventions", "Peru_intervention","Proportions.RData"))
writeRaster(mask_all, here("Interventions", "Peru_intervention","MDDmask_final.tif"), overwrite=TRUE)
writeRaster(mks_peru, here("Interventions", "Peru_intervention","MDDEES_final.tif"), overwrite=TRUE)

```

## 3.4 Vietnam

Pending...

# 4 Sampling and Extracting values

This will be dealt on a two part basis:
1.Identify the hectares that yield the maximum aggregated restoration values withing the Griscom restoration potential pixels. Contingent on the entry assumptions
2. Model the estimated values of ES gains for the services based on a random sampling.
This second part has implicit a couple of assumptions: 

- We are not considering any spatial configuration (landscape metrics) aspects affecting the total value
- Eventually, the model can be refined by incorporating additional parameters (e.g minimum distance to boundaries, distance between points, topography or inhabited areas (some of it is implicit in the input data, but needs to be confirmed).
- Coastal risk protection only occurs at the coast, any random sampling performed will haveto consider this. This can be adjusted using weights.


This approach utilizes a stratified random sampling technique to estimate the mean values of different bands in a multi-band raster dataset. Stratified random sampling is a probability sampling method where the population is divided into homogeneous subgroups called strata, and random samples are taken from each stratum. In this case, the strata are defined by the spatial extent of the raster dataset, and the pixels within the raster represent the individual sampling units.

Methodology

Define the Area of Interest: The first step is to define the area of interest (AOI) within the raster dataset. This AOI represents the spatial extent from which the samples will be drawn. In this specific case, the AOI is defined as 30,000 hectares.

Calculate the Sample Size: Based on the resolution of the raster and the desired AOI, the required number of pixels to be sampled is calculated. This ensures that the total area covered by the sampled pixels corresponds to the defined AOI.

Perform Stratified Random Sampling: The spatSample() function from the terra package in R is used to perform stratified random sampling. This function allows for random sampling of pixels within the defined AOI, while also excluding pixels with "NA" values in all bands, ensuring that only valid data points are included in the analysis.

Repeat Sampling: To reduce potential sampling bias and improve the accuracy of the estimates, the random sampling process is repeated multiple times (in this case, 30 times). This is analogous to the concept of bootstrapping, where repeated sampling with replacement is used to estimate the sampling distribution of a statistic.

Calculate Summary Statistics: For each repetition, the mean, standard deviation, and 95% confidence intervals are calculated for each band in the raster dataset. This provides a measure of the central tendency and variability of the sampled data.

Synthesize Results: The results from all repetitions are combined to calculate an overall mean and confidence interval for each band. This provides a more robust estimate of the expected values, effectively correcting for potential outliers and reducing the influence of individual sample variations.

Theoretical Justification

The use of stratified random sampling is justified as it ensures that the sample is representative of the entire population (i.e., all pixels within the AOI). By dividing the population into strata and sampling from each stratum, this method reduces the variability of the estimates compared to simple random sampling. This is particularly important when dealing with spatial data, where there may be inherent spatial autocorrelation or heterogeneity.

The repeated sampling approach further enhances the robustness of the estimates by providing a distribution of possible values. This allows for the calculation of confidence intervals, which provide a measure of the uncertainty associated with the estimates. By synthesizing the results from multiple repetitions, the overall estimates are less susceptible to the influence of individual sample variations and provide a more accurate representation of the true population values.

References

Cochran, W. G. (1977). Sampling techniques (3rd ed.). John Wiley & Sons.
Lohr, S. L. (2010). Sampling: Design and analysis (2nd ed.). Brooks/Cole. 1  
1.
Bivand, R. S., Pebesma, E. J., & Gómez-Rubio, V. (2013). Applied spatial data analysis with R (2nd ed.). Springer.


## 4.1 Espirito Santo

### 4.1.1 Prepare Data Esp Santo.

Align and resampe all the raster datasets to the same crs and origin.

```{r filter data 1, eval=FALSE, include=FALSE}
serv <- rast(here("Interventions", "Brazil_intervention", "esp_santo_bkg.tif"))
a_files <- tiffes[grep("BRAZIL", tiffes)]
#a_files <- a_files[2]
serv_1 <- lapply(a_files, rast)
tf <- basename(a_files)
temp <- trim(project(serv, serv_1[[1]]))
serv_1 <- lapply(serv_1, function(r){
  r <- crop(r,temp, snap= "in", extend=TRUE)
  if (!compareGeom(r, temp, stopOnError = FALSE)) {
      message("Aligning raster: ", names(r))
      # Resample to align with the template
      resample(r, temp, method = "bilinear")
    } else {
      # Return raster as is if already aligned
      message("Raster already aligned: ", names(r))
      r
    }
  r <- trim(r)
  #r <- mask(r,temp)
r <- project(r, serv, method = 'bilinear')
})

serv_1 <- lapply(serv_1, function(r){
  r <- mask(r,serv)
})

map(1:length(serv_1), function(x) writeRaster(serv_1[[x]], paste0(here("Interventions", "Brazil_intervention"), '/', tf[x]), overwrite=TRUE))
serv_1 <- lapply(tiffes, rast)
serv_1 <- (do.call(c,serv_1))
serv_1 <- trim(merge(serv_1,serv)) #add background of Zeros
writeRaster(serv_1, paste0(here("Interventions", "Brazil_intervention"),'/', 'serv_BRAZIL.tif'), overwrite=TRUE)
```


### 4.1.2 Run Sampling and Syntetize results - Espirito Santo

Here, we are sampling the data for obtain estimate of the expected restoration gains assuming randomly selected pixels extracted form the potential restoration areas. Again, some assumptions will have to be reviewed, but this is an initial assessment.
The targeted intervention area is 30.200 ha.

```{r sampling target areas brazil, eval=FALSE, include=FALSE}
serv_1 <- rast(here("Interventions", "Brazil_intervention",'/', 'serv_BRAZIL.tif'))
rest_m <- rast(here('Interventions', 'Brazil_intervention', 'SS_griscom.tif'))
rcl <- matrix(c(
  -Inf, 0, 0,  # Any value from -Infinity to 0 remains 0
  0, Inf, 1   # Any value from 0 to Infinity becomes 1
), ncol = 3, byrow = TRUE)
rest_m <- classify(rest_m, rcl)

rest_m <-project(rest_m, serv_1)
rest_m <- terra::resample(rest_m, serv_1)
serv_1 <- mask(serv_1,rest_m, maskvalues=0)
#Calculate the number of pixels needed for 30,000 hectares
pixel_area <- 30 * 30  # Area of a single pixel in square meters (30m resolution)
hectare_area <- 10000  # Area of one hectare in square meters
pixels_needed <- round((30200 * hectare_area) / pixel_area)

# Number of repetitions
n_repetitions <- 30
# Function to perform the sampling and calculations
sample_and_calculate <- function(i, raster, pixels_needed) {
  # Sample pixels and directly extract values (without na.rm)
  sample_values <- spatSample(raster, size = pixels_needed, 
                              method = "random", 
                              na.rm = FALSE,  
                              as.points = FALSE, 
                              xy = FALSE,
                              values = TRUE) 

  # Remove rows where ALL values are NA
  sample_values <- sample_values[rowSums(is.na(sample_values)) != ncol(sample_values), ]

  # If not enough samples after removing NAs, resample
  if (nrow(sample_values) < pixels_needed) {
    sample_values <- rbind(
      sample_values,
      spatSample(raster, size = pixels_needed - nrow(sample_values),
                 method = "random", na.rm = TRUE, 
                 as.points = FALSE, xy = FALSE, values = TRUE)
    )
  }

  # Calculate summary statistics
  band_stats <- apply(sample_values, 2, function(x) { 
    mean_val <- mean(x)
    sd_val <- sd(x)
    n_val <- length(x)
    se_val <- sd_val / sqrt(n_val)
    margin_error <- qt(0.975, df = n_val - 1) * se_val
    lower_ci <- mean_val - margin_error
    upper_ci <- mean_val + margin_error
    return(c(mean = mean_val, lower_ci = lower_ci, upper_ci = upper_ci))
  })

  band_stats_df <- as.data.frame(t(band_stats))
  band_stats_df$repetition <- i

  # Calculate sum of pixel values for each band
  band_sums <- colSums(sample_values) 
  band_stats_df$sum <- band_sums

  return(band_stats_df)
}

# Using mclapply (parallel processing)
num_cores <- 15 
results_list <- mclapply(1:n_repetitions, sample_and_calculate, 
                         raster = serv_1, 
                         pixels_needed = pixels_needed,
                         mc.cores = num_cores) 

results_list <- lapply(results_list, function(df){
  df <- df %>% mutate(band=rownames(df))
})
# Combine all results into a single data frame
all_results <- as_tibble(do.call(rbind, results_list))


# Add new columns with the service names and units.
all_results <- all_results %>%
  mutate(Service = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Coastal Protection",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "Nature Access",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Export",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on Hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Export",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

all_results <- all_results %>%
  mutate(units = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Risk Reduction Index",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "People within 1 hour",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Export (kg/ha/year)",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination (equivalent people fed)",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Export (ton/kg/year)",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

all_results <- all_results %>%
  mutate(color = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "#7a0177",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "#A57C00",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "#2c944c",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "#dd1c77",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "#dd1b56",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "#08306b",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))
save(all_results, file= here("Interventions", "Brazil_intervention", "all_res_bra.RData"))
```
 
 
### 4.1.3 Plot Results Espirito Santo

```{r plot brazil outpus, echo=FALSE}
load(here("Interventions", "Brazil_intervention", "all_res_bra.RData"))
df <-all_results %>%  filter(band!="nature_access_diff_Sc3v1_PNVnoag-esa2020") %>% filter(band!= "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790")
# Assuming your 'all_results' data frame has the columns 'Service', 'units', and 'color'
# Assuming your 'all_results' data frame has the columns 'Service', 'units', and 'color'

# Generate the ggplot object
plot <- ggplot(df, aes(y = sum, fill = color)) +
  geom_boxplot() +
  labs(title = str_wrap("Total estimated service change in units for the target intervention area - Espirito Santo", width = 50), 
       y = "Total Sum Value") +
  theme_bw() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    legend.position = "none"  # Remove legend since color is already mapped
  ) +
  scale_fill_identity() + 
  facet_wrap(~ Service, scales = "free_y", labeller = labeller(Service = label_wrap_gen(width = 10))) +  
  scale_y_continuous(labels = function(x) {
    if (max(x, na.rm = TRUE) > 100000) {
      paste0(format(x / 1000, big.mark = ".", decimal.mark = ","), "k")
    } else {
      format(x, big.mark = ".", decimal.mark = ",")
    }
  }) 
plot
```

## 4.2 Yucatan

The approach is the same as in Brazil.
Two main intervention areas were used tro build the final polygon: the jaguar corridor and 
### 4.2.1 Prepare Data Yucatan.
Align and resample all the raster bands to the ame crs and origin.

```{r filter data 2, eval=FALSE, include=FALSE}
serv <- rast(here("Interventions", "Mex_intervention", "yucatan_bkg.tif"))
a_files <- tiffes[grep("MEXICO", tiffes)]
#a_files <- a_files[2]
serv_1 <- lapply(a_files, rast)
tf <- basename(a_files)
temp <- trim(project(serv, serv_1[[1]]))
serv_1 <- lapply(serv_1, function(r){
  r <- crop(r,temp, snap= "in", extend=TRUE)
  if (!compareGeom(r, temp, stopOnError = FALSE)) {
      message("Aligning raster: ", names(r))
      # Resample to align with the template
      resample(r, temp, method = "bilinear")
    } else {
      # Return raster as is if already aligned
      message("Raster already aligned: ", names(r))
      r
    }
  r <- trim(r)
  #r <- mask(r,temp)
r <- project(r, serv, method = 'bilinear')
})

serv_1 <- lapply(serv_1, function(r){
  r <- mask(r,serv)
})

map(1:length(serv_1), function(x) writeRaster(serv_1[[x]], paste0(here("Interventions", "Mex_intervention"), '/', tf[x]), overwrite=TRUE))
serv_1 <- lapply(tiffes, rast)
serv_1 <- (do.call(c,serv_1))
serv_1 <- trim(merge(serv_1,serv)) #add background of Zeros
writeRaster(serv_1, paste0(here("Interventions", "Mex_intervention"),'/', 'serv_MEXICO.tif'), overwrite=TRUE)
```

### 4.1.2 Run Sampling and Synthetize results - Yucatan

```{r sampling targe area MX, eval=FALSE, include=FALSE}
#serv_1 <- rast(here("Interventions", "Mex_intervention",'/', 'serv_MEXICO.tif'))
rest_m <- rast(here('Interventions', 'Mex_intervention', 'Mex_griscom.tif'))
rcl <- matrix(c(
  -Inf, 0, 0,  # Any value from -Infinity to 0 remains 0
  0, Inf, 1   # Any value from 0 to Infinity becomes 1
), ncol = 3, byrow = TRUE)
rest_m <- classify(rest_m, rcl)

rest_m <-project(rest_m, serv_1)
rest_m <- terra::resample(rest_m, serv_1)
serv_1 <- mask(serv_1,rest_m, maskvalues=0)
# Calculate the number of pixels needed for 15,000 hectares
pixel_area <- 30 * 30  # Area of a single pixel in square meters (30m resolution)
hectare_area <- 10000  # Area of one hectare in square meters
pixels_needed <- round((15000 * hectare_area) / pixel_area)

# Number of repetitions
n_repetitions <- 30
# Function to perform the sampling and calculations
sample_and_calculate <- function(i, raster, pixels_needed) {
  # Sample pixels and directly extract values (without na.rm)
  sample_values <- spatSample(raster, size = pixels_needed, 
                              method = "random", 
                              na.rm = FALSE,  
                              as.points = FALSE, 
                              xy = FALSE,
                              values = TRUE) 

  # Remove rows where ALL values are NA
  sample_values <- sample_values[rowSums(is.na(sample_values)) != ncol(sample_values), ]

  # If not enough samples after removing NAs, resample
  if (nrow(sample_values) < pixels_needed) {
    sample_values <- rbind(
      sample_values,
      spatSample(raster, size = pixels_needed - nrow(sample_values),
                 method = "random", na.rm = TRUE, 
                 as.points = FALSE, xy = FALSE, values = TRUE)
    )
  }

  # Calculate summary statistics
  band_stats <- apply(sample_values, 2, function(x) { 
    mean_val <- mean(x)
    sd_val <- sd(x)
    n_val <- length(x)
    se_val <- sd_val / sqrt(n_val)
    margin_error <- qt(0.975, df = n_val - 1) * se_val
    lower_ci <- mean_val - margin_error
    upper_ci <- mean_val + margin_error
    return(c(mean = mean_val, lower_ci = lower_ci, upper_ci = upper_ci))
  })

  band_stats_df <- as.data.frame(t(band_stats))
  band_stats_df$repetition <- i

  # Calculate sum of pixel values for each band
  band_sums <- colSums(sample_values) 
  band_stats_df$sum <- band_sums

  return(band_stats_df)
}

# Using mclapply (parallel processing)
num_cores <- 15 
results_list <- mclapply(1:n_repetitions, sample_and_calculate, 
                         raster = serv_1, 
                         pixels_needed = pixels_needed,
                         mc.cores = num_cores) 

results_list <- lapply(results_list, function(df){
  df <- df %>% mutate(band=rownames(df))
})
# Combine all results into a single data frame
all_results <- as_tibble(do.call(rbind, results_list))


# Add new columns with the service names and units.
all_results <- all_results %>%
  mutate(Service = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Coastal Protection",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "Nature Access",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Export",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on Hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Export",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

all_results <- all_results %>%
  mutate(units = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Risk Reduction Index",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "People within 1 hour",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Export (kg/ha/year)",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination (equivalent people fed)",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Export (ton/kg/year)",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

all_results <- all_results %>%
  mutate(color = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "#7a0177",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "#A57C00",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "#2c944c",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "#dd1c77",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "#dd1b56",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "#08306b",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

save(all_results, file=here("Interventions", "Mex_intervention", "all_results_mx.RData")) 
```

### 4.1.3 . Plot Results Yucatan 

```{r plot yucatan, echo=FALSE}
load(file=here("Interventions", "Mex_intervention", "all_results_mx.RData"))
df <-all_results %>%  filter(band!="nature_access_diff_Sc3v1_PNVnoag-esa2020") %>% filter(band!= "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790")
# Assuming your 'all_results' data frame has the columns 'Service', 'units', and 'color'
# Assuming your 'all_results' data frame has the columns 'Service', 'units', and 'color'

# Generate the ggplot object
plot <- ggplot(df, aes(y = sum, fill = color)) +
  geom_boxplot() +
  labs(title = str_wrap("Total estimated service change in units for the target intervention area - Yucatan", width = 50), 
       y = "Total Sum Value") +
  theme_bw() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    legend.position = "none"  # Remove legend since color is already mapped
  ) +
  scale_fill_identity() + 
  facet_wrap(~ Service, scales = "free_y", labeller = labeller(Service = label_wrap_gen(width = 10))) +  
  scale_y_continuous(labels = function(x) {
    if (max(x, na.rm = TRUE) > 100000) {
      paste0(format(x / 1000, big.mark = ".", decimal.mark = ","), "k")
    } else {
      format(x, big.mark = ".", decimal.mark = ",")
    }
  }) 
print(plot)
```


# Next Steps:

Run the Analysis for Madre de Dios, incorpporate beneficiaries/population related data, as the target metric is not the sum of the values.


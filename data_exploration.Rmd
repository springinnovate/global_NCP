---
title: "DaTa Preparation NBS"    
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

I will use this project to pepare and extract insights from the raster
data for the polygons in the 5 prioritized areas.

# 1. Prepare Environemnt (Load necessary libraries)

```{r load libraries, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
```

## 1.1 set class names and colors

```{r set symbols, include=FALSE}
#create vector with the country names
nam <- c("BRAZIL","MADAGASCAR","MEXICO","PERU","VIETNAM") 
lb <- data.frame(
  Class <- c("Cultivated", "Forest", "Shrubs and Grasses", "Sparse",
            "Mangroves", "Urban", "Bare", "Water"),
  color=c("#dcf064", "#00b809", "#d29000", "#ffebaf", "#009678", "#c31400", 
            "#fff5d7", "#0046c8"),
  value=c(1:8))
names(lb) <- c("Class", "color", "value")
```

## 2 Crop the Data for the target areas

```{r load target area polygons}
poly <- st_read(here("vector", "areas_nbs2.geojson")) 
#Split the vector into a list of individual polygons
#create vector with names
poly <- poly%>%split(.$COUNTR)
```

## 2.1 Test with one raster

Cropping and masking individual raster layers by the 5 study regions.
Data processed here:

- ESA land cover maps
- Pollination Restoration
- Coastal Risk Reduction 


## 2.2 Iterate over all rasters



### 2.2.1. Prepare the lists of the subsets 
Select the different datasets for batch processing. This is the critical aspect here, i it is just hard to keep track of the names of the different products, and a lost of time is wasted here. Each time some asdjustmwnt needs to be done, new data processes, i struggle organizing this, let us try to have a logic sequence. 

#### 2.2.1.2 Restoration data
```{r select restroration data to process}

#Restoration:
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= 'ESAmod.*\\.tif$'))
#Remove points, risk reduction baseline
tiffes <- tiffes[-c(2,3,4)]
tiffes2 <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= 'pollination.*\\.tif$'))
tiffes2 <- tiffes2[c(2,3)]
tiffes <- c(tiffes,tiffes2)
```


#### 2.2.1.2 Acces data
```{r select accessibility data to process}

#Restoration:
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= 'nature_access.*\\.tif$'))
tiffes <- tiffes[2]
# Access restoration
acces_rest <- rast(tiffes)
basename <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5.*", "", filename)
}
filename <- clean_filename(basename)

#acces_rest <- project(acces_rest, acc2, method='bilinear')
acces_rest <- map(1:length(poly), function(x) crop(acces_rest, poly[[x]]))
acces_rest <- map(1:length(acces_rest), function(x) mask(acces_rest[[x]], poly[[x]]))
map(1:length(acces_rest), function(x) writeRaster(acces_rest[[x]], paste0(here('cropped_raster_data'),"/",filename, '_', nam[x], '.tif'), overwrite=TRUE))


# pemnding access restoration. No need to export repporjected, just get everything done here. 

```

#### 2.2.1.3 Acces data Restoration
```{r select accessibility data to process, eval=FALSE, include=FALSE}

#Restoration:
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= 'nature_access.*\\.tif$'))
tiffes <- tiffes[1]
# Access restoration
acces_rest2 <- rast(tiffes)
basename <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5.*", "", filename)
  sub(".tif.*", "", filename)
}
filename <- clean_filename(basename)


acces_rest2 <- mapply(function(raster1, raster2) {
  project(raster1, raster2, method = "bilinear")  # Adjust method if needed
}, acces_rest2, acces_rest, SIMPLIFY = FALSE)
acces_rest2 <- map(1:length(poly), function(x) crop(acces_rest2[[x]], poly[[x]]))
acces_rest2 <- map(1:length(acces_rest2), function(x) mask(acces_rest2[[x]], poly[[x]]))
map(1:length(acces_rest2), function(x) writeRaster(acces_rest2[[x]], paste0(here('cropped_raster_data'),"/",filename, '_', nam[x], '.tif'), overwrite=TRUE))


# pending access restoration. No need to export repporjected, just get everything done here. 

```



#### 2.2.1.4 Coastal data Protection  Conservation
```{r select accessibility data to process, eval=FALSE, include=FALSE}

#Restoration:
tiffes <- file.path(here("downloaded_data_ES"), list.files(paste0(here(inpath)),pattern= 'ESAmodVCFv2_cv_habitat_value_md5.*\\.tif$'))
# Access restoration
basename <- basename(tiffes)
acces_rest <- rast("~/Documents/WWF_nbs_op/Downloaded_data_ES/ESAmodVCFv2_cv_habitat_value_md5_c01e9b17aee323ead79573d66fa4020d.tif")

clean_filename <- function(filename){
  sub("_md5.*", "", filename)
  sub(".tif.*", "", filename)
}
filename <- clean_filename(basename)

acces_rest <- map(1:length(poly), function(x) crop(acces_rest, poly[[x]]))
acces_rest2 <- map(1:length(acces_rest), function(x) mask(acces_rest[[x]], poly[[x]]))
map(1:length(acces_rest), function(x) writeRaster(acces_rest[[x]], paste0(here('cropped_raster_data'),"/",filename, '_', nam[x], '.tif'), overwrite=TRUE))


# pending access restoration. No need to export repporjected, just get everything done here. 

```



This runs the same process in batch. Memory was optimzied to not load all the data at thesame time, but process and write each raster individually before going to the next one.  


```{r iterate cropMask, eval=FALSE, include=FALSE}

# Loop through each geotiff file
for (i in seq_along(tiffes)) {
  # Get the current tiff file
  tiff_file <- tiffes[i]
  
  # Load the current geotiff (don't load all at once)
  access <- rast(tiff_file)
  
  # Apply crop and mask operations to each polygon
  cropped <- map(1:length(poly), function(x) crop(access, poly[[x]]))
  masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(tiff_file))
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) {
    writeRaster(masked[[x]], paste0(here('cropped_raster_data'),"/", base_filename, "_", nam[x], ".tif"), overwrite = TRUE)
  })
  # Optionally, clear the variable to free up memory
  rm(access, cropped, masked)
  gc()  # Garbage collection to clear memory
}
```

## 2.3. Reproj and CropMask Accesibility Raster

The accessibility data was handled separately because it is  in another crs because of course, it is a
distance raster and an equal distance projection is required.

```{r reproj access, eval=FALSE, include=FALSE}

# Access 60m 2019
acces_rest <- rast(here(inpath, 'global_people_access_population_2019_60.0m_md5_d264d371bd0d0a750b002a673abbb383.tif'))
acces_rest <- project(acces_rest, acc2, method='bilinear')
acces_rest <- map(1:length(poly), function(x) crop(acces_rest, poly[[x]]))
acces_rest <- map(1:length(acces_rest), function(x) mask(acces_rest[[x]], poly[[x]]))


map(1:length(access_rest), function(x) writeRaster(acces_rest[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'global_people_access_population_2019_60.0m_', nam[x], '.tif'), overwrite=TRUE))
```


# 3 Explore Datasets

For summarizing geotiff raster files and comparing values across
protected areas or different land covers for an initial assessment,
especially for an audience without geospatial expertise, here are some
accessible metrics : 1. Basic Statistical Summaries:

## 3.1 Summaries Land covers

Extract the shares for the different land covers for the reclassified
ESA land cover maps for the 5 study areas for 1992 and 2020.

```{r basic summaries LC, eval=FALSE, include=FALSE}

path_lc <- here('ESA_LC') 
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "rec"))
lc <- lapply(tf,rast)


lsmet <- lapply(lc, function(r){
  mets <- lsm_c_ca(r)
})
lsmet <- lapply(seq_along(lsmet), function(i) {
  lsmet[[i]]$AREA <- nam[i]
  return(lsmet[[i]])
})
lsmet <- do.call(rbind, lsmet)
lsmet <- left_join(lsmet, lb, by= join_by(class == value))
lsmet <- lsmet %>% select(!c(level,id,metric))
lsmet <-  lsmet %>% group_by(AREA, layer) %>%
  mutate(percentage = value / sum(value) * 100) %>%  # Calculate percentage
  ungroup() %>% mutate(year=case_when(
    layer == 1 ~ 1992,
    layer == 2 ~ 2020
  ))

save(lsmet, file=here("output_data", 'shares_lc2.RData'))

```

## 3.2. Chart the shares per land cover

-   Land Cover ESA
-   Area Proportion for both years (1992,2020)

```{r land covers, echo=FALSE, fig.width=9, fig.height=7}
load(here("output_data", 'shares_lc2.RData'))
t <- ggplot(lsmet, aes(x = Class, y = percentage, fill = color)) +
  geom_bar(stat = "identity") +  # Use identity since we are plotting pre-calculated values
  scale_fill_identity() +  # Use the exact colors provided in the `color` column
  facet_grid(year ~ AREA) +  # Create a grid with rows for `AREA` and columns for `layer`
  labs(title = "Class Distribution by Area and Year",
       x = "Class",
       y = "Percentage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) +  # Rotate x-axis labels for readability
  geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.01)),  # Convert to percentage format
            vjust = -0.5,  # Position labels slightly above the bars
            size = 2.5)  # Adjust label size as needed

print(t)
```


# 4. Extract Summaries Inside/Outside AOIs

Go through these change raster (priority 4 and take a look at them
and think what can be done here, and show differences for land covers),
inside outside protected areas. UICN cat National park Ia , Ib, II, III,
IV , V, VI and those non UICN (especially important in Mexico)

Barplots/distributions outside inside protected areas/land cover classes
as a way to see if protected areas are abnormally represented. This is
for conservation. Once i get this done, i need the same for griscom \##

Get summaries per PA.

Here, we take the rasters with the target data (provided ecosystem service on time t (baseline) , recover (restoration potential) or whatever else and we take the areas that we want to run comparions between inside/outside to check the relative productivity of these areas. This is key for decision making/show donors/officials for example)

It is possible to do this systematically, to save on work at the edn, gain on replicability and to be able to make adjustments easily (for example "we need all PA, even the non-UICN"). We load the polygons/rasters, cropMask to get what's inside and the inverse (*this I need to work on, it is poorly written and inneficient*)

We need to create binary masks to use as  background/the/ homgonezie the total area of interest. This is necessary because we need to know the share of the total area that is under the target. A class/status (protected area, restoration priority, land cover whatever) as a share of the total. This is necessary because we want to comparethe relative share of the area with the share of the service/target data to check compare **productivities**  inside/outside. This is the whole purpose of this par.

The whole process is run on the EPSG 8857 crs (Equal earth) to facilitate dealing with distances and areas. 
Thjer is some complication because we wantthe data organzied by country/studyarea and then by service, so some manipulation was necessary before loading. The key is that the labeling is already done and next times, the same type of analysis should be way easier. 
 
 


## 4.1.  Extract the shares PA/total


```{r load protected  areas}
#load protected areas
pa = st_read(here("vector", "wdpa_nbs.shp"))
pa <- pa%>%split(.$COUNTR)

```

## Extract shares
```{r extract sahres}
# add backgrounds/templates to align 
path_lc <- here('ESA_LC') 
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "rec"))
lc <- lapply(tf,rast)
# Drop the second band. There must be an easier way. Now i have th background to put to the ecosystem service rasters. Somethin  
# Create a background to fill the NAs from ESA's land cover maps

# create rcl matrix
rcl <- matrix(c(
  0, Inf, 0   # Any value greater than 0 becomes 1
), ncol = 3, byrow = TRUE)

bkg <- lapply(lc, function(r){
  r <- classify(r[[1]], rcl)
})# Load the rasters with the target data.
# 
rm(lc)
bkg_1 <- map2(bkg, pa, mask)
bkg_1 <- lapply(bkg_1, function(r){
  r <- subst(r, from = 0, to =1)
})
msk_ap <- map2(bkg_1,bkg, merge)

freq_ap <- lapply(msk_ap, function(r){
  f <- freq(r)
})
country_names <- unique(country_names)

freq_ap <- lapply(seq_along(freq_ap), function(i) {
  freq_ap[[i]]$country <- country_names[i]  # Add country column
  freq_ap[[i]]  # Return the modified dataframe
})
freq_ap <- as_tibble(do.call(rbind, freq_ap))

freq_ap <- freq_ap %>%
  group_by(country) %>%
  mutate(share = (count / sum(count)) * 100) %>%
  ungroup()

freq_ap <- freq_ap %>% filter(layer == 1)  %>%  filter(value == 1)
freq_ap <- freq_ap %>% select(!c(1,2,3))


```

### 4.1.1 List the conservation data 
```{r select conservation data}
#Restoration:
tiffes <- file.path(here("cropped_raster_data" ), list.files(paste0(here("cropped_raster_data")),pattern= '.tif$'))
#Remove points, risk reduction baselin    
tiffes <- tiffes[c(21:30,36:40,56:60,71:75)]
```

### 4.1.2 Mask the Service Rasters by the AOI (1)
```{r apply mask AOI AP, eval=TRUE, include=FALSE}

# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  FilePath = tiffes,
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)

# Step 4: Extract the sorted file paths
tiffes <- file_info$FilePath

baseES <- lapply(tiffes, rast)
# set the number of different products to split the list 
num <- length(unique(product_names))
# Group the list of ES as nested list, one for eah country
# split again to have nested lists
baseES <- split(baseES, rep(1:length(nam), each = num))


# Step 2: Apply mask for each group of rasters using the corresponding country vector
baseES_ap <- map2(
  baseES,   # The list of raster sublists
  pa,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector))
  }
)
# "api" stands for "outside". I don´t know why I gave tho name, it makes no sense 
baseES_api <- map2(
  baseES,   # The list of raster sublists
  pa,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector, inverse=TRUE))
  }
)


# Step 3: Flatten back the list to get a single list of masked rasters
baseES_ap <- unlist(baseES_ap, recursive = FALSE)
baseES_api <- unlist(baseES_api, recursive = FALSE)

```



################################
## 4.2 Calculate Shares (1)


```{r extract total  ES provision, eval=FALSE, include=FALSE}

#calculate ES output
# Define a function to compute the total sum for each SpatRaster
compute_sum <- function(raster) {
  total_sum <- global(raster, fun = "sum", na.rm = TRUE)[,1]  # Use global() to get the sum
  return(total_sum)
}
totSE_ap <-  lapply(baseES_ap, compute_sum)
totSE_api <-  lapply(baseES_api, compute_sum)

totSE_ap <- as_tibble(unlist(totSE_ap))
totSE_api <- as_tibble(unlist(totSE_api))
totSE <- cbind(file_info[c(2,3)], totSE_api, totSE_ap)


# dropoutside 

#rearrange columns 
names(totSE) <- c("Service", "country","Outside", "Inside")
totSE <- left_join(totSE,freq_ap)

# I need to add and then get the total!!!!!!!! 
totSE <- totSE %>% mutate(WholeArea= Outside+Inside)
# The freaking error is here!!!!!
totSE <- totSE %>% mutate(share_service= (Inside/WholeArea)*100)


names(totSE) <- c("Service", "Country","Outside", "Inside", "Share_AOI", "WholeArea", "Share_service")

#
totSE <- totSE %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Service == "global_n_retention_esamod2_compressed" ~ "Nitrogen Retention",
    Service == "global_sed_retention_esamod2_compressed" ~ "Sediment Retention",
    Service == "pollination_ppl_fed_on_ag_10s_esa2020" ~ "Pollination",
    Service == "ESAmodVCFv2_cv_habitat_value_md5_c01e9b17aee323ead79573d66fa4020d" ~ "Coastal Protection",
    TRUE ~ Service  # Keep other names unchanged, if any
  )) %>%
  # Select relevant columns and reshape the data into long format
  select(Service, Share_service, Share_AOI, Country) %>%
  pivot_longer(cols = c(Share_service, Share_AOI),
               names_to = "Metric",
               values_to = "Value")


totSE_l <- totSE %>%
  filter(!(Metric == "Share_AOI" & duplicated(paste(Country, Metric))))

write.csv(totSE, file=here('output_data', "service_prov_PA.csv"))

totSE_l <- totSE_l %>% mutate(Service=case_when(
  Metric== "Share_AOI" ~ "Area",
  TRUE ~ Service 
))

```


## 4.3 Calcualte Shares (2) 
```{r calculate Access Conservation}
tiffes <- file.path(here("cropped_raster_data" ), list.files(paste0(here("cropped_raster_data")),pattern= '.tif$'))
#Remove points, risk reduction baseline
tiffes <- tiffes[c(56:60)]
```

### 4.4 Mask the Service Rasters by the AOI (2)
```{r apply mask AOI AP2, eval=TRUE, include=FALSE}

# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  FilePath = tiffes,
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)

# Step 4: Extract the sorted file paths
tiffes <- file_info$FilePath

baseES <- lapply(tiffes, rast)
# set the number of different products to split the list 
num <- length(unique(product_names))
# Group the list of ES as nested list, one for eah country
# split again to have nested lists
baseES <- split(baseES, rep(1:length(nam), each = num))


# Step 2: Apply mask for each group of rasters using the corresponding country vector
baseES_ap <- map2(
  baseES,   # The list of raster sublists
  pa,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector))
  }
)
# "api" stands for "outside". I don´t know why I gave tho name, it makes no sense 
baseES_api <- map2(
  baseES,   # The list of raster sublists
  pa,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector, inverse=TRUE))
  }
)


# Step 3: Flatten back the list to get a single list of masked rasters
baseES_ap <- unlist(baseES_ap, recursive = FALSE)
baseES_api <- unlist(baseES_api, recursive = FALSE)

```


### 4.4 Calculate Shares (2)


```{r extract total  ES provision}

#calculate ES output
# Define a function to compute the total sum for each SpatRaster
compute_sum <- function(raster) {
  total_sum <- global(raster, fun = "sum", na.rm = TRUE)[,1]  # Use global() to get the sum
  return(total_sum)
}
totSE_ap <-  lapply(baseES_ap, compute_sum)
totSE_api <-  lapply(baseES_api, compute_sum)

totSE_ap <- as_tibble(unlist(totSE_ap))
totSE_api <- as_tibble(unlist(totSE_api))
totSE <- cbind(file_info[c(2,3)], totSE_api, totSE_ap)


# dropoutside 

#rearrange columns 
names(totSE) <- c("Service", "country","Outside", "Inside")
totSE <- left_join(totSE,freq_ap)

# I need to add and then get the total!!!!!!!! 
totSE <- totSE %>% mutate(WholeArea= Outside+Inside)
# The freaking error is here!!!!!
totSE <- totSE %>% mutate(share_service= (Inside/WholeArea)*100)


names(totSE) <- c("Service", "Country","Outside", "Inside", "Share_AOI", "WholeArea", "Share_service")

#
totSE <- totSE %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Service == "nature_access_lspop2019_esa2020modVCFhab" ~ "Access",
    TRUE ~ Service  # Keep other names unchanged, if any
  )) %>%
  # Select relevant columns and reshape the data into long format
  select(Service, Share_service, Share_AOI, Country) %>%
  pivot_longer(cols = c(Share_service, Share_AOI),
               names_to = "Metric",
               values_to = "Value")


totSE_l2 <- totSE %>%
  filter(!(Metric == "Share_AOI" & duplicated(paste(Country, Metric))))
totSE_l <- rbind(totSE_l, totSE_l2)

totSE_l <- totSE_l %>% mutate(Service=case_when(
  Metric== "Share_AOI" ~ "Area",
  TRUE ~ Service 
))
write.csv(totSE_l, file=here('output_data', "service_prov_PA.csv"))
```
## 4.4  Plot the shares

```{r plot shares AP, fig.width=9, fig.height=6.5}

totSE_l <- as_tibble(read.csv(here('output_data', 'service_prov_PA.csv')))
totSE_l$Service <- factor(totSE_l$Service, levels = c('Area', 'Nitrogen Retention', 'Sediment Retention', "Pollination", "Access", "Coastal Protection"))

# Step 2: Create the bar plot
p <- ggplot(totSE_l, aes(x = Metric, y = Value, fill = Service)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) + 
  aes(width = ifelse(Metric == "Share_AOI", 0.5, 0.9)) + 
  scale_y_continuous(limits = c(0, 100)) +
  facet_wrap(~ Country, ncol = 5) +  # Create 5 columns (one for each country)
  scale_fill_manual(values = c(
    "Area" = "gray50",         # Medium gray for "Share Area"
    "Nitrogen Retention" = "#2c944c",  # Shade of green
    "Sediment Retention" = "#08306b",  # Shade of blue
    "Pollination" = "#dd1c77",          # Shade of purple
    "Access" = "#A57C00",
    "Coastal Protection" = "#7a0177"
  )) +
  labs(
    title = "Share of the protected areas and of the total ES provided inside them",
    x = "Metric",
    y = "%",
    fill = "Service"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal",  # Arrange legend items horizontally
    legend.spacing.x = unit(0.1, 'cm'),  # Add horizontal spacing between legend items
    legend.title = element_text(size = 12, face = "bold"),  # Adjust legend title size
    legend.text = element_text(size = 10),  # Adjust legend text size
    legend.box.just = "center"  # Center the legend items
  )


print(p)
```

########################################################################## 
Pendingthis same thing with the beneficiaries and other data wit other crs/spatial resolution

########################################################################## 

-   Coastal Risk Reduction
-   Restoration
    -   Nitrogen Export
    -   nature access restoration
    -   pollination restoration
    -   sediment export
    -   Conservation?

# Get the histograms


```{r set list of names}

# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)


prod_nam <- unique(product_names)

num <- length(unique(product_names))
#serv <- c("Coastal_Protection", "Nitrogen_Retention", "Sediment_Retention", "Pollination", "Acces") 
#serv <- c("Access") 

new_nam <- cbind(prod_nam, serv)
```


```{r extract density restoration hist, eval=FALSE, include=FALSE}

baseES_v <- lapply(baseES_api, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

baseESap_v <- lapply(baseES_ap, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

# Add two new columns to each data frame in the list
baseES_v  <- Map(function(df, row) {
  names(df)[1] <- "Value"
  df$Service <- row[[1]]  
  df$Country <- row[[2]]  
  return(df)
}, baseES_v , split(file_info, seq(nrow(file_info))))

baseESap_v  <- Map(function(df, row) {
  names(df)[1] <- "Value"
  df$Service <- row[[1]]  
  df$Country <- row[[2]]  
  return(df)
}, baseESap_v , split(file_info, seq(nrow(file_info))))

           
##################### do not run inside the Rmd!!!!!!!!
baseES_v <- do.call(rbind, baseES_v)
baseESap_v <- do.call(rbind, baseESap_v)
##################### do not run inside the Rmd!!!!!!!!
baseES_v <- baseES_v%>% mutate(restoration = "0")
baseESap_v <- baseESap_v%>% mutate(restoration = "1")

baseES_v <- rbind(baseES_v, baseESap_v)
rm(baseESap_v)


baseES_v <- baseES_v %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Service == new_nam[1,][1] ~ new_nam[1,][2],
    # Service == new_nam[2,][1] ~ new_nam[2,][2],
    # Service == new_nam[3,][1] ~ new_nam[3,][2],
    # Service == new_nam[4,][1] ~ new_nam[4,][2],
    TRUE ~ Service  # Keep other names unchanged, if any
  )) 

write.csv(baseES_v, here('output_data', "distributionServ_pa2.csv"))
# Calculate the 98th percentile for each combination of Service and Country
```

```{r set list of names}

# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)


prod_nam <- unique(product_names)

num <- length(unique(product_names))
serv <- c("Coastal_Protection", "Nitrogen_Retention", "Sediment_Retention", "Pollination") #Access 

new_nam <- cbind(prod_nam, serv)
```


```{r extract density restoration hist, eval=FALSE, include=FALSE}

baseES_v <- lapply(baseES_api, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

baseESap_v <- lapply(baseES_ap, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

# Add two new columns to each data frame in the list
baseES_v  <- Map(function(df, row) {
  names(df)[1] <- "Value"
  df$Service <- row[[1]]  
  df$Country <- row[[2]]  
  return(df)
}, baseES_v , split(file_info, seq(nrow(file_info))))

baseESap_v  <- Map(function(df, row) {
  names(df)[1] <- "Value"
  df$Service <- row[[1]]  
  df$Country <- row[[2]]  
  return(df)
}, baseESap_v , split(file_info, seq(nrow(file_info))))


           
##################### do not run inside the Rmd!!!!!!!!
baseES_v <- do.call(rbind, baseES_v)
baseESap_v <- do.call(rbind, baseESap_v)
##################### do not run inside the Rmd!!!!!!!!
baseES_v <- baseES_v%>% mutate(restoration = "0")
baseESap_v <- baseESap_v%>% mutate(restoration = "1")

baseES_v <- rbind(baseES_v, baseESap_v)
rm(baseESap_v)


baseES_v <- baseES_v %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Service == new_nam[1,][1] ~ new_nam[1,][2],
    Service == new_nam[2,][1] ~ new_nam[2,][2],
    Service == new_nam[3,][1] ~ new_nam[3,][2],
    Service == new_nam[4,][1] ~ new_nam[4,][2],
    TRUE ~ Service  # Keep other names unchanged, if any
  )) 

write.csv(baseES_v, here('output_data', "distributionServ_pa2.csv"))

base_esold <- read.csv(here('output_data', "distributionServ_pa1.csv"))
base_esold <- base_esold[-1]
baseES_v <- rbind(base_esold, baseES_v)
write.csv(baseES_v, here('output_data', "distributionServ_pa.csv"))


# Calculate the 98th percentile for each combination of Service and Country
```

```

baseES_vf <- read.csv(here('data', "distributionServ.csv"))

# Calculate the 98th percentile for each combination of Service and Country
percentile_98 <- baseES_vf %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.98)) %>%
  ungroup()
percentile_95 <- baseES_vf %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.95)) %>%
  ungroup()


# Join the threshold back to the original data and filter values up to the 98th percentile
baseES_vf_filtered <- baseES_vf %>%
  left_join(percentile_98, by = c("Service", "Country")) %>%
  filter(Value <= threshold)

baseES_vf_filtered <- baseES_vf %>%
  left_join(percentile_95, by = c("Service", "Country")) %>%
  filter(Value <= threshold)

```

Count <- unique(country_names) 

C <- 5

df <- baseES_vf_filtered %>% filter(Country==Count[C]) %>% filter(Service!="Nature Access")


t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  #facet_wrap(~Service, scales = "free") +
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `set` variable
  labs(title = paste("Density Plot of Services Inside and Outside Protected Areas -", Count[C]),
       x = "Value",
       y = "Density",
       fill = "Set") +
  #scale_y_continuous(trans = 'log10')# Label for the fill legend
baseES_vf$Service <- factor(baseES_vf$Service, levels = c(
  "Nitrogen Retention", "Sediment Retention", "Pollination", "Nature Access"))
write.csv(baseES_vf, here('output_data', "distributionServ_pa.csv"))
```

## 4.3. Produce plots
### 4.3.05 Load density data PA
```{r load_dens_rest}
  baseES_vf <- as_tibble(read.csv(here('output_data', "distributionServ_pa.csv")))
  baseES_vf$Service <- factor(baseES_vf$Service, levels = c(
    "Nitrogen Retention", "Sediment Retention", "Pollination", "Nature Access"))

# Calculate both the 2nd and 98th percentiles
percentiles <- baseES_vf %>%
  group_by(Service, Country) %>%
  summarize(
    lower_threshold = quantile(Value, 0.02),
    upper_threshold = quantile(Value, 0.98)
  ) %>%
  ungroup()

# Join the thresholds back to the original data and filter values within the 2nd and 98th percentiles
baseES_filtered <- baseES_vf %>%
  left_join(percentiles, by = c("Service", "Country")) %>%
  filter(Value >= lower_threshold & Value <= upper_threshold)


```
### 4.3.1 BRazil

This is calcualted for the percentile 95 to remove extreme values
```{r density plots pa 1, echo=FALSE, fig.height=6, fig.width=9}


C <- 1

df <- baseES_filtered %>% filter(Country==nam[C])
t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Protected Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Protected Area"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
>>>>>>> main
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
<<<<<<< HEAD
t
```

# Plot the shares

```{r ggplot, h=8, w=8}

# Step 2: Create the bar plot
p <- ggplot(totSE_l, aes(x = Metric, y = Value, fill = Service)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) + 
  #scale_y_continuous(limits = c(0, 100))+
  facet_wrap(~ Country, ncol = 5) +  # Create 5 columns (one for each country)
  scale_fill_brewer(palette = "Set3") +  # Use a color palette for services
  labs(title = "Share of the protected areas and of the total ES provided inside them",
       x = "Metric",
       y = "%",
       fill = "Service") +
  #theme_minimal() +
   theme(
=======
print(t)
```

### 4.3.2 Madagascar
```{r density plots pa 2, echo=FALSE, fig.height=6, fig.width=9}

C <- 2

df <- baseES_filtered %>% filter(Country==nam[C]) 
t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Protected Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Protected Area"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```

### 4.3.3 Mexico-Yucatan

```{r density plots 2, echo=FALSE, fig.height=6, fig.width=9}
#baseES_vf <- read.csv(here('output_data', "distributionServ_pa.csv"))

C <- 3

df <- baseES_filtered %>% filter(Country==nam[C]) 
t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Protected Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Protected Area"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```

### 4.3.4 Peru- Madre de Dios

```{r density plots 2, echo=FALSE, fig.height=6, fig.width=9}
#baseES_vf <- read.csv(here('output_data', "distributionServ_pa.csv"))

C <- 4

df <- baseES_vf %>% filter(Country==nam[C]) 
t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Protected Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Protected Area"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```

### 4.3.2 Vietnam
```{r density plots 2, echo=FALSE, fig.height=6, fig.width=9}
#baseES_vf <- read.csv(here('output_data', "distributionServ_pa.csv"))

C <- 5

df <- baseES_filtered %>% filter(Country==nam[C]) 
t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Protected Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Protected Area"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```

# 5. Restorastion Areas (Nature base) 

```{r resto, eval=FALSE, include=FALSE}

#load protected areas
rest_tf  <- file.path(here('restoration'), list.files(here('restoration'), pattern="tif"))

pa = lapply(rest_tf, rast)
pa <- pa[-1]

# create rcl matrix
rcl <- matrix(c(
  0, 0, NA,   # Zero becomes NA
  0, Inf, 1   # Any value greater than 0 becomes 1
), ncol = 3, byrow = TRUE)

# this produces a binbary mask. 1 inside restoration areas, 0 outside. this also works as background to fill later
pa <- lapply(pa, function(r){
  r <- classify(r, rcl)
})
###########################################
#Calcualte share inside outside AOI (when it is a raster)
freq_ap <- lapply(pa, function(r){
  f <- freq(r)
})

country_names <- unique(country_names)

freq_ap <- lapply(seq_along(freq_ap), function(i) {
  freq_ap[[i]]$Country <- country_names[i]  # Add country column
  freq_ap[[i]]  # Return the modified dataframe
})
freq_ap <- as_tibble(do.call(rbind, freq_ap))

freq_ap <- freq_ap %>%
  group_by(Country) %>%
  mutate(share = (count / sum(count)) * 100) %>%
  ungroup()
freq_ap <- freq_ap %>% filter(layer == 1)  %>%  filter(value == 1)
freq_ap <- freq_ap %>% select(!c(1,2))

#############################################
#############################################
# Load the rasters with the target data.
tiffes <- file.path(wd, list.files(wd, pattern="tif"))
#tiffes <-  tiffes[c(36:40, 46:50, 66:70, 86:90)]
#tiffes <-  tiffes[c(31:35, 41:45, 81:85)]
#tiffes <-  tiffes[c(1:5)]#, 26:30)] #access
tiffes <-  tiffes[c(56:60)]#, 26:30)] #access restoration
#tiffes <-  tiffes[c(11:15, 1:45, 81:85)]
# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  FilePath = tiffes,
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)

# Step 4: Extract the sorted file paths
tiffes <- file_info$FilePath

baseES <- lapply(tiffes, rast)

num <- length(unique(product_names))
# Group the list of ES as nested list, one for each country
baseES <- split(baseES, rep(1:length(baseES), each = num))

# #unnest the list. This only works if there is only 1 service, else we nee to do this y hand 
# msk <- unlist(baseES)
# msk <- 

# Step cropmask: Add the backgrounds
#resample to bring to the same resolution and apply the mask 
pa <- map2(
  pa, 
  msk,
  function(r1,r2){
    r1 <- resample(r1,r2, method = "near", , threads= 10)
  })


# Step 2: Apply mask for each group of rasters using the corresponding country vector
baseES_ap <- map2(
  baseES,   # The list of raster sublists
  pa,     # The list of AOIs
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector, maskvalue=0))
  }
)
# "api" stands for "outside". I don´t know why I gave tho name, it makes no sense 
baseES_api <- map2(
  baseES,   # The list of raster sublists
  pa,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector, maskvalue=0, inverse=TRUE))
  }
)

# Step 3: Flatten back the list to get a single list of masked rasters
baseES_ap <- unlist(baseES_ap, recursive = FALSE)
baseES_api <- unlist(baseES_api, recursive = FALSE)
################################

# This extracts the background/the total area of interest. This is necessary because we need to know the share of the total area that is under the target 
# class/status (protected area, restoration, land cover whatever) as a share of the total. This is necessary because we want to compare this share with the share of the service/target data to check compare between inside/outside. This is the whole purpose of
# produce backgeiund if zeros

# bkg <- lapply(pa, function(r){
#   r <- subst(r, from = c(1), to =0)
# })
# 
# 

#####################################################
# leave it here, but this has the frequencies. from here getting the shares is very easy. I always forget tho. 

# split again to have nested lists
msk_ap <- split(msk_ap, rep(1:length(bkg), each = num))
msk_api <- split(msk_api, rep(1:length(bkg), each = num))


##################################################################
##################################################################

#calculate ES output
# Define a function to compute the total sum for each SpatRaster
compute_sum <- function(raster) {
  total_sum <- global(raster, fun = "sum", na.rm = TRUE)[,1]  # Use global() to get the sum
  return(total_sum)
}

totSE_ap <-  lapply(baseES_ap, compute_sum)#, mc.cores = detectCores() - 4)
totSE_api <-  lapply(baseES_api, compute_sum)#, mc.cores = detectCores() - 4)


totSE_ap <- as_tibble(unlist(totSE_ap))
totSE_api <- as_tibble(unlist(totSE_api))
totSE <- cbind(file_info[c(2,3)], totSE_api, totSE_ap)

#rearrange columns 
names(totSE) <- c("Service", "Country","Outside", "Inside")
totSE <- left_join(totSE,freq_ap)#!!!!!!!!!

###############################################
###############################################
# I need to add and then get the total!!!!!!!! 
totSE <- totSE %>% mutate(WholeArea= Outside+Inside)
# The freaking error is here!!!!!
totSE <- totSE %>% mutate(share_service= (Inside/WholeArea)*100)



names(totSE)[5] <- "pix_count_in"
names(totSE)[6] <- "share_area"
#calculate ES output


# clean the service name: v#doesn ot work well with the freaking name of theacces data, nbut anyway, i really cleaned this code, which was the key aspect.
totSE$Service <- sub("_.*", "", totSE$Service)
write.csv(totSE, file=here('output_data', "service_prov_rest.csv"))



totSE_l <- totSE %>%
  select(Service, share_service, share_area, Country) %>%
  pivot_longer(cols = c(share_service, share_area),
               names_to = "Metric",
               values_to = "Value")


totSE_l <- totSE_l %>%
  filter(!(Metric == "share_area" & duplicated(paste(Country, Metric))))

totSE_l <- totSE_l %>% mutate(Service=case_when(
  Service== "nature" ~ "access",
  TRUE ~ Service 
))

#3remove repeate share_area columns
totSE_l <- totSE_l %>%
  group_by(Country, Metric) %>%
  filter(!(Metric == "share_area" & row_number() > 1)) %>%
  ungroup()


# change the name to area where it comes


totSE_l <- totSE_l %>% mutate(Service=case_when(
  Metric== "share_area" ~ "area",
  TRUE ~ Service 
))
########### Listoooooooo!!!!!!!
```

## 5.1  Plot the shares

```{r plot shares, fig.width=9, fig.height=7}

#load(here('output_data', 'serv_dist2.RData'))
# Step 2: Create the bar plot
p <- ggplot(totSE_l, aes(x = Metric, y = Value, fill = Service)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) + 
  aes(width = ifelse(Metric == "share_area", 0.5, 0.9)) + 
  scale_y_continuous(limits = c(0, 100)) +
  facet_wrap(~ Country, ncol = 5) +  # Create 5 columns (one for each country)
  scale_fill_manual(values = c(
    "area" = "gray50",         # Medium gray for "Share Area"
    "nitrogen" = "#2c944c",  # Shade of green
    "sediment" = "#08306b",  # Shade of blue
    "pollination" = "#dd1c77",          # Shade of purple
    "access" = "#A57C00"
  )) +
  labs(
    title = "Share of the Restoration Potential areas and of the total ES provided inside them",
    x = "Metric",
    y = "%",
    fill = "Service"
  ) +
  theme(
>>>>>>> main
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal",  # Arrange legend items horizontally
    legend.spacing.x = unit(0.1, 'cm'),  # Add horizontal spacing between legend items
    legend.title = element_text(size = 12, face = "bold"),  # Adjust legend title size
    legend.text = element_text(size = 10),  # Adjust legend text size
    legend.box.just = "center"  # Center the legend items
  )


<<<<<<< HEAD

```

# Restorastion Areas Griscom

```{r resto}
wd <- '/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/global_products/Restoration'



tiffes <- file.path(wd,list.files(wd, pattern = "\\.tif$", full.names = FALSE))
tiffes <- tiffes[-5]



rest <- lapply(tiffes, rast)
rest <- lapply(rest, function(r){
  r <- as.numeric(r)
  r <- classify(r, cbind(20101, Inf, 1), others = 0)
})

  r <- classify(r, cbind(0, Inf, 1), others = NA)
rest <- 

               # This function extracts the 1 and 0 for mthe categorical rasters from restoration and corrects the weird values. did this because i had no metadata. Add the function to a new package eventully
process_raster <- function(raster) {
  # Step 1: Extract unique values
  unique_values <- unique(values(raster, na.rm = TRUE))
  unique_values <- as.vector(unique_values)  # Convert to vector

  # Step 2: Get frequency counts of each unique value
  value_frequencies <- freq(raster)
  val_f <- as.numeric(value_frequencies[, 2])  # Get the frequencies

  # Step 3: Convert raster to numeric
  raster_numeric <- as.numeric(raster)

  # Step 4: Substitute the values with the frequencies
  raster_subst <- subst(raster_numeric, from = unique_values, to = val_f)

  # Return the reclassified raster
  return(raster_subst)
}

names <- basename(tiffes)

rest  <- lapply(rest, process_raster)
#reorganize the list by countries, not by products. 
map(1:length(rest), function(x) writeRaster(rest[[x]], here('data', 'griscom_areas', names[x])))

# Step 1: Extract country names from the "source" attribute of each SpatRaster object
get_country <- function(r) {
  # Extract the source file path
  source_name <- sources(r)[1]  # Extract the first source (in case there are multiple)

  # Use a regular expression to extract the country name from the file name
  # Assuming the file name format is "something_COUNTRY.tif"
  country <- sub(".*_([^_]+)\\.tif$", "\\1", source_name)
  return(country)
}

country_names <- unique(sapply(baseES, get_country))  # Get unique country names
country_list <- setNames(vector("list", length(country_names)), country_names)

# Step 3: Group SpatRasters by country and populate the nested list
for (i in seq_along(baseES)) {
  country <- get_country(baseES[[i]])
  country_list[[country]] <- c(country_list[[country]], list(baseES[[i]]))
}

baseES <- country_list
rm(country_list)

baseES_rp <- lapply(seq_along(baseES), function(i) {
  # Get the current sublist of SpatRasters from baseES
  current_sublist <- baseES[[i]]
  # Get the corresponding template SpatRaster for reprojection
  template_raster <- rest[[i]]
  # Use lapply to reproject each SpatRaster in the sublist using the template raster
  reprojected_sublist <- lapply(current_sublist, function(r) {
    # Reproject the SpatRaster to match the CRS and extent of the template raster
    project(r, template_raster, method = "bilinear")
  })
  # Return the reprojected sublist
  return(reprojected_sublist)
})


baseES <- baseES_rp
rm(baseES_rp)

baseES_0 <- lapply(seq_along(baseES), function(i) {
  # Get the current sublist of SpatRasters from baseES
  current_sublist <- baseES[[i]]
  # Get the corresponding template SpatRaster for reprojection
  template_raster <- rest[[i]]
  # Use lapply to reproject each SpatRaster in the sublist using the template raster
  masked_sublist <- lapply(current_sublist, function(r) {
    # Reproject the SpatRaster to match the CRS and extent of the template raster
    mask(r, template_raster, maskvalues=0)
  })
  # Return the reprojected sublist
  return(masked_sublist)
})
# ok so far so good, but of course resolution with freaking access is always an issue
baseES_1 <- lapply(seq_along(baseES), function(i) {
  # Get the current sublist of SpatRasters from baseES
  current_sublist <- baseES[[i]]
  # Get the corresponding template SpatRaster for reprojection
  template_raster <- rest[[i]]
  # Use lapply to reproject each SpatRaster in the sublist using the template raster
  masked_sublist <- lapply(current_sublist, function(r) {
    # Reproject the SpatRaster to match the CRS and extent of the template raster
    mask(r, template_raster, maskvalues=1)
  })
  # Return the reprojected sublist
  return(masked_sublist)
})

 # Step 1: Flatten the nested list into a single list of SpatRaster objects
flattened_list <- unlist(baseES_1, recursive = FALSE)

# Step 2: Extract information for the data frame

# Create a dataframe to store the information
raster_info <- data.frame(
  Country = character(),   # Empty column for country names
  Product = character(),   # Empty column for product names
  stringsAsFactors = FALSE # Prevent automatic conversion to factors
)

# Step 3: Generate the correct country names for each raster
# Calculate the number of rasters per country
rasters_per_country <- length(flattened_list) / length(country_names)

# Repeat each country name for the number of rasters per country
repeated_country_names <- rep(country_names, each = rasters_per_country)

# Step 4: Iterate over the flattened list and extract the relevant information
for (i in seq_along(flattened_list)) {
  # Get the corresponding country name from the repeated country names vector
  country_name <- repeated_country_names[i]
  
  # Get the product name from the `name` slot of the SpatRaster
  product_name <- names(flattened_list[[i]])
  
  # If the product name contains a "~" or "_md5", extract only the part before it
  if (grepl("_md5", product_name)) {
    product_name <- sub("_md5.*$", "", product_name)
  }

  # Step 5: Add the extracted information to the dataframe
  raster_info <- rbind(raster_info, data.frame(Country = country_name, Product = product_name))
}

# Step 5: Optional - If you need to store the raster info back into a named list with country and product
# Create a named list where each raster is named by country and product

# baseES_0 <- setNames(flattened_list, paste(raster_info$Country, raster_info$Product, sep = "_"))
baseES_1 <- setNames(flattened_list, paste(raster_info$Country, raster_info$Product, sep = "_"))

# Get the histograms
baseES_v <- lapply(baseES_0, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

baseESap_v <- lapply(baseES_1, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})


# Adding corresponding rows from `df` to each tibble in `list_of_tibbles`
baseES_v <- map2(baseES_v,                  # First input: List of tibbles
  split(raster_info, seq(nrow(raster_info))),         # Second input: Split the dataframe into individual rows
  ~ mutate(.x, Country = .y$Country, Product = .y$Product)  # Add `Country` and `Product` columns from df to each tibble
)

baseESap_v <- map2(baseESap_v,                  # First input: List of tibbles
  split(raster_info, seq(nrow(raster_info))),         # Second input: Split the dataframe into individual rows
  ~ mutate(.x, Country = .y$Country, Product = .y$Product)  # Add `Country` and `Product` columns from df to each tibble
)


new_col <- "Value"
# Use lapply to rename the first column of each tibble
baseES_v <- lapply(baseES_v, function(tbl) {
  tbl %>%
    rename(!!new_col := 1)  # Rename first column using column position
})

baseESap_v <- lapply(baseESap_v, function(tbl) {
  tbl %>%
    rename(!!new_col := 1)  # Rename first column using column position
})


baseES_v <- do.call(rbind, baseES_v)
baseESap_v <- do.call(rbind, baseESap_v)
##################### do not run inside the Rmd!!!!!!!!
=======
print(p)
```


## 5.2  Get the histograms

```{r extract density restoration hist, eval=FALSE, include=FALSE}

baseES_v <- lapply(baseES_api, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

baseESap_v <- lapply(baseES_ap, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})
baseES_v <- lapply(baseES_v, function(df) {
  # Extract the service name (everything before the first "_")
  service_name <- sub("_.*", "", names(df))
  # Rename the first column to "value"
  names(df) <- "value"
  # Add a new column "Service" with the extracted service name
  df <- df %>%
    mutate(Service = service_name)
  return(df)
})
baseESap_v <- lapply(baseESap_v, function(df) {
  # Extract the service name (everything before the first "_")
  service_name <- sub("_.*", "", names(df))
  # Rename the first column to "value"
  names(df) <- "value"
  # Add a new column "Service" with the extracted service name
  df <- df %>%
    mutate(Service = service_name)
  return(df)
})
country_names <- as_tibble(country_names)
names(country_names) <-"Country"

baseES_v <- Map(function(df, country_names) {
  df$country <- country_names$Country 
  df
}, baseES_v, split(country_names, seq(nrow(country_names))))

baseESap_v <- Map(function(df, country_names) {
  df$country <- country_names$Country 
  df
}, baseESap_v, split(country_names, seq(nrow(country_names))))

##################### do not run inside the Rmd!!!!!!!!
baseES_v <- do.call(rbind, baseES_v)
baseESap_v <- do.call(rbind, baseESap_v)
##################### do not run inside the Rmd!!!!!!!!

>>>>>>> main
baseES_v <- baseES_v%>% mutate(restoration = "0")
baseESap_v <- baseESap_v%>% mutate(restoration = "1")

baseES_v <- rbind(baseES_v, baseESap_v)

<<<<<<< HEAD
baseES_v <- baseES_v %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Product == "global_n_retention_esamod2_compressed" ~ "Nitrogen Retention",
    Product == "global_sed_retention_esamod2_compressed" ~ "Sediment Retention",
    Product == "pollination_ppl_fed_on_ag_10s_esa2020" ~ "Pollination",
    Product == "access_WGS84" ~ "Nature Access",
    TRUE ~ Product  # Keep other names unchanged, if any
  )) 

write.csv(baseES_v, here('data', "distributionServ_restoration.csv"))
# Calculate the 98th percentile for each combination of Service and Country
percentile_98 <- baseES_v %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.98)) %>%
  ungroup()
percentile_95 <- baseES_v %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.95)) %>%
  ungroup()


# Join the threshold back to the original data and filter values up to the 98th percentile
baseES_filtered <- baseES_v %>%
  left_join(percentile_98, by = c("Service", "Country")) %>%
  filter(Value <= threshold)

baseES_filtered <- baseES_v %>%
  left_join(percentile_95, by = c("Service", "Country")) %>%
  filter(Value <= threshold)



C <- 5

df <- baseES_filtered %>% filter(Country==country_names[C]) %>% filter(Service!="Nature Access")

rm(t)
t <- ggplot(df, aes(x = Value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  #facet_wrap(~Country, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `set` variable
  labs(title = paste("Density Plot of Services Inside and Outside Griscom restoration Areas -", country_names[C]),
       x = "Value",
       y = "Density",
       fill = "Restoration") +
  #scale_y_continuous(trans = 'log10')# Label for the fill legend
=======

# Write the output
write.csv(baseES_v, here('output_data', "distributionServ_restoration.csv"))
```


## 5.2. Produdce the Histograms

### 5.2.05 Load and prepare the data 
```{r laod data density rest}
baseES_v <- as_tibble(read.csv(here('output_data', "distributionServ_restoration.csv")))

baseES_v$Service <- factor(baseES_v$Service, levels = c(
  "nitrogen", "sediment", "pollination"))
baseES_v$restoration <- factor(baseES_v$restoration, levels = c(0,1))


#Calculate the 98th percentile for each combination of Service and Country
percentile_98 <- baseES_v %>%
  group_by(Service, country) %>%
  summarize(threshold = quantile(value, 0.98)) %>%
  ungroup()
# # Join the threshold back to the original data and filter values up to the 98th percentile
baseES_filtered <- baseES_v %>%
  left_join(percentile_98, by = c("Service", "country")) %>%
  filter(value <= threshold)
```

### 5.2.1 Brazil
```{r plot hist resto bra, echo=FALSE, fig.height=6, fig.width=9}
C <- 1

df <- baseES_filtered %>% filter(country==nam[C]) %>% filter(Service!="Nature Access")
t <- ggplot(df, aes(x = value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Restoration Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Restoration"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
>>>>>>> main
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
<<<<<<< HEAD
t
```

```{r get prportions restoration}

rest <- lapply(rest, function(r){
  r <- project(r, baseES[[1]])
  })


s1 <- list(baseES[[1]],baseES[[6]],baseES[[11]], baseES[[16]])
s2 <- list(baseES[[2]],baseES[[7]],baseES[[12]], baseES[[17]])
s3 <- list(baseES[[3]],baseES[[8]],baseES[[13]], baseES[[18]])
s4 <- list(baseES[[4]],baseES[[9]],baseES[[14]], baseES[[19]])
s5 <- list(baseES[[5]],baseES[[10]],baseES[[15]], baseES[[20]])

#baseEs_ap <- map(baseES, ap, mask)
s11 <- lapply(s1, function(r){
  r <- mask(r,rest[[1]],maskvalues=0)
})
s12 <- lapply(s1, function(r){
  r <- mask(r,rest[[1]],maskvalues=1)
})
s2 <- lapply(s2, function(r){
  r <- mask(r,ap[[2]])
})
s3 <- lapply(s3, function(r){
  r <- mask(r,ap[[3]])
})
s4 <- lapply(s4, function(r){
  r <- mask(r,ap[[4]])
})
s5 <- lapply(s5, function(r){
  r <- mask(r,ap[[5]])
})

```

# Produce land cover change maps

```

### 5.2.2. Madagascar 

```{r plot hist resto mdg, echo=FALSE, fig.height=6, fig.width=9}

C <- 2

df <- baseES_v %>% filter(country==nam[C]) %>% filter(Service!="Nature Access")
t <- ggplot(df, aes(x = value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Restoration Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Restoration"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```

### 5.2.3 Mexico - Yucatan

```{r plot hist resto MX, echo=FALSE, fig.height=6, fig.width=9}
# Calculate the 98th percentile for each combination of Service and Country

C <- 3

df <- baseES_filtered %>% filter(country==nam[C]) %>% filter(Service!="Nature Access")
t <- ggplot(df, aes(x = value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Restoration Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Restoration"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```

### 5.2.4. Peru Madre de Dios

```{r plot r plot hist resto PE, echo=FALSE, fig.height=6, fig.width=9}

C <- 4

df <- baseES_v%>% filter(country==nam[C]) %>% filter(Service!="Nature Access")
t <- ggplot(df, aes(x = value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Restoration Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Restoration"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```
### 5.2.5. Vietnam 

```{r plot r plot hist resto VT, echo=FALSE, fig.height=6, fig.width=9}
# Calculate the 98th percentile for each combination of Service and Country
C <- 5

df <- baseES_filtered %>% filter(country==nam[C]) %>% filter(Service!="Nature Access")
t <- ggplot(df, aes(x = value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `restoration` variable
  labs(
    title = paste("Density Plot of Services Inside and Outside Restoration Areas -", nam[C]),
    x = "Value (Log Scale)",
    y = "Density",
    fill = "Restoration"
  ) +
  scale_x_continuous(trans = 'log10') +  # Apply log10 transformation to x-axis
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
print(t)
```
####################################################3


# 6. Location/Population based Services

Now, i need to do the same thing  for the layers in different cr, nature access, restoration, coastal protection and beneficiaries.

```{r get summaries, eval=FALSE, include=FALSE}
#load protected areas




```


```{r land cover change}

#path to lc classifications
path_lc <- '/Volumes/Jero_HDD/POSDOC/ESA_LC'
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "all"))
lc <- lapply(tf,rast)
lc <- c(lc[[1]],lc[[2]])

#load study areas
poly <- st_read('/Volumes/Jero_HDD/POSDOC/NBS_OPS/vector/areas_nbs.geojson')
nam <- poly$COUNTR
nam <- sort(nam)
poly <- poly%>%split(.$COUNTR)
lc <-map(1:length(poly), function(x) crop(lc, poly[[x]]))
lc <-map(1:length(poly), function(x) mask(lc[[x]], poly[[x]]))
map(1:length(poly), function(x) writeRaster(lc[[x]], paste0(path_lc, '/', 'ESA_LC_rec_', nam[x], '.tif')))

chmaps <- lapply(lc, function(r){
  ch_mapR(r)
})

map(1:length(chmaps), function(x) writeRaster(chmaps[[x]][[1]], paste0(path_lc, '/', 'ESA_LC_change', nam[x], '.tif')), overwite=TRUE)


# pending: set a rule to deal wit those combinations that have to few pixels, not worth the time and effort,

```

####################################################################### 

Second part with justin.

# Explore Data.

The ecorregiosn Raster is not suited, 830 different values. Could be
done but does not make a lot of sense

```{r explore data}

ecoregions <- rast('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/Ecoregions2017_compressed_md5_316061.tif')

ec_uniqe <- unique(ecoregions)
```

Use vector file, just the main biomes.

# PA

```{r filter out PAs}

pa <- st_read('/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/global-2024-05-08.gpkg')

att <- unique(pa$IUCN_CAT)

filt <- c("Not Reported", "Not Assigned", "Not Applicable")
pa <- pa %>% filter(!(IUCN_CAT %in% filt))

st_write(pa, '/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/PA_filt.gpkg')



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

pa <- st_transform(pa, crs=st_crs(countries))

```

split the rasters by biomes

```{r split countries}
wd. <- '/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products'

tf <- list.files(wd., pattern='1992')
tf <- clean_filename(tf)

tf <- tf[-c(2,3,5)]


tiffes <- file.path(wd., list.files(wd., pattern='1992'))
rs <- lapply(tiffes, rast) 

rs <- rs[-3]

biomes <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/global_products/biomes_WWF.gpkg')


nam <- sort(biomes$BIOME)

biomes <- biomes %>% split(.$BIOME)

# Apply cropping and masking recursively using lapply and map
rs_msk <- lapply(rs, function(raster) {
  map(biomes, function(polygon) {
    # Crop the raster to the extent of the polygon
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
})

outpath <- '~/Documents/Natural_capital/Global_mapping/rest_masked_biomes'
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})

summary_df <- map2_dfr(seq_along(rs_msk), rs_msk, function(i, inner_list) {
  
  # i corresponds to the index of the outer list (e.g., 1, 2, 3...)
  outer_label <- paste0("Outer_", i)  # Label for the outer list
  
  # Loop through each SpatRaster in the inner list
  map_dfr(inner_list, function(raster) {
    extract_raster_summary(raster, outer_label)
  })
})

```

# Split by Countries/continents

# mask by protected areas (all togeter, chategories I-VI)

# Reclassify Land Cover maps.

# Change Maps

```{r change_map_calc}
esat <- c('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif','/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif')


esas <- lapply(esat,rast)


nam <- c(2020,1992)

map(1:2, function(x) writeRaster(esas[[x]], paste0('ESA_LC_', nam[x],'.tif')))
cls <- lapply(esas,freq)


wd <- '~/Documents/Natural_capital/NBS_OP/data/ESA_LC'

tf <- file.path(wd, list.files(wd, pattern='tif'))

tf <- tf[c(2,3)]

esas <- lapply(tf,rast)



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

cont <- countries %>% filter(subregion=='Central America')

# Define the reclassification rules in one operation
reclass_table <- data.frame(
  from = c(10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100,
           110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180,
           190, 200, 201, 202, 210, 220),
  to = c(1, 1, 1, 1, 1, 1,   # Cultivated
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # Forests
         3, 3, 3, 3, 3,   # Grasses and Shrubs
         4, 4, 4, 4, 4,   # Sparse Vegetation
         5, 5, 5,   # Mangroves
         6,   # Urban
         7, 7, 7,   # Bare
         8,   # Water
         9)   # Ice
)

esas <- lapply(esas, function(r) {
  subst(r, from = reclass_table$from, to = reclass_table$to)
})


map(1:2, function(x) writeRaster(esas[[x]], paste0('~/Documents/Natural_capital/NBS_OP/data/ESA_LC/', "Forests_esa", year[x], '.tif')))

# Extract forests
esas <- lapply(esas, function(r) {
  subst(r, from = 2, to =1, others=NA)
})


chg <- ch_mapR(esas[[1]],esas[[2]])
freq(chg)

mat_1 <- crosstabm(esas[[1]],esas[[2]])

diffs_p <-  diffTablej(mat, digits=3, analysis = "change")

```

# Get Sankey Diagrams

```{r Sankey Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```

# Next step

1.  Zonal Countries. one per service

mask for the forest

not sum, average.

Historic services.

name(test)

Map 4 maps for each servie for the average change fior that time peiod

map(1:2, function(x) writeRaster(esas[[x]],
paste0('\~/Documents/Natural_capital/NBS_OP/data/ESA_LC/',
"ESA_LC_Rec\_", year[x], '.tif')))

cls [[1]] layer value count 1 1 10 105009519 2 1 11 102087508 3 1 12
2415511 4 1 20 29796402 5 1 30 48088904 6 1 40 44613941 7 1 50 137316150
8 1 60 84649974 9 1 61 11061590 10 1 62 39391781 11 1 70 113472860 12 1
71 45563958 13 1 72 19882 14 1 80 109865531 15 1 81 55745 16 1 82 21 17
1 90 37417117 18 1 100 58048747 19 1 110 15730987 20 1 120 140408175 21
1 121 3007375 22 1 122 32663372 23 1 130 177774130 24 1 140 42447574 25
1 150 155885143 26 1 151 61 27 1 152 1072086 28 1 153 3664319 29 1 160
11677666 30 1 170 2394625 31 1 180 34499895 32 1 190 10933052 33 1 200
247641877 34 1 201 1572727 35 1 202 1244423 36 1 210 5675137546 37 1 220
871449826

So, now what does this mean?

Add column with the class names with multiple aggregation levels (ESA)

```         
Mean: The average value within each zone (e.g., inside/outside protected areas or across land cover types). This provides a simple understanding of the central tendency.
Median: The middle value when data is ordered, useful when your data has outliers that could skew the mean.
Standard Deviation: Indicates the variation or dispersion from the average, helping show if values are clustered or spread out.
Min/Max: The range of values, showing the extremes within each zone.
```

2.  Zonal Statistics (Per Category):

    Sum: The total value across a zone, particularly useful for
    variables like ecosystem services (e.g., total nitrogen retention
    within protected areas). Count of Pixels: The number of pixels in
    each zone, which could give an idea of the area covered by specific
    land covers. Percentage Area Contribution: For each zone or land
    cover class, express the variable as a percentage of the total
    landscape (e.g., “30% of nitrogen retention occurs in protected
    areas”).

3.  Comparisons by Category:

    Inside vs. Outside Protected Areas: You can calculate mean, sum, and
    standard deviation inside and outside protected areas to compare the
    effectiveness of protection measures in preserving ecosystem
    services. Land Cover Comparison: Use zonal means to compare how
    different land cover types contribute to ecosystem services like
    sediment retention or distance to beneficiaries. Express results as
    percentages, e.g., “Forests account for 60% of nitrogen retention.”

4.  Histogram or Distribution Analysis:

    A frequency distribution showing how the values of each variable are
    distributed across the entire study area or within protected vs.
    unprotected zones. This can help show if most of the service is
    concentrated in a few high-value areas or spread out.

5.  Change Detection (If Time Series Exists):

    If your rasters cover different time periods, you could show net
    changes in ecosystem service values (e.g., changes in nitrogen
    retention) inside and outside protected areas or across different
    land covers.

6.  Normalized or Standardized Scores:

    Convert variables into percentile ranks or Z-scores to facilitate
    comparisons across variables that might have different units (e.g.,
    how “high” or “low” the ecosystem service is in relation to the
    mean).

7.  Threshold Analysis:

    For variables where a threshold is meaningful (e.g., distance to
    beneficiaries \< 10 km), you can count the number of pixels above or
    below the threshold in each category (protected areas, land covers).
    For example, "80% of the people dependent on agriculture live within
    5 km of high-sediment retention areas."

8.  Spatial Autocorrelation (Advanced):

    If you're dealing with spatially explicit relationships, calculating
    spatial autocorrelation (Moran’s I) might help quantify clustering
    patterns of ecosystem services inside and outside protected areas.

9.  Cumulative Impact Maps:

    Create a composite map where multiple variables are aggregated to
    show high- vs. low-value areas for ecosystem services. This could
    simplify complex data into an easily interpretable map for
    non-specialists.

These summaries will provide enough detail to highlight key insights,
while also being interpretable by those without technical expertise. Let
me know if you'd like to dive deeper into any specific metric or
approach!

Produyces forest loss maps/gain permanences

Country/continent

Biomes. (Köppen)

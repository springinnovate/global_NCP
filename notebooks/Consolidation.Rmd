---
title: "Current Status"
output: html_notebook
---

```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
library(tidyr)
library(forcats)
library(scales)
library(RColorBrewer)
library(htmltools)
library(leaflet)
library(devtools)
load_all()
#source the helper functions
source(here("R", "hotSpotR.R"))
source(here("R","pct_change_calc.R"))
source(here("R", "utils_lcc_metrics.R"))
source(here("R", "perc_filteR.R"))
```

# 1. Clean and Organize Synthesis Data

Make sure that they are complete and correclty named)

Right now, 24/04/2025 the problem is on the way the dataframe with the class colors and names is built. The process is still too manual (writing the table) and is easy to get it wrong when it should not be big deal. Ideally, there should be a dialogue box to fill this instead. This should take care of the class names, values and colors neatl Also, the order of the factors needs to be set, and new classes have to be added. Again, i don't how to do it easily. I mean, it is easy, but annoying to do and easy to get wrong.

```{r consolidate final outputs}

#Builds a list of files in the vector directory
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data'
sets <- file.path(inpath, list.files(inpath, pattern = "gpkg$"))
# Set and select an index to load the desired dastaset

sets1 <- sets[c(4,5)] # 10 km grid, zonals stats for ES services 1992, 2020), sums and means
sets2 <- sets[c(2,3)] # 10 km grid, zonal stats for beneficiaries  , sums and means (depends on the variable)

# Step 1. Load vectors with the summary stats objects
sf_a <- lapply(sets1, st_read)

# Step 2: Add fid to each (all have same row count & order)
sf_a <- lapply(sf_a, function(x) dplyr::mutate(x, fid = dplyr::row_number()))

# Update: include the coastal prtection data:
sf_cp <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/summary_pipeline_workspace/grid_10k_synth_zonal_2025_07_20_23_45_27.gpkg')
# Define name replacements per object. Had to do it manually, there is just too much variability to automatize, did it by hand, but this is the kind of thing an LLM helps to write easily. Still, there must be a smarter, more flexible way to do it (maybe the best approach is to keep a consistent naming convention for the modeled rasters, this has been a PIA since i started this work)

rename_list <- list(
  c(
    usle_1992_mean = "global_usle_marine_mod_ESA_1992_mean",
    usle_2020_mean = "global_usle_marine_mod_ESA_2020_mean",
    nature_access_1992_mean = "nature_access_lspop2019_ESA1992_mean",
    nature_access_2020_mean = "nature_access_lspop2019_ESA2020._mean",
    n_ret_ratio_1992_mean = "N_ret_ratio_1992_mean",
    n_ret_ratio_2020_mean = "N_ret_ratio_2020_mean",
    sed_ret_ratio_1992_mean = "Sed_ret_ratio_1992_mean",
    sed_ret_ratio_2020_mean = "Sed_ret_ratio_2020_mean"
  ),
  c(
    n_export_1992_sum = "global_n_export_tnc_esa1992_sum",
    n_export_2020_sum = "global_n_export_tnc_esa2020_sum",
    n_retention_1992_sum = "global_n_retention_ESAmar_1992_fertilizer_sum",
    n_retention_2020_sum = "global_n_retention_ESAmar_2020_fertilizer_sum",
    sed_export_1992_sum = "global_sed_export_marine_mod_ESA_1992_sum",
    sed_export_2020_sum = "global_sed_export_marine_mod_ESA_2020_sum",
    pollination_1992_sum = "realized_polllination_on_ag_ESA1992_sum",
    pollination_2020_sum = "realized_polllination_on_ag_ESA2020_sum"
  )
)
sf_a <- Map(function(x, renames) {
  dplyr::rename(x, !!!renames)
}, sf_a, rename_list)

# Calculate bitemporal change in % and absolute terms. Drop original columns
sf_a <- lapply(sf_a, function(x) compute_change(x, suffix = c("_sum", "_mean"), change_type= "both", drop_columns = TRUE))

# Join into a single sf object
sf_f <- sf_a[[1]]

# Step 2: Loop over the rest and join by 'fid'
for (i in 2:length(sf_a)) {
  sf_f <- dplyr::left_join(sf_f, sf_a[[i]] %>% sf::st_drop_geometry(), by = "fid")
}
rm(sf_a)
 # NaN are divisions by 0, NA is when values are NA

# load beneficiaries data. There is no change going on here right now, but maybe in the future. The workflow can handle this
sf_ben <- lapply(sets2,st_read)
# Add fid (primary key)
sf_ben <- lapply(sf_ben, function(x) dplyr::mutate(x, fid = dplyr::row_number()))

sf_benf <- sf_ben[[1]]

# Step 2: Loop over the rest and join by 'fid'
for (i in 2:length(sf_ben)) {
  sf_benf <- dplyr::left_join(sf_benf, sf_ben[[i]] %>% sf::st_drop_geometry(), by = "fid")
}
rm(sf_ben) # remove not needed anymore
# join beneficiary data to the ES change data
sf_f <- dplyr::left_join(sf_f, sf_benf %>% sf::st_drop_geometry(), by = "fid")
rm(sf_benf) 
# export output

st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```
# 1.1 Include new columns (C.Protection)


```{r add c protection}
# Update: include the coastal prtection data:
sf_cp <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/summary_pipeline_workspace/grid_10k_synth_zonal_2025_07_20_23_45_27.gpkg')
# Define name replacements per object. Had to do it manually, there is just too much variability to automatize, did it by hand, but this is the kind of thing an LLM helps to write easily. Still, there must be a smarter, more flexible way to do it (maybe the best approach is to keep a consistent naming convention for the modeled rasters, this has been a PIA since i started this work)
sf_cp <- sf_cp %>% mutate(fid = dplyr::row_number())
# Calculate bitemporal change in % and absolute terms. Drop original columns
sf_cp <- compute_variable_change(sf_cp, suffix = c("_mean"), change_type= "both", drop_columns = TRUE)

sf_cp <- st_drop_geometry(sf_cp)
# load previous version
sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'))
sf_f <-  sf_f %>% mutate(fid = dplyr::row_number()) # not sure why fid is missing. Nevermind, added here 
sf_f <- left_join(sf_f, sf_cp)
# reorganize columns:

sf_f <- sf_f %>%
  relocate(
    coastal_protection_Rt_abs_chg,
    coastal_protection_Rt_pct_chg,
    .before = GHS_BUILT_S_E2020_mean
  )


# export output

st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```
# 2 Load Vector Data and pivot

Here, the spatial objects with the additional attributes are loaded and reformatted for analysis and chart preparation - **pivot & tidy**. Add to the documentation , environment or however that's called the vectors to select and order the columns. This should actually be done as a database structure. The columns live somewhere and are summoned upon need from a set of options (list_dir).

```{r pivot, eval=FALSE, include=FALSE}
# 
#  get pct as standalone data to add to the table
# Most recent version of synthesis data:

 sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'))
 plt <- st_drop_geometry(sf_f)
 
# get the benef vars (all that are not "chg)
socio_vars <- names(plt)[
  names(plt) != "fid" & (plt_)
  !grepl("chg$", names(plt))
]

# Pivot all columns that contain pct_chg, keeping other relevant vars
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, service, pct_chg, all_of(socio_vars)) %>% 
   mutate(service = str_replace(service, "_pct_chg.*", ""))
  
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_chg)) %>% filter(pct_chg != Inf)  


# rename some variables (just for aesthetics)
  plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "n_export" ~ "N_export",
   service == "n_retention" ~ "N_retention",
   service == "coastal_protection_Rt" ~ "Coastal_Protection",
   service == "nature_access" ~ "Nature_Access",
   service == "pollination" ~ "Pollination",
   service == "usle" ~ "USLE",
   service == "n_ret_ratio" ~ "N_Ret_Ratio",
   service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
   TRUE ~ service
   ))

#. Have a method to name versions or something (maybe?)
write.csv(plt, file= here('output_data', "full_metrics_grid_wide.csv")) # This maybe not be necessary anymore there
```

# 3. Get Hotspots

Need to think on an effective way to change the value (the share/quantile we want to extract and also filter by group!!!!)


So, we need a long table format, then define the threshold the services with gains/losses and then (optional) set the filters by groups. Taht wouls be awesome

```{r get_hotspots}

# 1. Identify hotspots by service. Keep present that in some cases the key hotspots are the gains, and in others are the losses. 

# ---------------------------------------------------------
# Identify Hotspots by Service Type (Losses and Damages)
# ---------------------------------------------------------

# Define which services are considered "losses" vs "damages"
loss_services <- c("Coastal_Protection", "Pollination", "N_Ret_Ratio", "Sed_Ret_Ratio", "Nature_Access", "N_retention")
gain_services <- c("Sed_export", "N_export", "USLE")
#pct_cutoff <- 0.01  # Top/bottom 1%
pct_cutoff <- 0.05  # for top/bottom 5%


# STEP 1: Identify Top/Bottom % Hotspots Per Service
df_hotspots <- plt_long %>%
  group_by(service) %>%
  mutate(
    n_total = n(),
    n_cut = ceiling(n_total * pct_cutoff),
    rank_high = rank(-pct_chg, ties.method = "first"),
    rank_low = rank(pct_chg, ties.method = "first"),
    hotspot_flag = case_when(
      rank_high <= n_cut ~ "high",
      rank_low <= n_cut ~ "low",
      TRUE ~ NA_character_
    ),
    hotspot_binary = !is.na(hotspot_flag)
  ) %>%
  ungroup()

# STEP 2: Aggregate Hotspot Summary Per spatial unit (HYBAS_ID for basins, fid for the grid)
hotspot_summary <- df_hotspots %>%
  filter(hotspot_binary) %>%
  group_by(fid) %>%
  summarise(
    hotspot_count = n(),
    hotspot_services = list(unique(service))
  ) %>%
  mutate(
    hotspot_services = sapply(hotspot_services, \(x) paste(trimws(as.character(x)), collapse = ", "))
  )


# STEP 3: Add Hotspot Types and Negative-Impact Subset
hotspot_summary <- hotspot_summary %>%
  mutate(
    # Extract list of services again for classification
    hotspot_services_list = strsplit(hotspot_services, ",\\s*"),
    # Classify each service as loss or damage
    hotspot_types = lapply(hotspot_services_list, function(svcs) {
      sapply(svcs, function(s) {
        if (s %in% loss_services) "loss"
        else if (s %in% gain_services) "damage"
        else NA_character_
      })
    }),
    hotspot_types = sapply(hotspot_types, function(x) paste(x[!is.na(x)], collapse = ", ")),
    # Keep only "negative" services
    hotspot_services_negative = sapply(hotspot_services_list, function(svcs) {
      neg <- svcs[svcs %in% c(loss_services, gain_services)]
      paste(neg, collapse = ", ")
    })
  ) %>%
  select(-hotspot_services_list)  # Optional cleanup


hotspot_1 <- left_join(sf_f, hotspot_summary)
# Export aoutputs (this also needs to be adjusted, it makes no sense to kee storing this inb two differnet places)

st_write(hotspot_1, paste0(inpath, '/',"10k_grid_hotspots.gpkg"), append=FALSE)

################# We're here. Need to finish tomorrow. I need to recall what happened here so i cna prodcue my final scatterplots and the maps. 
#I think everything makes sens, but i just can't think anymore. Need some rest and fresh mind to cathc up here, but it is almost done
services <- unique(plt_long$service)

for (s in services) {
  hotspots_change[[s]] <- ifelse(grepl(s, hotspots_change$hotspot_services_negative), 1, 0)
}

# Drop Geometry 
hotspots_change <- st_drop_geometry(hotspot_1)

###### #### #### Attention, Eventually get the ones we need in terms of per ha/per kmˆ2 but right now it is not necessary. 
# Pivot all columns that contain pct_ch, keeping other relevant vars
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_ch"),
    names_to = "service",
    values_to = "pct_ch"
  ) %>%
  select(HYBAS_ID,SUB_AREA, id, ee_r264_name, iso3, gtapv7_r50_description, area_sqkm_country, continent, region_wb,fao_reg,income_grp, subregion, BIOME, ecoregion, service, dir_ch_2, pct_ch, all_of(socio_vars), hotspot_count,hotspot_services,hotspot_types, hotspot_services_negative) %>% 
   mutate(service = str_replace(service, "_pct_chg.*", ""))
  



# Remove unnecesary columns
hot <- hotspots_change[c(1:19,35:37)]

 # Define the socioeconomic variables
socio_vars <- c(
  "GHS_BUILT_S_E2020_GLOBE_R2023A_4326_3ss_V1_0_mean",# this is also a problem!!!Will have to calculate again, but the sum. not the mean. 
  "rast_gdpTot_1990_2020_30arcsec.7_mean",
  #"GHS_POP_E2020_GLOBE_R2023A_4326_3ss_V1_0.1_mean", # this is a problem here!!!! Will have to calculate again, but the sum. not the mean. 
  #"GlobPOP_Count_30arc_2020_I32.1_sum", we don't need this one anymore, already calc per sq km
  "hdi_raster_predictions_2020.1_mean",
  "farmsize_mehrabi.1_mean",
  "GlobPOP_sqkm")

# Remove NAs and Inf values

###### #### #### Attention, thos will be reviewd for per sq/km and check pollinayion and USLE, but 
# Pivot all columns that contain pct_ch, keeping other relevant vars
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_ch"),
    names_to = "service",
    values_to = "pct_ch"
  ) %>%
  select(HYBAS_ID,SUB_AREA, id, ee_r264_name, iso3, gtapv7_r50_description, area_sqkm_country, continent, region_wb,fao_reg,income_grp, subregion, BIOME, ecoregion, service, dir_ch_2, pct_ch, all_of(socio_vars), hotspot_count,hotspot_services,hotspot_types, hotspot_services_negative) %>% 
   mutate(service = str_replace(service, "_pct_ch.*", ""))
  

plt_long <- plt_long%>% filter(service != "Pop") %>% filter(service!= "n_export")
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_ch)) %>% filter(pct_ch != Inf)  


# join hotspot data with the small plt_long data (8 columns). Th
#plt_long <- left_join(hot, plt_long)

# export final database long. 

write.csv(plt_long, here("output_data", "hotspot_data_long.csv")) 
```

# 4. Join Country Data (improve Analysis)

**I still have not been able to make sense of this shit. And it is really annoyingm anbd a huge waste of time. I absolutely need to transcend this.**

---
title: "Summary ES Analysis"
author: "Jeronimo Rodriguez-Escobar"
date: "2025-02-12"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

# Overview
This document outlines the steps to systematically extract zonal statistics from global **Ecosystem Service (ES)** raster datasets for specific spatial units (e.g., countries, biomes, watersheds, etc.).

## **1. Objectives**
1. Extract zonal statistics from ES global raster datasets using the `exactextractr` package.
2. Integrate extracted statistics as attributes to spatial polygon datasets.
3. Generate visualizations (bar plots, maps) for spatial representation of results.

**Important Considerations:**
- Dependencies and overlapping regions (e.g., GB, France) need special handling.
- Small island nations and unique geographic cases require additional checks.

## **2. Load Required Libraries**
```{r setup, include=FALSE}
library(sf)             # Spatial vector operations
library(dplyr)          # Data manipulation
library(terra)          # Raster processing
library(exactextractr)  # Zonal statistics
library(ggplot2)        # Visualization
library(forcats)        # Factor reordering
library(tidytext)
library(here)           # Managing paths
library(patchwork)      # Combining plots
library(tidyr)
library(parallel)       # Parallel computing
library(purrr)
```

## **3. Load Polygon Data**


This assumes that the polygon files are stored in the same working directory. Feel free to change it if necessary.  The different subsets except for biomes are obtained from the dataset provided by Justin to which a "dissolve by attribute" operation has been performed. This is better done in Python/QGis/ArcGis, R is not great at that. 

```{r load polygons, eval=FALSE, include=FALSE}
# Read spatial polygon dataset
# Set attribute column for analysis
 
# By Continent
set <- 'Continent.gpkg' 
# By Subregion
#set <- 'subregion.gpkg'
# By individual territory; "Country" 
#set <- 'Country.gpkg'
# By WWF biome
# set <- 'Biome.gpkg' 
# by WB Region
#set <- "World_Bank_Region.gpkg"

# load polygons
poly <- st_read(here('vector', set)) 

# select column with the asttribute to iterate through.
#col <- "id"
#col <- "WWF_biome"
col <- 'continent'
#col <- 'subregion'
#col <- 'region_wb'
#col <- 'id'
cols <- col
```

## **4. Load Global ES as Raster Data**


```{r load raster es, include=FALSE}
inpath <- here('input_ES')

tiffes <- file.path(inpath, list.files(inpath, pattern= 'tif$'))
filename <- basename(tiffes)
raster_name <- gsub(".tif$", "", filename)

# Load raster files
rasters_list <- lapply(tiffes, rast)
# Define service names and colors
service <- rep(c("Coastal_Protection", "Nitrogen_Export", "Sediment_Export", "Usle", "Nature_Access", "Pollination"), each=2)
year <- rep(c(1992,2020), 6)
filename <- as_tibble(cbind(service, raster_name, year))

# create a table with the content to fill the table
color <- rep(c("#9e9ac8", "#2c944c", "#08306b", "#17c0ff", "#A57C00", "#dd1c77"), each=2)
cd <- as_tibble(cbind(filename, color))
rm(filename)
cd
```

## **5. Compute Differences**


##### NOT RUN #######

```{r get differences, eval=FALSE, include=FALSE}
outpath <- here("input_ES")
m_index <- length(rasters_list) %/% 2
lab <- unique(service)
lab <- gsub(" ", "_", lab)


l1 <- rasters_list[seq(1,length(rasters_list), by=2)]
l2 <- rasters_list[seq(2,length(rasters_list), by=2)]
 # test with future_map2 next time to get this to run faster. 

diffs <- map2(l2, l1, ~ .x - .y)

# Export difference rasters
map(1:length(diffs), function(x) writeRaster(diffs[[x]], paste0(outpath, "/change_calc/", lab[x], '_diff.tif'),overwrite=TRUE))
```


### 6.1  Prepare Potential of Sed retention data.

```{r getpotential Sed retention , eval=FALSE, include=FALSE}
inpath <- here("input_ES", "Pot_Sed_retention")
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)
# Use terra::rast() to load each file as a SpatRaster object


rasters_list <- lapply(tiffes, rast)
rcl <- matrix(c(
  -Inf, 0, NA   # Any value from 0 to Infinity becomes 1
), ncol = 3, byrow = TRUE)

rasters_list <- lapply(rasters_list, function(r){
  r <- classify(r,rcl, right=FALSE)
})

m_index <- length(rasters_list) %/% 2
#lab <- unique(service)
l1 <- rasters_list[seq(1,length(rasters_list), by=2)]
l2 <- rasters_list[seq(2,length(rasters_list), by=2)]
 # test with future_map2 next time to get this to run faster. 

pot_SedRet1 <- (l1[[2]]-l1[[1]])/l1[[2]]


pot_SedRet2 <- (l2[[2]]-l2[[1]])/l2[[2]]

psr_92 <- classify(pot_SedRet1,rcl, right=FALSE)
psr_20 <- classify(pot_SedRet2,rcl, right=FALSE)

writeRaster(psr_92, paste0(inpath, '/', 'pot_SedRet_92.tif'), overwrite=TRUE)
writeRaster(psr_20, paste0(inpath, '/', 'pot_SedRet_20.tif'), overwrite=TRUE)
diff <- psr_92 - psr_20
writeRaster(diff, paste0(inpath, '/', 'pot_Sed_r_diff.tif'), overwrite)

```


## 7 Prepare the yearly summary data

### 7.1 Compute Zonal Statistics for each target raster/year 

**Note:** Although Exact exactextractr is used  here ot obtain zonal summary statistics, in this case the mean value for each polygon, it can also return a list of dataframes, one per feature with the cell values from pte input raster. This can be very useful for further analysis. 
Besides, it is also possible to include user-defined functions (somerthing that i am going to use for the USLE analysis).


```{r compute stats 1, message=TRUE, include=FALSE}
target_stats <- c("mean")
num.cores <- length(rasters_list) # set as many cores as rasters to be evaluated.  sequentially, and there is no progress bar available 
# here, set a way to change the input poly and columns externally instead of editing inside of the functions. That is cumbersome and prone to errors. 

results_list <- mclapply(rasters_list, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)
  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)},mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df <- do.call(rbind, results_list)

# add color and service columns
zonal_df <- left_join(zonal_df,cd)
# Add the year. this uses the original filenames to extract the year, but might be too hardcoded for generalization 

zonal_df$year <- ifelse(grepl("1992", zonal_df$raster_name), 
                          1992, 
                          ifelse(grepl("2020", zonal_df$raster_name), 2020, NA))


zonal_wide <- pivot_wider(zonal_df, id_cols=all_of(col), names_from=c(service, year), values_from = mean)
# adjust column names #

######### HERE: Include a way/convention to adjust the column names autmatically. Saves work and keeps consistency. I had to do this in Stpe 6, but this is not organized. should be done herer bedore exporting

poly <- left_join(poly, zonal_wide, by= col)
st_write(poly, here('vector', set), append = FALSE) 
write.csv(zonal_df, here('output_data', paste0('zonal_df_',col, '.csv')))

```
### 7.3. Extract zonal stats for the differences.  

In theory, i could get everything done on a single run together but it somehow gets too complicated. The names of the input rasters don't always follow the same sequence, and sometiomes need to be rearranged. 
There are two main cases: dealing with two or more different  sets of layers (eg one for each year). The second a case with on;ly one set (dealing wit the differneces for example). This is somethign that is really not that complicated and could be organized/simplified here, but needs some work that is not priority right now.

I will have to review this part. Ideally, i should get everything done together but the year issue made it tricky. Easier to do each on its owrn, the problem is that there is still too much manual handling. Eventually this should be generalized.

```{r compute stats diff, message=TRUE, include=FALSE}

# set the path and load the rasters with the differences that we calcualted in step 3
#build paths to files
inpath <- here('input_ES')
inpath <- paste0(inpath, "/", 'change_calc')

# load the rasters with the calculated differences 
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)

rasters_list <- lapply(tiffes, rast)
service <- c("Coastal Protection", "Nature Access","Nitrogen Export","Pollination","Sediment Export", "Usle")
service <- gsub(" ", "_", service)

filename<- as_tibble(cbind(service,filename))
# We'll compute mean 
target_stats <- "mean"

# Iterate function over the rasters
num.cores <- length(rasters_list)
results_list <- mclapply(rasters_list, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)

  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)},mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df_diff <- do.call(rbind, results_list)
#Adjust column so i can assemble the whole thing (yearly and difference values) in the same dataframe
cd <- cd %>% mutate(year='diff')

zonal_df_diff <- left_join(zonal_df_diff,cd)
#zonal_df <- zonal_df[-c(4,6)]
zonal_wide <- pivot_wider(zonal_df_diff, id_cols=all_of(cols), names_from=c(service), values_from = mean)
# adjust column names 
colnames(zonal_wide)[colnames(zonal_wide) != cols] <- paste0(colnames(zonal_wide)[colnames(zonal_wide) != cols], "_diff")

poly <- left_join(poly, zonal_wide, by= cols)
st_write(poly, here('vector', set), append = FALSE) 

zonal_df <- rbind(zonal_df, zonal_df_diff)


write.csv(zonal_df, here('output_data', paste0('zonal_df_',col, '.csv')))
```

### 7.X get change in terms of % (do at the end)



This is a WIP. I need to be able to add the numbers as columns at the end....nooo
easier. We are going to calculate them dikrectly fro mthe filtered zonal_df dataframe, but not now. We need to think about the differences calculated and the % of difference. anyway i already have that piece of data. 

```{r extract change in %}

# Function to compute percentage change for all relevant variables in an sf object
calculate_percentage_change <- function(sf_obj) {
  # Identify all variables that exist in both 1992 and 2020
  base_vars <- service#c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination")
  
  # Iterate through each variable to compute percentage change
  for (var in base_vars) {
    col_1992 <- paste0(var, "_1992")  # Column name for 1992
    col_2020 <- paste0(var, "_2020")  # Column name for 2020
    col_pct  <- paste0(var, "_pct_ch")   # New percentage change column
    
    # Check if the required columns exist in the sf object
    if (all(c(col_1992, col_2020) %in% names(sf_obj))) {
      sf_obj <- sf_obj %>%
        mutate(!!col_pct := ((!!sym(col_2020) - !!sym(col_1992)) / !!sym(col_1992)) * 100)
    }
  }
  return(sf_obj)
}

# Apply function to all sf objects in the list
#pols <- lapply(poly, calculate_percentage_change)
poly <- calculate_percentage_change(poly)
# Check the updated column names

# get pct as standalone data to add to the table

plt <- st_drop_geometry(poly)
plt  <- plt %>%
  select(1, contains("pct"))

write.csv(plt, here('output_data', paste0('pct_chg_',cols, '.csv')))

#st_write(poly, here('vector', set), append = FALSE) 
```


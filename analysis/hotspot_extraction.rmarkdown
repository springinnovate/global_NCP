---
title: "Ecosystem Service Change & Hotspot Extraction"
subtitle: "v{{< meta analysis_version >}}"
author: "Jerónimo Rodríguez Escobar"
email: "jeronimo.rodriguez@wwfus.org"
date: last-modified
date-format: "YYYY-MM-DD HH:mm zzz"
analysis_version: "v1.0.2"
format:
  html:
    toc: true
    number-sections: true
    code-fold: false
    toc-title: "Contents"
    theme: cosmo
  pdf:
    toc: true
    number-sections: true
    geometry: margin=1in
    df-print: kable
editor: source

params:
  analysis_version: "v1.0.2"
---

## Overview

Derives hotspot flags and produces bar/violin plots summarizing change by subregion.
Assumes Consolidation.qmd has been run first to produce the canonical grid + change dataset (10k_change_calc.gpkg).
Key steps: load the canonical grid, pivot change fields to long form, define hotspots using the configured thresholds, export hotspot layers, and save plots to outputs/plots/.

::: {.callout-tip icon="true"}
## Future Tasks & Ideas

Here are some ideas and future tasks for this analysis:

1.  **Adapt analysis for multi-temporal data:** Adapt analysis to handle updated modeled ES layers and multiple points in time (beyond bi-temporal T0, T1). Strategize for incorporating multi-temporal data.
2.  **Quantify hotspot vs. non-hotspot change:** Develop a method to quantify and visualize the share of total change (from bar plots) that occurs within hotspots versus outside of them, possibly using stacked bar plots.
:::

## Setup and Libraries

Load all core packages (tidyverse, spatial, plotting helpers) and initialize reproducibility settings. This chunk also sources `R/paths.R` and runs `devtools::load_all()` so downstream chunks can reuse helper functions.

```{r}
#| label: setup
#| message: false
#| warning: false
#| echo: false

# Core tidy + spatial
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(forcats)
library(ggplot2)
library(scales)
library(sf)
library(terra)
library(exactextractr)
library(here)

# Plot helpers
library(viridisLite)
library(ggnewscale)
library(ragg)

# Dev/project helpers
library(devtools)   # for load_all()
# Paths helper (set GLOBAL_NCP_DATA in ~/.Renviron first)
knitr::opts_knit$set(root.dir = here::here())
source(here::here("R","paths.R"))
# Optional viewers (enable only if you use them)
# library(httpgd)
# library(leaflet)
# library(htmltools)

# ---- Global options / performance ---------------------------------------
options(dplyr.summarise.inform = FALSE)
sf::sf_use_s2(TRUE)
Sys.setenv(GDAL_NUM_THREADS = "ALL_CPUS", PROJ_NETWORK = "ON")
# terraOptions(tempdir = file.path(tempdir(), "terra_tmp"))  # comment out, or:
dir.create(file.path(tempdir(), "terra_tmp"), recursive = TRUE, showWarnings = FALSE)



set.seed(1)

# ---- Project wiring ------------------------------------------------------
# Load package-style functions from R/ (if this is a package-ish repo)
devtools::load_all(quiet = TRUE)

# ---- Python (enable when needed) ----------------------------------------
library(reticulate)
use_virtualenv("/home/jeronimo/venvs/coastal_snap_env", required = TRUE)

```

## Metadata Banner

Published: `r format(Sys.time(), "%Y-%m-%d %H:%M %Z")`

```{r}
#| label: run-metadata
#| echo: false
#| message: false
#| warning: false
#| results: 'asis'

# Prints version/time/git/data-root info so renders are traceable (no heavy compute).

analysis_version <- tryCatch(params$analysis_version, error = function(e) NULL)
if (is.null(analysis_version) || is.na(analysis_version)) {
  analysis_version <- Sys.getenv("ANALYSIS_VERSION", unset = "dev")
}

# Git info (robust to non-git folders)
git_branch <- tryCatch(
  system2("git", c("rev-parse", "--abbrev-ref", "HEAD"), stdout = TRUE),
  error = function(e) NA_character_
)
git_commit <- tryCatch(
  system2("git", c("rev-parse", "--short", "HEAD"), stdout = TRUE),
  error = function(e) NA_character_
)

# Data root from your paths helper, with fallbacks
data_root <- tryCatch(data_dir(), error = function(e) NULL)
if (is.null(data_root)) {
  data_root <- Sys.getenv("GLOBAL_NCP_DATA", unset = "")
}
message("data_dir(): ", data_dir())
message("Outputs root: ", out_plots())
cat(paste0(
  "::: callout-note\n",
  "**Run metadata**\n\n",
  "- Analysis version: ", analysis_version, "\n",
  "- Rendered: ", format(Sys.time(), "%Y-%m-%d %H:%M %Z"), "\n",
  "- Git: ", if (!is.na(git_branch)) git_branch else "NA", " @ ",
                 if (!is.na(git_commit)) git_commit else "NA", "\n",
  "- Data root: ", if (nzchar(data_root)) data_root else "unset", "\n",
  ":::"
))
```

```{r}
#| label: svc-order-global
#| include: false
#| echo: false
if (!exists("svc_order", inherits = TRUE)) {
  svc_order <- c(
    "C_Risk","N_export","Sed_export",
    "C_Risk_Red_Ratio","N_Ret_Ratio","Sed_Ret_Ratio",
    "Pollination","Nature_Access"
  )
}
if (!exists("canonical_lookup", inherits = TRUE)) {
  canonical_lookup <- c(
    sed_export       = "Sed_export",
    n_export         = "N_export",
    n_retention      = "N_retention",
    nature_access    = "Nature_Access",
    pollination      = "Pollination",
    usle             = "USLE",
    n_ret_ratio      = "N_Ret_Ratio",
    sed_ret_ratio    = "Sed_Ret_Ratio",
    coastal_protection_rt            = "C_Risk",
    coastal_protection_rt_nohab_all  = "C_Risk_NoHab",
    coastal_protection_rt_ratio      = "C_Risk_Red_Ratio",
    rt_ratio         = "C_Risk_Red_Ratio",
    rt               = "C_Risk",
    c_risk           = "C_Risk",
    c_risk_red_ratio = "C_Risk_Red_Ratio",
    rt_service       = "C_Prot_service",
    rt_nohab         = "Rt_nohab"
  )
}
```



## Produce signed change bars

Bars are computed directly from `processed/change_calc.gpkg`. We output two variants—zeros kept vs zeros dropped—both with trimmed tails and a dashed global mean. Pivoted `plt_long` is only necessary for hotspots/KS. Trim/no-trim diagnostics and sign-flip tests live in `analysis/hotspot_sandbox.qmd`.

What this chunk does:
- Read the processed GPKG and normalize IDs (`fid`, `c_fid`), dropping any `c_fid.x/y` artifacts.
- Build a service mapping (`svc_map`) so raw column bases (with/without `_mean`) map to the canonical services; construct `col_pct`/`col_abs` names.
- `agg_by_group()`: intersect requested `groupings` with available columns; for each grouping × service, pick the pct/abs change column, handle `Inf` pct values (`handle_inf`), optionally drop zeros (`drop_zero`), trim extremes at `cut_q`, then compute mean change by group. Returns tidy rows with `grouping, group, service, metric, mean_chg`.
- Run `agg_by_group()` two ways: trimmed keep0/drop0 → `signed_keep0`, `signed_drop0`.
- A later chunk computes global reference means; plotting is in its own chunk so you can skip it when debugging.

```{r}
#| label: make-signed-bars-data
#| eval: true
#| message: false
#| warning: false

services <- svc_order
groupings <- c("income_grp","region_wb","continent","region_un","WWF_biome")
cut_q <- 0.999
handle_inf <- "na"  # options: "na", "cap"
verbose <- TRUE
# helper for lightweight progress messages

vmsg <- function(...) if (isTRUE(verbose)) message(...)

# Load canonical grid + change data from Consolidation.qmd output.
# This file is the single source of truth for grid attributes and grouping joins.
gpkg <- file.path(data_dir(), "processed", "10k_change_calc.gpkg")
if (!file.exists(gpkg)) {
  stop("Consolidation.qmd must be run first to produce 10k_change_calc.gpkg\nExpected at: ", gpkg)
}

vmsg("Reading canonical grid + change: ", gpkg)
sf_f <- sf::st_read(gpkg, quiet = TRUE)
sf_f <- sf::st_drop_geometry(sf_f)

# Filter out Antarctica and Seven Seas from the dataset as they contain no relevant data for this analysis
if ("continent" %in% names(sf_f)) {
  sf_f <- dplyr::filter(sf_f, !continent %in% c("Antarctica", "Seven seas (Open Ocean)"))
}

# Ensure required keys present; rebuild if missing (e.g. if dropped during IO)
if (!"fid" %in% names(sf_f)) {
  sf_f$fid <- seq_len(nrow(sf_f))
}
if (!"c_fid" %in% names(sf_f)) {
  sf_f$c_fid <- sf_f$fid
}

# Ensure required keys present (Consolidation.qmd guarantees this)
stopifnot("fid" %in% names(sf_f), "c_fid" %in% names(sf_f))

# build mapping; include both raw base and no-_mean base, with ready-to-use columns
chg_cols <- grep("_(abs|pct)_chg$", names(sf_f), value = TRUE)
vmsg("Found ", length(chg_cols), " change columns; mapping services...")
services_raw   <- unique(sub("_(abs|pct)_chg$", "", chg_cols))     # as in file (may include _mean)
services_clean <- stringr::str_remove(services_raw, "_mean$")
services_lower <- tolower(services_clean)
svc_map <- tibble::tibble(
  col_base  = c(services_raw, services_clean),
  canonical = dplyr::recode(c(services_lower, services_lower),
                            !!!canonical_lookup, .default = c(services_clean, services_clean))
) |>
  dplyr::distinct() |>
  dplyr::mutate(col_pct = paste0(col_base, "_pct_chg"),
                col_abs = paste0(col_base, "_abs_chg"))

agg_by_group <- function(df, services, groupings, cut_q = 0.999,
                         handle_inf = c("na","cap"), drop_zero = FALSE) {
  handle_inf <- match.arg(handle_inf)
  groupings <- intersect(groupings, names(df))
  vmsg("Groupings available: ", paste(groupings, collapse = ", "))
  out <- list()
  for (g in groupings) {
    vmsg("  Grouping: ", g)
    for (svc in services) {
      map_rows <- dplyr::filter(svc_map, canonical == svc)
      if (!nrow(map_rows)) next
      cols_pct <- map_rows$col_pct[map_rows$col_pct %in% names(df)]
      cols_abs <- map_rows$col_abs[map_rows$col_abs %in% names(df)]
      if (!length(cols_pct) && !length(cols_abs)) next

      add_one <- function(col, metric) {
        v <- df[[col]]
        if (metric == "pct" && any(is.infinite(v))) {
          if (handle_inf == "na") v[is.infinite(v)] <- NA_real_
        }
        if (isTRUE(drop_zero)) v[v == 0] <- NA_real_
        cap <- stats::quantile(abs(v), cut_q, na.rm = TRUE)
        v_trim <- pmax(pmin(v, cap), -cap)
        tibble::tibble(
          service = svc,
          group   = df[[g]],
          metric  = metric,
          val     = v_trim
        )
      }

      rows <- list()
      if (length(cols_pct)) rows[[length(rows)+1]] <- add_one(cols_pct[1], "pct")
      if (length(cols_abs)) rows[[length(rows)+1]] <- add_one(cols_abs[1], "abs")
      rows <- dplyr::bind_rows(rows)
      if (!nrow(rows)) next
      rows <- rows |>
        dplyr::filter(!is.na(group)) |>
        dplyr::group_by(service, metric, group) |>
        dplyr::summarise(mean_chg = mean(val, na.rm = TRUE), .groups = "drop") |>
        dplyr::mutate(grouping = g)
      out[[length(out)+1]] <- rows
    }
  }
  dplyr::bind_rows(out)
}

signed_keep0 <- agg_by_group(sf_f, services, groupings, cut_q = cut_q,
                             handle_inf = handle_inf, drop_zero = FALSE)
signed_drop0 <- agg_by_group(sf_f, services, groupings, cut_q = cut_q,
                             handle_inf = handle_inf, drop_zero = TRUE)
print(dplyr::count(signed_keep0, grouping, metric, service))
print(dplyr::count(signed_drop0, grouping, metric, service))
```

```{r}
#| label: compute-global-refs
#| eval: true
#| message: false
#| warning: false
#| echo: false

compute_global_refs <- function(df, services, cut_q = 0.999,
                                handle_inf = c("na","cap"), drop_zero = FALSE) {
  handle_inf <- match.arg(handle_inf)
  vmsg("Computing global refs: drop_zero = ", drop_zero)
  out <- list()
  for (svc in services) {
    map_rows <- dplyr::filter(svc_map, canonical == svc)
    if (!nrow(map_rows)) next
    for (metric in c("pct","abs")) {
      cols <- if (metric == "pct") map_rows$col_pct else map_rows$col_abs
      cols <- cols[cols %in% names(df)]
      if (!length(cols)) next
      v <- df[[cols[1]]]
      if (metric == "pct" && any(is.infinite(v))) {
        if (handle_inf == "na") v[is.infinite(v)] <- NA_real_
      }
      if (isTRUE(drop_zero)) v[v == 0] <- NA_real_
      cap <- stats::quantile(abs(v), cut_q, na.rm = TRUE)
      v_trim <- pmax(pmin(v, cap), -cap)
      out[[length(out)+1]] <- tibble::tibble(
        service = svc,
        metric  = metric,
        ref     = mean(v_trim, na.rm = TRUE)
      )
    }
  }
  dplyr::bind_rows(out)
}

glob_keep0 <- compute_global_refs(sf_f, services, cut_q = cut_q,
                                  handle_inf = handle_inf, drop_zero = FALSE)
glob_drop0 <- compute_global_refs(sf_f, services, cut_q = cut_q,
                                  handle_inf = handle_inf, drop_zero = TRUE)
```

```{r}
#| label: make-signed-bars-plot
#| eval: true
#| message: false
#| warning: false
#| echo: false

plot_signed_alt <- function(df, grouping, metric,
                            refs,
                            variant_label = "keep0",
                            variant_desc  = "Zeros kept",
                            out_dir = "outputs/plots/signed_alt") {
  d <- dplyr::filter(df, grouping == !!grouping, metric == !!metric)
  if (!nrow(d)) return(invisible(NULL))
  d$service <- factor(d$service, levels = services)
  d$group   <- factor(d$group, levels = rev(sort(unique(d$group))))
  refs_use <- dplyr::filter(refs, metric == !!metric)
  refs_use$service <- factor(refs_use$service, levels = services)
  p <- ggplot(d, aes(x = mean_chg, y = group, fill = group)) +
    geom_col(show.legend = FALSE) +
    geom_vline(xintercept = 0, color = "#7f7f7f", linewidth = 0.6) +
    geom_vline(data = refs_use, aes(xintercept = ref),
               linetype = "dashed", color = "#4a4a4a", linewidth = 0.5) +
    facet_wrap(~ service, scales = "free_x", ncol = 3) +
    labs(title = paste0("Signed mean change (alt) — ", grouping, " [", metric, "]"),
         subtitle = variant_desc,
         y = NULL,
         x = if (metric == "pct") "Mean % change (trimmed, signed)" else "Mean absolute change (trimmed, signed)") +
    theme_minimal(base_size = 12) +
    theme(strip.background = element_rect(fill = "#f3f4f6", color = NA),
          strip.text = element_text(face = "bold"))
  dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)
  fp <- file.path(out_dir, paste0("bars_signed_alt_", variant_label, "_", tolower(grouping), "_", metric, ".png"))
  ggsave(fp, p, width = 12, height = 8, dpi = 300, bg = "white")
  message("Saved alt bars: ", fp)
}

# Write both variants without touching current outputs
for (g in unique(signed_keep0$grouping)) {
  for (m in unique(signed_keep0$metric)) {
    vmsg("Writing alt bars (keep zeros): ", g, " / ", m)
    plot_signed_alt(signed_keep0, g, m,
                    refs = glob_keep0,
                    variant_label = "keep0",
                    variant_desc  = "Zeros kept; dashed = global mean",
                    out_dir = "outputs/plots/signed_alt_keep0")
    vmsg("Writing alt bars (drop zeros): ", g, " / ", m)
    plot_signed_alt(signed_drop0, g, m,
                    refs = glob_drop0,
                    variant_label = "drop0",
                    variant_desc  = "Zeros dropped; dashed = global mean",
                    out_dir = "outputs/plots/signed_alt_drop0")
  }
}
```

### QA: Check for sign flips in change metrics

This section addresses potential sign flips where the sign of the percentage change (`_pct_chg`) might differ from the absolute change (`_abs_chg`).

**Update:** A comprehensive diagnostic test was run using `analysis/diagnostic_sign_flip.R` on the full QA dataset. The test confirmed that:
1. All `_abs_chg` values match `(Year_2020 - Year_Base)` exactly.
2. No sign flips or calculation anomalies were found in the sample.
3. Input service values are non-negative.

The chunk below is kept for reference but set to `eval: false`.

```{r}
#| label: qa-sign-flips
#| eval: false
#| echo: true

# Purpose: Detect and report sign flips between absolute and percentage change metrics.
# This helps diagnose counter-intuitive results when baseline values are negative.

qa_gpkg_path <- file.path(data_dir(), "processed", "10k_grid_ES_change_benef.gpkg")

if (!file.exists(qa_gpkg_path)) {
  warning("QA geopackage not found at: ", qa_gpkg_path, ". Skipping sign flip check.")
} else {
  message("Loading full QA data for sign flip check from: ", qa_gpkg_path)
  qa_df <- sf::st_read(qa_gpkg_path, quiet = TRUE)
  qa_df <- sf::st_drop_geometry(qa_df)

  # Get all change columns
  abs_chg_cols <- names(qa_df)[grepl("_abs_chg$", names(qa_df))]

  sign_flip_found <- FALSE

  for (abs_col in abs_chg_cols) {
    pct_col <- sub("_abs_chg$", "_pct_chg", abs_col)

    if (pct_col %in% names(qa_df)) {

      sign_flips_df <- qa_df %>%
        dplyr::filter(sign(.data[[abs_col]]) * sign(.data[[pct_col]]) == -1)

      if (nrow(sign_flips_df) > 0) {
        sign_flip_found <- TRUE
        base_name <- sub("_abs_chg$", "", abs_col)
        message(paste0("WARNING: Found ", nrow(sign_flips_df), " rows with sign flips for service: '", base_name, "'"))

        # Recalculate baseline to demonstrate the cause
        diagnostic_df <- sign_flips_df %>%
          dplyr::mutate(
            recalculated_t0 = .data[[abs_col]] / (.data[[pct_col]] / 100),
            recalculated_t1 = .data[[abs_col]] + recalculated_t0
          ) %>%
          dplyr::select(
            fid,
            recalculated_t0,
            recalculated_t1,
            dplyr::all_of(abs_col),
            dplyr::all_of(pct_col)
          )

        print(head(diagnostic_df))
      }
    }
  }

  if (!sign_flip_found) {
    message("No sign flips detected between absolute and percentage change metrics.")
  }
}
```


## Load cached long table (plt_long) for hotspots/KS

::: callout-note
**Data sharing and external validation**

The pivoted long table (`plt_long`) is exported as an RDS file to the processed data directory (e.g., `/home/jeronimo/data/global_ncp/processed/plt_long.rds`). This file contains all per-cell, per-service change metrics and is the canonical input for hotspot and KS analyses.

- **For R users:** Load with:
  ```r
  source("R/config.R") # to load cfg object
  plt_long <- readRDS(file.path(cfg$paths$processed_data_dir, "plt_long.rds"))
  ```
- **For Python users:** Use `pyreadr` (`pip install pyreadr`), e.g.:
  ```python
  import pyreadr
  # Note: You may need to define the path manually if not running in an R-integrated environment
  processed_data_dir = "/home/jeronimo/data/global_ncp/processed"
  result = pyreadr.read_r(f'{processed_data_dir}/plt_long.rds')
  df = result[None]
  ```
- **For other tools:** RDS is a binary format; consider converting to CSV or Feather for broader compatibility if needed.

This object is suitable for external review, reproducibility checks, and further interpretation in any suitable software.
:::

Hotspot export and KS still expect `plt_long`. To avoid the heavy pivot here, we load the cached table from the processed data directory. If it’s missing, this script can regenerate it.

```{r}
#| label: pivot
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| cache: false

# Generate plt_long directly from sf_f to ensure freshness and consistency
# This replaces the need to run scratch/pivot_long.R separately.

# 1. Ensure source data (sf_f) is available
if (!exists("sf_f")) {
  gpkg <- file.path(data_dir(), "processed", "10k_change_calc.gpkg")
  if (!file.exists(gpkg)) {
    stop("Canonical change file not found: ", gpkg)
  }
  message("Loading canonical change data for pivoting...")
  sf_f <- sf::st_read(gpkg, quiet = TRUE)
  sf_f <- sf::st_drop_geometry(sf_f)

  if ("continent" %in% names(sf_f)) {
    sf_f <- dplyr::filter(sf_f, !continent %in% c("Antarctica", "Seven seas (Open Ocean)"))
  }
  if (!"fid" %in% names(sf_f)) {
    sf_f$fid <- seq_len(nrow(sf_f))
  }
  if (!"c_fid" %in% names(sf_f)) {
    sf_f$c_fid <- sf_f$fid
  }
}

# 2. Define grouping columns to keep
#    (Must match what is used in make-signed-bars-data and hotspots_config)
pivot_groupings <- c("income_grp","region_wb","continent","region_un","WWF_biome","country_name")
pivot_groupings <- intersect(pivot_groupings, names(sf_f))

# 3. Identify change columns
chg_cols <- grep("_(abs|pct)_chg$", names(sf_f), value = TRUE)

message("Pivoting ", length(chg_cols), " change columns to long format...")

# 4. Pivot to long format
#    Structure: fid, c_fid, <groupings>, service, abs_chg, pct_chg
plt_long <- sf_f |>
  dplyr::select(dplyr::any_of(c("fid", "c_fid", pivot_groupings)), dplyr::all_of(chg_cols)) |>
  tidyr::pivot_longer(
    cols = dplyr::all_of(chg_cols),
    names_to = c("service", ".value"),
    names_pattern = "^(.*)_(abs|pct)_chg$"
  ) |>
  # Normalize service names to canonical forms (matching HOTS_CFG)
  dplyr::mutate(service = tolower(service)) |>
  dplyr::mutate(service = dplyr::recode(service, !!!canonical_lookup)) |>
  dplyr::rename(abs_chg = abs, pct_chg = pct) |>
  dplyr::mutate(service = as.factor(service))

# 5. Cache the result (optional but helpful for debugging)
rds_path <- file.path(cfg$paths$processed_data_dir, "plt_long.rds")
dir.create(dirname(rds_path), recursive = TRUE, showWarnings = FALSE)
saveRDS(plt_long, rds_path)
message("plt_long generated and cached to: ", rds_path)
```

## Hotspot extraction workflow (global + subregional)

We identify per-service hotspots using a 5% percentile rule and **direction vectors**: - `loss_services` (e.g., Nature_Access, Pollination, N/Sed_Ret_Ratio, C_Risk_Red_Ratio) → we flag the **lowest** values; - `gain_services` (Sed_export, N_export, C_Risk) → we flag the **highest** values.

We run this **once globally** and then **once per subregion** (World Bank region, income group, continent, UN region, WWF biome). For each run and for each metric (absolute and percent change) we write a compact **GPKG** containing only the hotspot cells, plus a CSV index:

-   Output root: `processed/hotspots/`
    -   `abs/global/hotspots_global_abs.gpkg`
    -   `pct/global/hotspots_global_pct.gpkg`
    -   `abs/<group_col>/hotspots_<group_col>_<group_val>_abs.gpkg`
    -   `pct/<group_col>/hotspots_<group_col>_<group_val>_pct.gpkg`
-   Index: `processed/hotspots/_hotspots_index.csv` (columns: scope, group_col, group_val, metric, n_hot, gpkg).

These files are meant for QGIS/QA and downstream stats (e.g., KS) without recomputing hotspots.


### Hotspot rules & export configuration

The analysis uses a single, central configuration so the hotspot rules are consistent everywhere:

Thresholding: we flag hotspots using the top/bottom tails of the distribution per service. Here we use a percentile cutoff (e.g., 5%) rather than a fixed count.

Direction of concern: services in loss are “worse when they go down” (we keep the lowest tail); services in gain are “worse when they go up” (we keep the highest tail).

Combos (optional): grouped service sets that we count per cell for quick composite summaries.

Export switches: choose whether to write GPKGs and/or the CSV index.

```{r}
#| label: hotspots_config
#| include: true
#| eval: true
HOTS_CFG <- list(
  analysis_name   = "global_NCP_hotspots",
  pct_cutoff      = 0.05,
  threshold_mode  = "percent",
  rule_mode       = "vectors",
  loss = c("Nature_Access","Pollination","N_Ret_Ratio","Sed_Ret_Ratio","C_Risk_Red_Ratio"),
  gain = c("Sed_export","N_export","C_Risk"),
  combos = list(
    deg_combo = c("Nature_Access","Pollination","N_export","Sed_export","C_Risk"),
    rec_combo = c("Nature_Access","Pollination","N_Ret_Ratio","Sed_Ret_Ratio","C_Risk_Red_Ratio")
  ),
# centralize the grouping columns here
  groupings = c("income_grp","region_wb","continent","region_un","WWF_biome","country_name"),
  # IO
  write_layers = TRUE,
  write_index  = TRUE,
  out_dir      = file.path(data_dir(), "processed", "hotspots")
)
```

```{r}
#| label: show_hotspots_config
#| echo: false
loss_txt <- paste(HOTS_CFG$loss, collapse = ", ")
gain_txt <- paste(HOTS_CFG$gain, collapse = ", ")
grp_txt <- paste(HOTS_CFG$groupings, collapse = ", ")
combo_txt <- if (length(HOTS_CFG$combos))
  paste(paste0("**", names(HOTS_CFG$combos), "**: ",
               vapply(HOTS_CFG$combos, \(v) paste(v, collapse=", "), "")),
        collapse = "<br>") else "None"

cat(paste0(
"::: callout-note\n",
"**Hotspot configuration**\n\n",
"- Cutoff: ", HOTS_CFG$pct_cutoff * 100, "% (", HOTS_CFG$threshold_mode, ")\n",
"- Rule mode: ", HOTS_CFG$rule_mode, "\n",
"- Loss services: ", loss_txt, "\n",
"- Gain services: ", gain_txt, "\n",
"- Combos: ", combo_txt, "\n",
"…\n- Groupings: ", grp_txt, "\n…",
"- Write layers: ", HOTS_CFG$write_layers, " | Write index: ", HOTS_CFG$write_index, "\n",
"- Output dir: `", HOTS_CFG$out_dir, "`\n",
":::"
))
```

### Validate hotspot configuration

```{r}
#| label: validate_hotspots_config
#| message: false
#| warning: false
#| eval: false
# stopifnot(exists("plt_long"))
# svc_all <- unique(plt_long$service)
# if (length(intersect(HOTS_CFG$loss, HOTS_CFG$gain)) > 0) {
#   stop("A service appears in BOTH `loss` and `gain`. Fix HOTS_CFG.")
# }
# miss_loss <- setdiff(HOTS_CFG$loss, svc_all)
# miss_gain <- setdiff(HOTS_CFG$gain, svc_all)
# if (length(miss_loss) > 0 || length(miss_gain) > 0) {
#   warning("Services in HOTS_CFG not found in `plt_long$service`:\n",
#           if (length(miss_loss)) paste0("  - missing loss: ", paste(miss_loss, collapse=", "), "\n"),
#           if (length(miss_gain)) paste0("  - missing gain: ", paste(miss_gain, collapse=", "), "\n"))
# }
```

### Export hotspot layers

::: callout-note
**Hotspot export module**\
- Computes hotspot cells once (global + by subregion) for ABS and PCT change.\
- Writes compact GPKGs for mapping/QA and maintains `_hotspots_index.csv`.\
- Prereqs: `plt_long` in memory, `HOTS_CFG` defined (loss/gain/combos/etc.).
:::

```{r}
#| label: hotspots_export
#| eval: false
#| message: true
#| warning: false
#| echo: false
#| include: false
#| cache: false

stopifnot(exists("plt_long"))

cat("Hotspot export target directory: ", HOTS_CFG$out_dir, "\n", file = stderr())
cat("plt_long rows: ", nrow(plt_long), "\n", file = stderr())

# Diagnostic: Check service name alignment
svc_in_data <- unique(as.character(plt_long$service))
svc_in_cfg <- unique(c(HOTS_CFG$loss, HOTS_CFG$gain))
cat("Services in data: ", paste(head(svc_in_data, 10), collapse=", "), "\n", file = stderr())
cat("Services in CFG:  ", paste(head(svc_in_cfg, 10), collapse=", "), "\n", file = stderr())
cat("Services missing from CFG: ", paste(setdiff(svc_in_data, svc_in_cfg), collapse=", "), "\n", file = stderr())

# ---- Geometry: prefer an in-memory slim grid, else build it --------------
if (exists("grid_sf") && inherits(grid_sf, "sf")) {
  geom_sf <- dplyr::select(grid_sf, dplyr::any_of(c("fid", "c_fid")))
} else {
  gpkg_grid <- file.path(data_dir(), "processed", "10k_change_calc.gpkg")
  stopifnot(file.exists(gpkg_grid))
  geom_sf <- sf::st_read(gpkg_grid, quiet = TRUE) |>
    dplyr::select(dplyr::any_of(c("fid", "c_fid", "continent")), dplyr::everything())
}

# Ensure we filter Antarctica to match the logic used for plt_long (sf_f)
if ("continent" %in% names(geom_sf)) {
  geom_sf <- dplyr::filter(geom_sf, continent != "Antarctica")
}

if (!"fid" %in% names(geom_sf)) {
  geom_sf$fid <- seq_len(nrow(geom_sf))
}
if (!"c_fid" %in% names(geom_sf)) {
  geom_sf$c_fid <- geom_sf$fid
}
geom_sf <- dplyr::select(geom_sf, fid, c_fid)  # keep it slim
stopifnot("fid" %in% names(geom_sf), !any(duplicated(geom_sf$fid)))

# Diagnostic: check ID overlap
n_overlap <- length(intersect(unique(plt_long$c_fid), unique(geom_sf$c_fid)))
cat("Diagnostic: plt_long unique c_fids=", length(unique(plt_long$c_fid)), " geom_sf unique c_fids=", length(unique(geom_sf$c_fid)), " Overlap=", n_overlap, "\n", file = stderr())

# ---- Helper: safe slug for filenames -------------------------------------
slug <- function(x) {
  x <- gsub("[^A-Za-z0-9]+", "_", x)
  x <- gsub("_+", "_", x)
  sub("^_|_$", "", x)
}

# ---- Core runner (uses central HOTS_CFG) ----------------------------------
run_one_hotset <- function(df, value_col, scope,
                           group_col = NA_character_, group_val = NA_character_,
                           sf_obj = geom_sf,
                           cfg = HOTS_CFG) {
  # Optional subsetting by a specific group value
  if (!is.na(group_col) && !is.na(group_val)) {
    df <- df[df[[group_col]] %in% group_val, , drop = FALSE]
  }
  n_total <- nrow(df)
  if (nrow(df) == 0L) {
    return(tibble::tibble(scope, group_col, group_val = as.character(group_val),
                          metric = if (identical(value_col, "abs_chg")) "abs" else "pct",
                          n_hot = 0L, n_total = 0L, pct_hot = 0, gpkg = NA_character_))
  }

  # Safety: geometry must have all fids present in df
  stopifnot("fid" %in% names(df), "fid" %in% names(sf_obj))
  if (!all(df$fid %in% sf_obj$fid)) {
    missing <- setdiff(unique(df$fid), sf_obj$fid)
    stop(sprintf("Geometry is missing %d fid(s), e.g. %s",
                 length(missing), paste(head(missing, 5), collapse = ", ")))
  }

  # Single source of truth for rules/directions/combos
  hs <- extract_hotspots(
    df             = df,
    value_col      = value_col,
    pct_cutoff     = cfg$pct_cutoff,
    threshold_mode = cfg$threshold_mode,
    rule_mode      = cfg$rule_mode,
    loss_services  = cfg$loss,
    gain_services  = cfg$gain,
    combos         = cfg$combos,
    id_cols        = c("c_fid"),
    sf_obj         = sf_obj,
    write_sf_path  = NULL,
    clean_names    = TRUE
  )

  # Output layout
  out_root <- file.path(data_dir(), "processed", "hotspots")
  metric_stub <- if (identical(value_col, "abs_chg")) "abs" else "pct"
  folder <- if (is.na(group_col)) file.path(out_root, metric_stub, "global")
            else                   file.path(out_root, metric_stub, tolower(group_col))
  dir.create(folder, recursive = TRUE, showWarnings = FALSE)

  file_stub <- if (is.na(group_col)) {
    sprintf("hotspots_global_%s", metric_stub)
  } else {
    sprintf("hotspots_%s_%s_%s", tolower(group_col), slug(group_val), metric_stub)
  }
  out_gpkg <- file.path(folder, paste0(file_stub, ".gpkg"))

  # Write only hotspot features
  if (!is.null(hs$hotspots_sf) && nrow(hs$hotspots_sf) > 0) {
     cat("[DIAGNOSTIC] Writing ", nrow(hs$hotspots_sf), " hotspots to: ", out_gpkg, "\n", file = stderr())
     # Force removal to ensure we aren't silently failing on overwrite
     if (file.exists(out_gpkg)) unlink(out_gpkg)
     sf::st_write(hs$hotspots_sf, out_gpkg, quiet = TRUE, delete_dsn = TRUE)
     cat("[DIAGNOSTIC] Write complete. Timestamp: ", file.info(out_gpkg)$mtime, "\n", file = stderr())
     n_hot <- nrow(hs$hotspots_sf)
  } else {
    cat("No hotspots found for ", metric_stub, " scope=", scope, " group=", group_val, "\n", file = stderr())
    out_gpkg <- NA_character_; n_hot <- 0L
  }

  tibble::tibble(
    scope      = scope,
    group_col  = ifelse(is.na(group_col), NA_character_, group_col),
    group_val  = ifelse(is.na(group_val), NA_character_, as.character(group_val)),
    metric     = metric_stub,
    n_hot      = n_hot,
    n_total    = n_total,
    pct_hot    = if (n_total > 0) (n_hot / n_total) * 100 else 0,
    gpkg       = out_gpkg
  )
}

# ---- Execute: global + subregional runs -----------------------------------

# Pull groupings from config, with a safe fallback
groupings <- if (!is.null(HOTS_CFG$groupings)) {
  HOTS_CFG$groupings
} else {
  c("income_grp","region_wb","continent","region_un","WWF_biome","country_name")
}

# Run both metrics (hotspot export)
metrics_hs <- c("abs_chg","pct_chg")
index_rows <- lapply(metrics_hs, function(m) run_one_hotset(plt_long, value_col = m, scope = "global"))

# Subregional runs
for (gc in groupings) {
  if (!gc %in% names(plt_long)) next
  vals_chr <- as.character(stats::na.omit(unique(plt_long[[gc]])))
  cat("Processing grouping: ", gc, " with ", length(vals_chr), " values.\n", file = stderr())
  if (!length(vals_chr)) next

  for (m in metrics_hs) {
    index_rows <- append(index_rows, list(
      purrr::map_dfr(vals_chr, \(v) run_one_hotset(
        df        = plt_long,
        value_col = m,
        scope     = "by_group",
        group_col = gc,
        group_val = v
      ))
    ))
  }
}

hot_index <- dplyr::bind_rows(index_rows)

# Ensure the output dir exists before writing the CSV index
csv_dir <- file.path(data_dir(), "processed", "hotspots")
dir.create(csv_dir, recursive = TRUE, showWarnings = FALSE)
readr::write_csv(hot_index, file.path(csv_dir, "_hotspots_index.csv"))

# Small console summary
dplyr::glimpse(hot_index, width = 120)

```


## Trimmed change bar plots

::: callout-note
**How to read these bars**

-   Each bar shows the **trimmed mean absolute change** (\|Δ\|) per service within each group; facet axes are free.
-   Bars are **always positive** by design: height = **magnitude of change**, not direction.
-   Direction-of-concern used elsewhere in the analysis:
    -   Worse when **up** ➜ `Sed_export`, `N_export`, `C_Risk`.
    -   Worse when **down** ➜ `Nature_Access`, `Pollination`, `N_Ret_Ratio`, `Sed_Ret_Ratio`, `C_Risk_Red_Ratio`.
-   These bars answer **“where is change largest?”**. See hotspot maps/violins for **up vs. down** patterns.
:::

Short answer: your current barplots use all grid cells (the full 10-km population), not just hotspots. They summarize trimmed means per subregion/global from plt_long via aggregate_change_simple(), with cut_q=0.999 to cap outliers and (optionally) drop_zeros=TRUE. That’s why every bar is positive—those bars are the magnitude of change, not the direction.

Outputs: PNGs land in `outputs/plots/{abs|pct}/<group_col>/bars_*.png` plus a flat copy in `outputs/plots/latest/bars/` for embedding here. The legacy global-only single-bar chart was removed to keep the gallery focused on grouped views.

```{r}
#| label: bars_by_region
#| message: false
#| warning: false
#| echo: false
#| eval: false
#| cache: false

# --- knobs you can tweak quickly -----------------------------------------
group_col         <- "region_wb"   # e.g. "income_grp","continent","region_un","WWF_biome"
metric            <- "pct"         # "pct" or "abs"
cut_q             <- 0.999         # trim extreme 0.1% so bars aren't dominated by outliers
include_global    <- FALSE         # TRUE to add a "Global" bar into each facet
keep_only_ordered <- TRUE          # show only services in your svc_order
save_plot         <- TRUE
out_dir           <- file.path("outputs","plots")
out_stub          <- paste0("bars_", tolower(group_col), "_", metric)

# --- facet order (your canonical set first) --------------------------------
svc_order <- c(
  "C_Risk","N_export","Sed_export",
  "C_Risk_Red_Ratio","N_Ret_Ratio","Sed_Ret_Ratio",
  "Pollination","Nature_Access"
)

# --- compute trimmed mean magnitude by region -----------------------------
stopifnot(group_col %in% names(plt_long))
gc <- group_col

df_trim <- plt_long |>
  dplyr::filter(!is.na(.data[[gc]])) |>
  dplyr::mutate(
    abs_cell = abs(.data$abs_chg),
    pct_cell = abs(.data$pct_chg)
  ) |>
  dplyr::group_by(.data$service) |>
  dplyr::mutate(
    abs_cap = stats::quantile(.data$abs_cell, cut_q, na.rm = TRUE),
    pct_cap = stats::quantile(.data$pct_cell, cut_q, na.rm = TRUE)
  ) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    abs_trim = pmin(.data$abs_cell, .data$abs_cap),
    pct_trim = pmin(.data$pct_cell, .data$pct_cap)
  ) |>
  dplyr::group_by(.data$service, .data[[gc]]) |>
  dplyr::summarise(
    abs_mean = mean(.data$abs_trim, na.rm = TRUE),
    pct_mean = mean(.data$pct_trim, na.rm = TRUE),
    .groups = "drop"
  )

# optional: add a "Global" reference row per service
if (isTRUE(include_global)) {
  glob <- plt_long |>
    dplyr::mutate(
      abs_cell = abs(.data$abs_chg),
      pct_cell = abs(.data$pct_chg)
    ) |>
    dplyr::group_by(.data$service) |>
    dplyr::mutate(
      abs_cap = stats::quantile(.data$abs_cell, cut_q, na.rm = TRUE),
      pct_cap = stats::quantile(.data$pct_cell, cut_q, na.rm = TRUE)
    ) |>
    dplyr::ungroup() |>
    dplyr::mutate(
      abs_trim = pmin(.data$abs_cell, .data$abs_cap),
      pct_trim = pmin(.data$pct_cell, .data$pct_cap)
    ) |>
    dplyr::group_by(.data$service) |>
    dplyr::summarise(
      abs_mean = mean(.data$abs_trim, na.rm = TRUE),
      pct_mean = mean(.data$pct_trim, na.rm = TRUE),
      .groups = "drop"
    )
  glob[[gc]] <- factor("Global")
  df_trim <- dplyr::bind_rows(df_trim, glob)
}

# order facets and, by default, drop services not in svc_order
if (keep_only_ordered) df_trim <- dplyr::filter(df_trim, .data$service %in% svc_order)
extras <- setdiff(unique(df_trim$service), svc_order)
df_trim <- dplyr::mutate(df_trim,
  service = factor(.data$service, levels = c(svc_order, sort(extras)))
)

# x order: put "Global" first if present, else alphabetical by group
if (isTRUE(include_global)) {
  df_trim[[gc]] <- forcats::fct_relevel(as.factor(df_trim[[gc]]), "Global", after = 0)
} else {
  df_trim[[gc]] <- as.factor(df_trim[[gc]])
}

yvar <- if (identical(metric, "abs")) "abs_mean" else "pct_mean"

p_bars <- ggplot2::ggplot(df_trim, ggplot2::aes(x = .data[[gc]], y = .data[[yvar]])) +
  ggplot2::geom_col() +
  ggplot2::facet_wrap(~ service, ncol = 3, scales = "free_y", drop = FALSE) +
  ggplot2::labs(
    title = paste0("Magnitude of change by ", gc, " (", metric, ")"),
    x = NULL,
    y = if (identical(metric, "abs")) "Mean |Δ| (service units)" else "Mean |Δ| (%)"
  ) +
  ggplot2::theme_minimal(base_size = 11) +
  ggplot2::theme(
    axis.text.x = ggplot2::element_text(angle = 45, hjust = 1),
    panel.grid.major.x = ggplot2::element_blank()
  )

if (isTRUE(save_plot)) {
  # put plots in outputs/plots/<metric>/<group_col>/
  out_root <- out_plots()
  out_dir <- file.path(out_root, metric, tolower(group_col))
  dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)

  # encode key params in the base name
  file_base <- paste(
    "bars",
    tolower(group_col),
    metric,
    paste0("cut", gsub("\\.", "", as.character(cut_q))),
    if (include_global) "with-global" else "no-global",
    sep = "_"
  )

  # avoid overwrite by adding _v02, _v03, …
  final_path <- file.path(out_dir, paste0(file_base, ".png"))
  i <- 2
  while (file.exists(final_path)) {
    final_path <- file.path(out_dir, sprintf("%s_v%02d.png", file_base, i))
    i <- i + 1
  }

  ggplot2::ggsave(
    filename = final_path,
    plot = p_bars,
    width = 12, height = 8, dpi = 300,
    device = ragg::agg_png,  # crisp text
    bg = "white"             # <- NO transparency
  )
  message("Saved plot to: ", normalizePath(final_path))
}

p_bars

```

Generate trimmed-mean bar plots (abs & pct) for each grouping. Plots save to `outputs/plots/...` and are embedded below.

#| label: make-all-barplots-inputs
#| message: false
#| warning: false
#| echo: false
#| cache: false
#| results: "hold"
#| eval: false
if (length(good_bars)) {
  invisible(file.copy(good_bars, latest_dir_bars, overwrite = TRUE))
}
```

```{r}
#| label: display-barplots
#| echo: false
#| results: 'asis'
#| cache: false
# Show barplots directly from signed_alt_keep0 and signed_alt_drop0
bar_dirs <- c(
  file.path("outputs","plots","signed_alt_keep0"),
  file.path("outputs","plots","signed_alt_drop0")
)
pngs <- unlist(lapply(bar_dirs, function(d) list.files(d, pattern = "^bars_signed_alt_.*\\.png$", full.names = TRUE)))

if (length(pngs)) {
  # Show keep0 first, then drop0
  pngs <- pngs[order(grepl("drop0", pngs))]
  rel_paths <- file.path("..", pngs)
  md <- c(
    "### Signed means (alt variants: keep0 vs drop0; dashed = global mean)",
    paste(sprintf("![](%s)", rel_paths), collapse = "\n\n")
  )
  knitr::asis_output(paste(md, collapse = "\n\n"))
} else {
  knitr::asis_output("> No signed bar plots were generated; check upstream chunks.")
}
```

## Hotspot violin plots

Summarize hotspot distributions by group using saved PNGs; skip computation if group columns are absent. Helper `run_
violins_by()` now lives in `R/hotspot_violins.R`, so we just call it from this report.

Outputs: PNGs are written to `outputs/plots/{abs|pct}/<group_col>/violins_*.png` and mirrored into `outputs/plots/latest/violins/` for embedding.

```{r}
#| label: make-hotspot-violins
#| message: false
#| warning: false
#| echo: false
#| cache: false
#| results: "hold"
#| eval: true

stopifnot(exists("plt_long"))
groupings <- if (!is.null(HOTS_CFG$groupings) && length(HOTS_CFG$groupings) > 0) {
  HOTS_CFG$groupings
} else {
  c("income_grp","region_wb","continent","region_un","WWF_biome","country_name")
}
missing_groupings <- setdiff(groupings, names(plt_long))
if (length(missing_groupings)) {
  warning("Grouping column(s) not found in plt_long and will be skipped: ",
          paste(missing_groupings, collapse = ", "))
}
groupings <- intersect(groupings, names(plt_long))
message("make-hotspot-violins groupings: ", if (length(groupings)) paste(groupings, collapse = ", ") else "none")
violin_order <- levels(plt_long$service)
if (is.null(violin_order)) {
  violin_order <- unique(plt_long$service)
}
violin_order <- as.character(violin_order)
if (length(groupings) == 0) {
  warning("No grouping columns found in plt_long; skipping hotspot violins.")
} else {
  for (gc in groupings) {
    # Check for valid data before attempting to plot
    n_valid <- sum(!is.na(plt_long[[gc]]))
    if (n_valid == 0) {
      message("Skipping violins for ", gc, ": all values are NA.")
      next
    }

    message("Rendering hotspot violins for: ", gc)
    tryCatch({
      run_hotspot_violins_by(
        df_long          = plt_long,
        group_col         = gc,
        loss              = HOTS_CFG$loss,
        gain              = HOTS_CFG$gain,
        pct_cutoff        = HOTS_CFG$pct_cutoff,
        threshold_mode    = HOTS_CFG$threshold_mode,
        svc_order         = violin_order,
        keep_only_ordered = TRUE
      )
    }, error = function(e) {
      message("Error generating violins for ", gc, ": ", e$message)
    })
  }
}
latest_dir_violins <- file.path("outputs","plots","latest","violins")
dir.create(latest_dir_violins, recursive = TRUE, showWarnings = FALSE)
old_latest_violins <- list.files(latest_dir_violins, full.names = TRUE, all.files = FALSE)
if (length(old_latest_violins)) invisible(file.remove(old_latest_violins))
all_new_violins <- list.files(out_plots(), pattern = "violins_.*\\.png$",
                              recursive = TRUE, full.names = TRUE)
all_new_violins <- all_new_violins[!grepl("/latest/", all_new_violins)]
good_violins <- all_new_violins[file.info(all_new_violins)$size > 0]
if (length(good_violins)) {
  invisible(file.copy(good_violins, latest_dir_violins, overwrite = TRUE))
}

```

```{r}
#| label: display-violins
#| echo: false
#| results: 'asis'
#| cache: false
vio_dir   <- file.path("outputs","plots","latest","violins")
vio_files <- list.files(vio_dir, pattern = "violins_.*\\.png$", full.names = FALSE)
if (length(vio_files)) {
  rel_paths <- file.path("..", vio_dir, sort(vio_files))
  md <- paste(sprintf("![](%s)", rel_paths), collapse = "\n\n")
  knitr::asis_output(md)
} else {
  knitr::asis_output("> No violin plots were generated; check upstream chunks.")
}
```


---
title: "Current Status"
output: html_notebook
---

```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
library(tidyr)
library(forcats)
library(scales)
library(RColorBrewer)
library(htmltools)
library(leaflet)
library(devtools)
library(reticulate)
load_all()
#source the helper functions
# set venv for Python
use_virtualenv("~/venvs/coastal_snap_env", required = TRUE)

inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data'
```


# Workflow Synthesis: Global Hotspots of Ecosystem Service Change

## 1. Data Inputs

Global 10 km grid covering terrestrial areas worldwide (polygon-based template).

Coastal protection model outputs (1992 and 2020) originally as points spaced ~500 m along shorelines.

Global raster datasets for nitrogen retention/export/ratio, sediment retention/export/USLE, pollination, and nature access (1992 and 2020).

Country polygon dataset for spatial aggregation and attribution.

## 2. Data Processing Pipeline

Step 1: Coastal Protection Aggregation

Calcualted three things Rt, Rt_nohab_all and Rt_nohab_all-Rt= Rt_service.

Computed mean and max per grid cell for each coastal protection metric (Rt_1992, Rt_2020, and related attributes).

Step 2: Zonal Statistics for Other Services

Used a geosharding-based pipeline to extract zonal statistics (mean or sum, depending on the service) from global raster layers to the 10 km grid. (also for the hydrosheds) 

Outputs for both 1992 and 2020 were appended as new grid attributes.

Step 3: Bi-temporal Change Calculation

Applied compute_change() function to derive absolute and percentage changes for all variables, using consistent naming conventions (_abs_chg, _pct_chg).

Step 4: Reshaping Data for Hotspot Detection

Converted the dataset to long format (plt_long) with columns: fid, service, and pct_chg.

Step 5: Hotspot Identification

Defined loss/damage services and gain services.

Identified hotspots as grid cells in the bottom 5% (losses) or top 5% (gains) per service.

Step 6: Hotspot Grouping into Service Combos

Created three combos for aggregated hotspot counts:

count_prov_access: coastal protection (Rt_service_mean), habitat-modified coastal protection (Rt_nohab_mean), pollination, nature access.

count_N: nitrogen retention, nitrogen export, nitrogen ratio.

count_sed: sediment retention, sediment export, USLE.

Calculated counts per combo and total hotspot count.

Step 7: Final Integration and Output

Generated binary columns (0/1) per service for hotspot presence.

Joined hotspot attributes back to the 10 km grid sf object.

Produced final geopackage file with:

Per-service hotspot flags.

Total hotspot count.

Counts for count_prov_access, count_N, and count_sed.

A column listing all hotspot services per grid cell.

## 3. Outputs

Final sf object containing all hotspot-related attributes for each 10 km grid cell.

Ready for mapping in QGIS or for further analysis (e.g., joining with socioeconomic variables).

Flexible structure allows filtering for losses only, per-service mapping, or aggregated combo-based analyses.


# 1. Clean and Organize Synthesis Data


```{r consolidate final outputs}

#Builds a list of files in the vector directory
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data'
sets <- file.path(inpath, list.files(inpath, pattern = "gpkg$"))
# Set and select an index to load the desired dastaset

sets1 <- sets[c(4,5)] # 10 km grid, zonals stats for ES services 1992, 2020), sums and means
sets2 <- sets[c(2,3)] # 10 km grid, zonal stats for beneficiaries  , sums and means (depends on the variable)

# Step 1. Load vectors with the summary stats objects
sf_a <- lapply(sets1, st_read)

# Step 2: Add fid to each (all have same row count & order)
sf_a <- lapply(sf_a, function(x) dplyr::mutate(x, fid = dplyr::row_number()))

# Update: include the coastal protection data:
sf_cp <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/summary_pipeline_workspace/grid_10k_synth_zonal_2025_07_20_23_45_27.gpkg')
# Define name replacements per object. Had to do it manually, there is just too much variability to automatize, did it by hand, but this is the kind of thing an LLM helps to write easily. Still, there must be a smarter, more flexible way to do it (maybe the best approach is to keep a consistent naming convention for the modeled rasters, this has been a PIA since i started this work)

# Update 2. Include corrected Access Data
rename_list <- list(
  c(
    usle_1992_mean = "global_usle_marine_mod_ESA_1992_mean",
    usle_2020_mean = "global_usle_marine_mod_ESA_2020_mean",
    nature_access_1992_mean = "nature_access_lspop2019_ESA1992_mean",
    nature_access_2020_mean = "nature_access_lspop2019_ESA2020_mean",
    n_ret_ratio_1992_mean = "N_ret_ratio_1992_mean",
    n_ret_ratio_2020_mean = "N_ret_ratio_2020_mean",
    sed_ret_ratio_1992_mean = "Sed_ret_ratio_1992_mean",
    sed_ret_ratio_2020_mean = "Sed_ret_ratio_2020_mean"
  ),
  c(
    n_export_1992_sum = "global_n_export_tnc_esa1992_sum",
    n_export_2020_sum = "global_n_export_tnc_esa2020_sum",
    n_retention_1992_sum = "global_n_retention_ESAmar_1992_fertilizer_sum",
    n_retention_2020_sum = "global_n_retention_ESAmar_2020_fertilizer_sum",
    sed_export_1992_sum = "global_sed_export_marine_mod_ESA_1992_sum",
    sed_export_2020_sum = "global_sed_export_marine_mod_ESA_2020_sum",
    pollination_1992_sum = "realized_polllination_on_ag_ESA1992_sum",
    pollination_2020_sum = "realized_polllination_on_ag_ESA2020_sum"
  )
)
sf_a <- Map(function(x, renames) {
  dplyr::rename(x, !!!renames)
}, sf_a, rename_list)

# Calculate bitemporal change in % and absolute terms. Drop original columns
sf_a <- lapply(sf_a, function(x) compute_change(x, suffix = c("_sum", "_mean"), change_type= "both", drop_columns = TRUE))

# Join into a single sf object
sf_f <- sf_a[[1]]

# Step 2: Loop over the rest and join by 'fid'
for (i in 2:length(sf_a)) {
  sf_f <- dplyr::left_join(sf_f, sf_a[[i]] %>% sf::st_drop_geometry(), by = "fid")
}
rm(sf_a)
 # NaN are divisions by 0, NA is when values are NA

# load beneficiaries data. There is no change going on here right now, but maybe in the future. The workflow can handle this
sf_ben <- lapply(sets2,st_read)
# Add fid (primary key)
sf_ben <- lapply(sf_ben, function(x) dplyr::mutate(x, fid = dplyr::row_number()))

sf_benf <- sf_ben[[1]]

# Step 2: Loop over the rest and join by 'fid'
for (i in 2:length(sf_ben)) {
  sf_benf <- dplyr::left_join(sf_benf, sf_ben[[i]] %>% sf::st_drop_geometry(), by = "fid")
}
rm(sf_ben) # remove not needed anymore
# join beneficiary data to the ES change data
sf_f <- dplyr::left_join(sf_f, sf_benf %>% sf::st_drop_geometry(), by = "fid")
rm(sf_benf) 

# export output

st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```



## 1.2 Include Coastal Protection (final)

This whole part need to be fixed at some point. The key part is to get the coastal with the right fid so we can join and add this correclty wqhen we have more data.
Right now keep this hewre
```{r include coastal points}


cp2 <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg')

cp2 <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg')

st_write(cp2, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg', append=FALSE)

# st_write(cp2, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg', append=FALSE)

# load cleaned coastal protection data:
cp_points <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data/10k_grid_coastal_calc.gpkg')
cp_points <- cp_points %>% rename(fid=fid_2)
# Ensure same CRS (optional) Add a conditional here that checks before trying to reproject!
cp_points <- st_transform(cp_points, st_crs(cp2))

cp_points <- cp_points %>% mutate(Rt_ratio_1992=(Rt_nohab_all_1992-Rt_1992)/Rt_nohab_all_1992) %>%
  mutate(Rt_ratio_2020=(Rt_nohab_all_2020-Rt_2020)/Rt_nohab_all_2020)
 cp_points$fid <- as.integer(cp_points$fid) 
st_write(cp_points, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/Spring/Inspring/coastal_risk_tnc_esa1992_2020_ch_f.gpkg', append=FALSE)
##########################################################
# Join and summarize
###### I WILL HAVE TO RUN THIS IN PYTHON BECAUSE IT IS TOO SLOW IN R. MAYBE I COULD CHUNCK HERE, BUT NO POINT OF LEANRING NEW TOOLS ON R WHNE PYTHON DOES IT FINE AND I CAN KEEP TRANSITIONING THERE 
#THIS MIGHT NOT EVEN BE NECESSARY ANYMORE!!!
cp2_joined <- st_join(cp2, cp_points) %>%
  group_by(fid) %>%
  summarise(
    Rt_1992_mean = mean(Rt_1992, na.rm = TRUE),
    Rt_1992_max = max(Rt_1992, na.rm = TRUE),
    Rt_nohab_1992_mean = mean(Rt_nohab_all_1992, na.rm = TRUE),
    Rt_nohab_1992_max = max(Rt_nohab_all_1992, na.rm = TRUE),
    Rt_service_1992_mean = mean(Rt_service_1992, na.rm = TRUE),
    Rt_service_1992_max = max(Rt_service_1992, na.rm = TRUE),
    Rt_service_1992_mean = mean(Rt_service_1992, na.rm = TRUE),
    Rt_service_1992_max = max(Rt_service_1992, na.rm = TRUE),
    Rt_ratio_1992_mean = mean(Rt_ratio_1992, na.rm=TRUE),
    Rt_ratio_1992_max = max(Rt_ratio_1992, na.rm=TRUE),
    Rt_2020_mean = mean(Rt_2020, na.rm = TRUE),
    Rt_2020_max = max(Rt_2020, na.rm = TRUE),
    Rt_nohab_2020_mean = mean(Rt_nohab_all_2020, na.rm = TRUE),
    Rt_nohab_2020_max = max(Rt_nohab_all_2020, na.rm = TRUE),
    Rt_service_2020_mean = mean(Rt_service_2020, na.rm = TRUE),
    Rt_service_2020_max = max(Rt_service_2020, na.rm = TRUE),
    Rt_ratio_2020_mean = mean(Rt_ratio_2020, na.rm=TRUE),
    Rt_ratio_2020_max = max(Rt_ratio_2020, na.rm=TRUE),
    geometry = first(geom)
  )

head(cp2_joined)
cp2_joined$fid <- as.integer(cp2_joined$fid)
sapply(cp2_joined, class)
# 
# # to separate it from the raw empty template, added this with the calculated data.
 st_write(cp2_joined,'/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_calc.gpkg', append=FALSE)


ly <- st_layers('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg')
ly <- st_layers('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_calc.gpkg')

tt <- st
```

### 1.2.1 Fix Coastal protection


```{r coastal protection consoldiation}
# Load grid (nothing special, just a template) This one hasa tge right id_numbers. (from the original, key for joining) 
cp2 <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.shp')

cp2 <- cp2 %>% mutate(fid_2 = row_number())

# Already calculated
cp_calc <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_calc.gpkg')

cp_calc <- cp_calc %>% mutate(fid_2 = row_number())
cp_calc <- st_drop_geometry(cp_calc)


cp_calc <- left_join(cp2,cp_calc)
cp_calc$fid_2 <- NULL

st_write(cp_calc, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_risk_1992_2020.gpkg')
```

## 1.3 Join Coastal protection and export again

```{r add to the existing synthesis data} 
#to preserve all original polygon columns: (not doing it right now but can be redone)
sf_f <- st_read(paste0(inpath,'/', '10k_grid_ES_change_benef.gpkg'))

# Calculate change#

cp_calc <- compute_change(cp_calc, suffix = c("_mean", "_max"), drop_columns = TRUE, change_type = "both")

cp_calc <- st_drop_geometry(cp_calc)
# export output
cp_calc <- cp_calc %>%
  select(-contains("_max"))

sf_f <- sf_f %>% mutate(fid = row_number()) # make sure that the fid matches! this is key!

# join the cp data:
sf_f <- left_join(sf_f, cp_calc, by = 'fid')
# move the columns to the right location
sf_f <- sf_f %>%
  relocate(
    Rt_mean_abs_chg,
    Rt_mean_pct_chg,
    Rt_nohab_mean_abs_chg,
    Rt_nohab_mean_pct_chg,
    Rt_service_mean_abs_chg,
    Rt_service_mean_pct_chg,
    Rt_ratio_mean_abs_chg,
    Rt_ratio_mean_pct_chg,
    .before = GHS_BUILT_S_E2020_mean
  )
sf_f$fid <- NULL

st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```

## 1.4 Add Updated Access Data

No need to run again, already updated the Access Data. 
```{r add access}
#| eval: false
#| include: false

sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg')) # why am I loading this? What is the purpose. This is always the final (most curtrent version). To this one is that ineed to do the adjustments, add new columns


# This is the old output with the calculated means for services (because we want to add the new columns to this output, even if it was created sli
sf_1 <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data/10k_grid_synth_serv_means.gpkg')

sf_1 <- sf_1 %>% mutate(fid = dplyr::row_number())


poly1 <- poly1 %>% rename(nature_access_lspop2019_ESA2020_mean = nature_access_lspop2019_ESA2020._mean)
sf_1 <- sf_1 %>% rename(nature_access_lspop2019_ESA2020_mean = nature_access_lspop2019_ESA2020._mean)
poly1 <- poly1 %>% mutate(fid = dplyr::row_number())

poly1 <- st_drop_geometry(poly1)


sf_1$nature_access_lspop2019_ESA2020_mean <- NULL
sf_1$nature_access_lspop2019_ESA2020_mean <- poly1$nature_access_lspop2019_ESA2020_mean

sf_1 <- sf_1 %>% select(fid, global_usle_marine_mod_ESA_1992_mean, global_usle_marine_mod_ESA_2020_mean,nature_access_lspop2019_ESA1992_mean,nature_access_lspop2019_ESA2020_mean, N_ret_ratio_1992_mean,N_ret_ratio_2020_mean, Sed_ret_ratio_1992_mean, Sed_ret_ratio_2020_mean)

st_write(sf_1, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data/10k_grid_synth_serv_means.gpkg', append=FALSE)


names(sf_f)

names(poly1)

poly1 <- poly1 %>% rename(nature_access_abs_chg=access_abs_chg)
poly1 %>% rename(nature_access_pct_chg = access_pct_chg)

names(sf_1)

names(sf_f)

sf_f$nature_access_abs_chg=poly1$nature_access_abs_chg
sf_f$nature_access_pct_chg=poly1$access_pct_chg
st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```



# 2 Load Vector Data and pivot



Here, the spatial objects with the additional attributes are loaded and reformatted for analysis and chart preparation - **pivot & tidy**. Add to the documentation , environment or however that's called the vectors to select and order the columns. This should actually be done as a database structure. The columns live somewhere and are summoned upon need from a set of options (list_dir).

```{r pivot}
#| eval: false
#| include: false
# 
#  get pct as standalone data to add to the table
# Most recent version of synthesis data:
sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'))
sf_f$fid <- seq_len(nrow(sf_f)) 
plt <- st_drop_geometry(sf_f)
 

# get the benef vars (all that are not "chg")
socio_vars <- names(plt)[
  names(plt) != c("fid", "c_fid") & 
  !grepl("chg$", names(plt))
]

# Pivot all columns that contain pct_ch, keeping other relevant vars

plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, c_fid, service, pct_chg, all_of(socio_vars)) %>% 
  mutate(service = str_replace(service, "_pct_chg.*", "")) %>% 
  mutate(service = str_replace(service, "_mean.*", ""))
  
  
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_chg)) %>% filter(!is.na(c_fid)) %>% filter(pct_chg != Inf)  

# rename some variables (just for aesthetics)

  plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "n_export" ~ "N_export",
   service == "n_retention" ~ "N_retention",
   service == "nature_access" ~ "Nature_Access",
   service == "pollination" ~ "Pollination",
   service == "usle" ~ "USLE",
   service == "n_ret_ratio" ~ "N_Ret_Ratio",
   service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
   service == "Rt_ratio" ~ "C_Risk_Red_Ratio",
   service == "Rt" ~ "C_Risk",
   TRUE ~ service
   ))
  
```

# 3. Get Hotspots

Need to think on an effective way to change the value (the share/quantile we want to extract and also filter by group!!!!)
So, we need a long table format, then define the threshold the services with gains/losses and then (optional) set the filters by groups. That would be awesome

Add the combos

```{r get_hotspots}
# --------------------------------------------------------------------
# Define Inputs
# --------------------------------------------------------------------
pct_cutoff <- 0.05  # Top/bottom % for hotspot threshold
df <- plt_long       # Your long-format input data

# Optional filtering
# df <- df %>% filter(income_grp == "5. Low income")

# Define service directionality
loss_services <- c("Nature_Access", "Pollination", "N_Ret_Ratio", "Sed_Ret_Ratio", "C_Risk_Red_Ratio")
gain_services <- c("Sed_export", "N_export", "C_Risk")

# Define flexible combos as a *named list*:
combos <- list(
  combo_1 = c("Nature_Access","Pollination","N_export", "Sed_export", "C_Risk"),
  combo_2 = c("Nature_Access","Pollination", "N_Ret_Ratio", "Sed_Ret_Ratio", "C_Risk_Red_Ratio")
)

# --------------------------------------------------------------------
# STEP 1: Identify top/bottom hotspots
# --------------------------------------------------------------------
df_hotspots <- df %>%
  group_by(service) %>%
  mutate(
    n_total = n(),
    n_cut = ceiling(n_total * pct_cutoff),
    rank_high = rank(-pct_chg, ties.method = "first"),
    rank_low = rank(pct_chg, ties.method = "first"),
    hotspot_flag = case_when(
      rank_high <= n_cut ~ "high",
      rank_low <= n_cut ~ "low",
      TRUE ~ NA_character_
    ),
    hotspot_binary = !is.na(hotspot_flag)
  ) %>%
  ungroup()

# --------------------------------------------------------------------
# STEP 2: Filter to relevant hotspot directions
# --------------------------------------------------------------------
df_hotspots_negative <- df_hotspots %>%
  filter(
    hotspot_binary,
    (service %in% loss_services & hotspot_flag == "low") |
    (service %in% gain_services & hotspot_flag == "high")
  )

# ----------------------------------------------
# Step 3 (Debugged version, with intermediate steps)
# ----------------------------------------------

# Base summarise (same as before)
hotspot_summary <- df_hotspots_negative %>%
  group_by(fid) %>%
  summarise(
    c_fid = first(c_fid),
    hotspot_count = n(),
    hotspot_services_list = list(unique(service))
  )

# Step 3.1 - Add flattened string of services
hotspot_summary <- hotspot_summary %>%
  mutate(
    hotspot_services = sapply(hotspot_services_list, function(x) paste(trimws(as.character(x)), collapse = ", "))
  )

# Step 3.2 - Compute dynamic combo counts outside mutate
combo_counts <- lapply(names(combos), function(combo_name) {
  combo_svcs <- combos[[combo_name]]
  sapply(hotspot_summary$hotspot_services_list, function(x) sum(x %in% combo_svcs))
})
names(combo_counts) <- paste0("count_", names(combos))

# Step 3.3 - Bind the combo columns to the summary object
hotspot_summary <- bind_cols(hotspot_summary, as_tibble(combo_counts))

# Step 3.4 - Add types
hotspot_summary <- hotspot_summary %>%
  mutate(
    hotspot_types = sapply(hotspot_services_list, function(svcs) {
      types <- sapply(svcs, function(s) {
        if (s %in% loss_services) "loss"
        else if (s %in% gain_services) "damage"
        else NA_character_
      })
      paste(na.omit(types), collapse = ", ")
    }),
    hotspot_services_negative = hotspot_services
  )

# --------------------------------------------------------------------
# STEP 4: Create binary matrix (wide format)
# --------------------------------------------------------------------
hotspot_binary_matrix <- hotspot_summary %>%
  select(fid, hotspot_services) %>%
  separate_rows(hotspot_services, sep = ",\\s*") %>%  # Fix: double escape "\\s*"
  mutate(is_hotspot = 1L) %>%
  pivot_wider(names_from = hotspot_services, values_from = is_hotspot, values_fill = 0)

# --------------------------------------------------------------------
# STEP 5: Join with SF object
# --------------------------------------------------------------------
hotspots_final <- sf_f %>%
  inner_join(hotspot_binary_matrix, by = "fid") %>%
  left_join(hotspot_summary %>% select(-hotspot_services_list), by = "fid")

# --------------------------------------------------------------------
# STEP 6: Export (optional)
# --------------------------------------------------------------------
st_write(hotspots_final, file.path(inpath, "10k_grid_hotspots_global.gpkg"), append = FALSE)


```


# 5 Scatterplots.


```{r pivot hotspot data and prepare for plotting}

plt <- st_drop_geometry(hotspots_final)
# --- Step 1: Identify the binary indicator columns in `plt` ---


# Step 1: Extract service indicator (binary hotspot) columns from `plt`
hotspot_cols <- c("C_Risk", "C_Risk_Red_Ratio", "Sed_Ret_Ratio", "Sed_export",
                  "Nature_Access", "N_export", "N_Ret_Ratio", "Pollination")


binary_long <- plt %>%
  select(fid, c_fid.x, all_of(hotspot_cols)) %>%
  pivot_longer(
    cols = -c(fid, c_fid.x),
    names_to = "service",
    values_to = "hotspot_binary"
  )

# Step 3: Pivot long the pct_chg values and normalize service names
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg$"),
    names_to = "service",
    values_to = "pct_chg"
  ) #%>%
  select(fid, c_fid.x, service, pct_chg, all_of(socio_vars)) %>%
  filter(!is.na(pct_chg), is.finite(pct_chg)) %>%
  mutate(service = str_replace(service, "_pct_chg$", "")) %>%
   plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "n_export" ~ "N_export",
   service == "n_retention" ~ "N_retention",
   service == "nature_access" ~ "Nature_Access",
   service == "pollination" ~ "Pollination",
   service == "usle" ~ "USLE",
   service == "n_ret_ratio" ~ "N_Ret_Ratio",
   service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
   service == "Rt_ratio" ~ "C_Risk_Red_Ratio",
   service == "Rt" ~ "C_Risk",
   TRUE ~ service
  )) %>%
  mutate(service = factor(service, levels = c("Nature_Access", "N_Ret_Ratio", "N_export", "Sed_Ret_Ratio", "Sed_export",  "Pollination",
                    "C_Risk_Red_Ratio", "C_Risk")))

plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, c_fid.x, service, pct_chg, all_of(socio_vars)) %>% 
  mutate(service = str_replace(service, "_pct_chg.*", "")) %>% 
  mutate(service = str_replace(service, "_mean.*", ""))
  
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_chg)) %>% filter(pct_chg != Inf)  


# ---------------------------------------------------------
# STEP 2: Set Service Order and Colors

# ---------------------------------------------------------

  plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "n_export" ~ "N_export",
   service == "n_retention" ~ "N_retention",
   service == "nature_access" ~ "Nature_Access",
   service == "pollination" ~ "Pollination",
   service == "usle" ~ "USLE",
   service == "n_ret_ratio" ~ "N_Ret_Ratio",
   service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
   service == "Rt_ratio" ~ "C_Risk_Red_Ratio",
   service == "Rt" ~ "C_Risk",
   TRUE ~ service
   ))
  

service_levels <- c("Nature_Access", "N_Ret_Ratio", "N_export", "Sed_Ret_Ratio", "Sed_export",  "Pollination",
                    "C_Risk_Red_Ratio", "C_Risk")


plt_long <- plt_long%>% filter(service %in% service_levels)

plt_long <- plt_long %>% mutate(service = factor(service, levels = service_levels))
```

What the fuck is this?

```{r adjust hotspots}

plt <- st_drop_geometry(hotspots_final)
# --- Step 1: Identify the binary indicator columns in `plt` ---


# Step 1: Extract service indicator (binary hotspot) columns from `plt`
hotspot_cols <- c("C_Risk", "C_Risk_Red_Ratio", "Sed_Ret_Ratio", "Sed_export",
                  "Nature_Access", "N_export", "N_Ret_Ratio", "Pollination")

binary_long <- plt %>%
  select(fid, c_fid.x, all_of(hotspot_cols)) %>%
  pivot_longer(
    cols = -c(fid, c_fid.x),
    names_to = "service",
    values_to = "hotspot_binary"
  )

# Step 2: Normalize service names to match `plt_long`
binary_long <- binary_long %>%
  mutate(service = case_when(
    service == "Rt_service_mean" ~ "C_Protection",
    service == "Rt_nohab_mean" ~ "C_Risk",
    TRUE ~ service
  ))

# Step 3: Pivot long the pct_chg values and normalize service names
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg$"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, c_fid.x, service, pct_chg, all_of(socio_vars)) %>%
  filter(!is.na(pct_chg), is.finite(pct_chg)) %>%
  mutate(service = str_replace(service, "_pct_chg$", "")) %>%
  mutate(service = case_when(
    service == "sed_export" ~ "Sed_export",
    service == "n_export" ~ "N_export",
    service == "n_retention" ~ "N_retention",
    service == "nature_access" ~ "Nature_Access",
    service == "pollination" ~ "Pollination",
    service == "usle" ~ "USLE",
    service == "n_ret_ratio" ~ "N_Ret_Ratio",
    service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
    service == "Rt_service_mean" ~ "C_Protection",
    service == "Rt_nohab_mean" ~ "C_Risk",
    TRUE ~ service
  )) %>%
  mutate(service = factor(service, levels = c(
    "Nature_Access", "N_Ret_Ratio", "N_export", "N_retention", "Sed_Ret_Ratio",
    "Sed_export", "USLE", "Pollination", "C_Protection", "C_Risk", "Rt_mean"
  )))

# Step 4: Join hotspot binary column
plt_long <- left_join(plt_long, binary_long, by = c("fid", "c_fid.x", "service"))

# Step 5: Filter to keep only hotspot points
plt_long <- filter(plt_long, hotspot_binary == 1)


```




```{r scatterplot,fig.height=7, fig.width=7, warning=FALSE}
# Make names shorter
socio_labels <- c(
  "GHS_BUILT_S_E2020_mean" = "Built Area",
  "fields_mehrabi_2017_mean" = "Field Size",
  "hdi_raster_predictions_2020_mean" = "HDI",
  "rast_adm1_gini_disp_2020_mean" = "Income Inequality",
  "rast_gdpTot_1990_2020_30arcsec_2020_sum" = "GDP (Total)",
  "GHS_POP_E2020_GLOBE_sum" = "Population (GHS)",
  "GlobPOP_Count_30arc_2020_sum" = "Population (Global)"
)

plt_long_socio <- plt_long %>%
  pivot_longer(
    cols = all_of(socio_vars),
    names_to = "socio_var",
    values_to = "socio_val"
  ) %>% filter(!is.na(socio_val))


# Step 2: Trim out top and bottom 2% of pct_chg within each service
 # plt_long_socio<- plt_long_socio %>%
 #  group_by(service) %>%
 #  mutate(
 #    pct_low = quantile(pct_chg, 0.01, na.rm = TRUE),
 #    pct_high = quantile(pct_chg, 0.99, na.rm = TRUE)
 #  ) %>%
 #  ungroup() %>%
 #  filter(pct_chg >= pct_low, pct_chg <= pct_high)

# Step 2: Loop over each service and generate faceted scatterplots
unique_services <- unique(plt_long_socio$service)

for (svc in unique_services) {
  plot_data <- filter(plt_long_socio, service == svc)
  plot_data <- plot_data %>%
  mutate(socio_label = socio_labels[socio_var])

  p <- ggplot(plot_data, aes(x = pct_chg, y = socio_val)) +
    geom_hex(bins = 60) +
    scale_fill_viridis_c(option = "D", direction = 1) +
    facet_wrap(~ socio_label, scales = "free_y", nrow = 2) +
    labs(
      title = paste("Ecosystem Service:", svc),
      x = "Ecosystem Service % Change",
      y = "Socioeconomic Variable Value",
      fill = "Density"
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(face = "bold"),
      plot.title = element_text(hjust = 0.5),
      axis.text = element_text(size = 9)
    )

  print(p)
}

```


```{r scatterplot,fig.height=7, fig.width=7, warning=FALSE}

#socio_vars <- socio_vars[-8]

plt <- st_drop_geometry(sf_f)

plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, c_fid, service, pct_chg, all_of(socio_vars)) %>% 
   mutate(service = str_replace(service, "_pct_chg.*", ""))
  
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_chg)) %>% filter(pct_chg != Inf)  


# ---------------------------------------------------------
# STEP 2: Set Service Order and Colors

# ---------------------------------------------------------

plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "n_export" ~ "N_export",
   service == "n_retention" ~ "N_retention",
   service == "nature_access" ~ "Nature_Access",
   service == "pollination" ~ "Pollination",
   service == "usle" ~ "USLE",
   service == "n_ret_ratio" ~ "N_Ret_Ratio",
   service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
   service == "Rt_service_mean" ~ "C_Protection",
   service == "Rt_nohab_mean" ~ "C_Risk",
   TRUE ~ service
   ))
  

service_levels <- c("Nature_Access", "N_Ret_Ratio", "N_export", "N_retention", "Sed_Ret_Ratio", "Sed_export", "USLE", "Pollination",
                    "C_Protection", "C_Risk", "Rt_mean")


plt_long <- plt_long %>% mutate(service = factor(service, levels = service_levels))
```


```{r plot 2}
plt_long_socio <- plt_long %>%
  pivot_longer(
    cols = all_of(socio_vars),
    names_to = "socio_var",
    values_to = "socio_val"
  ) %>% filter(!is.na(socio_val))


# Step 2: Trim out top and bottom 2% of pct_chg within each service
 plt_long_socio<- plt_long_socio %>%
  group_by(service) %>%
  mutate(
    pct_low = quantile(pct_chg, 0.01, na.rm = TRUE),
    pct_high = quantile(pct_chg, 0.99, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  filter(pct_chg >= pct_low, pct_chg <= pct_high)

# Step 2: Loop over each service and generate faceted scatterplots
unique_services <- unique(plt_long_socio$service)

for (svc in unique_services) {
  plot_data <- filter(plt_long_socio, service == svc)
  plot_data <- plot_data %>%
  mutate(socio_label = socio_labels[socio_var])

  p <- ggplot(plot_data, aes(x = pct_chg, y = socio_val)) +
    geom_hex(bins = 30) +
    scale_fill_viridis_c(option = "D", direction = 1) +
    facet_wrap(~ socio_label, scales = "free_y", nrow = 2) +
    labs(
      title = paste("Ecosystem Service:", svc),
      x = "Ecosystem Service % Change",
      y = "Socioeconomic Variable Value",
      fill = "Density"
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(face = "bold"),
      plot.title = element_text(hjust = 0.5),
      axis.text = element_text(size = 9)
    )

  print(p)
}
```
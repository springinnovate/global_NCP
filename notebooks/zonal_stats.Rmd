---
title: "Summary ES Analysis"
author: "Jeronimo Rodriguez-Escobar"
date: "2025-04-02"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

# Overview
We are building a structured approach to extract and analyze zonal statistics for ecosystem services (ES) at global scale using polygon datasets such as countries, income groups, biomes, and hydrological basins. The primary aim is to calculate and visualize changes in ES provision over time, towards allowing cross-regional comparisons and spatial trend analysis. While the workflow is a woprk in progress and can be further optimized and generalized, priorities are transparency, reproducibility, and documentation of the analytical logic behind each step. 

This document is intended to serve as a reference for collaborators working on ecosystem service modeling and spatial analysis, including potential integration into larger NCP assessments.

# 1. Objectives
1. Extract zonal statistics from ES global raster datasets using the `exactextractr` package. 
2. Integrate extracted statistics as attributes to spatial polygon datasets.
3. Generate visualizations (bar plots, maps) for spatial representation of results.

**Important Considerations:**
- Dependencies and overlapping regions. pose some challenges that need special handling.
- Small island nations and some unique geographic cases require additional checks, and case specific decisions at the individual territory level.
- So far the most promising subdivision runs have been by Basin and sub-basin obtained from [**Hydrosheds.org**](https://www.hydrosheds.org/products/hydrobasins). 

```{r setupl load libraries, include=FALSE}
library(sf)             # Spatial vector operations
library(dplyr)          # Data manipulation
library(terra)          # Raster processing
library(exactextractr)  # Zonal statistics
library(ggplot2)        # Visualization
library(forcats)        # Factor reordering
library(tidytext)
library(here)           # Managing paths
library(patchwork)      # Combining plots
library(tidyr)
library(parallel)       # Parallel computing
library(purrr)
library(stringr)
library(knitr)
library(kableExtra)
```

# 2. Prepare Input data

## 2.1. Load Polygon Data


This assumes that the polygon files are stored in the same working directory. Feel free to change it if necessary.  The different subsets except for biomes are obtained from the global country dataset provided by Justin *cartographic_ee_ee_r264_correspondence.gpkg* dataset provided by Justin J.

The analysis was performed at the individual feature (id) level,  grouped by Income group, Continent, and World Bank Region, dissolving the original vector data by the desired attribute. Dissolve operations are handled better in Python/QGis/ArcGis, R is not that great for that. 

The input polygon datasets are stored in the directory *vector*. The data included here has already been processed, but the user can reproduce the worklow by removing the new appended columns and running the workflow again. 

1. Country_id.gpkg:   individual countries/territories
2. Continent.gpkg:    Continent
3. Income_grp.gpkg:   Income group
4. Worl_Bank_Region.gpkg:    World Bank Region

Other sources

5. Biome.gpkg:    WWF Biome
6. hydrosheds_lev6.gpkg
7. hydrosheds_lev7.gpkg


```{r load polygons, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Read spatial polygon dataset

inpath <- here("vector")
sets <- file.path(inpath, list.files(inpath, pattern= 'gpkg$'))

#cols <- c("id", "continent", "income_grp", "region_wb", "WWF_biome", "HYBAS_ID", "HYBAS_ID")
#sets <- cbind(sets, cols)
# Set an index to select the desired dataset
t <- 6
# Select the target dataset from the list 
set <- sets[t]
# # load polygons
poly <- st_read(set) 

#get the name of the column withthe unique ids of the input vector
col <- colnames(poly[1])
cols <- col[1]

# Convert to data frame for nicer display
sets_df <- as.data.frame(basename(sets))
# Print nicely
kable(sets_df, format = "html", caption = "Geopackage Layers") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

```

## 2.2 Load Global ES as Raster Data**

For the 1992-2020 comparison, we included thse modeled Services


1.  Nitrogen Export. [**InVEST Nutrient Delivery Ratio**](http://data.naturalcapitalproject.org/invest-releases/3.5.0/userguide/ndr.html).
    Expressed in kg/pixel/year

2.  Sediment Retention.[**InVEST SDR: Sediment Delivery Ratio**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/sdr.html).
    Values in ton/pixel/year

3. Soil Erosion derived using the *Revised Universal Soil Loss Equation-USLE* 

4.  Pollination.  [**InVEST SDR: Pollinator Abundance Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/croppollination.html).
    Units represent Polllination Change in people fed on HABitat. More information in
  
5.  Coastal Protection. Unitless measure, refers to a derived vulnerability index. [**InVEST Coastal Vulnerability Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/coastal_vulnerability.html)

6.  Nature Access represented as [**the number of people within 1 hour travel of natural and semi-natural lands**](https://github.com/springinnovate/distance-to-hab-with-friction) (Chaplin-Kramer et al,
    2022).
    

```{r load raster es, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(kableExtra)

inpath <- here('input_ES')
# Override inpath manually if needed
#inpath <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/input_ES"

# List and clean file names
tiffes <- file.path(inpath, list.files(inpath, pattern = 'tif$'))
filename <- basename(tiffes)
raster_name <- gsub(".tif$", "", filename)

# Assign services and years
service <- rep(c("Coastal_Protection", "Nitrogen_Export", "Sediment_Export", "Usle", "Nature_Access", "Pollination"), each = 2)
year <- rep(c(1992, 2020), 6)

# Create color mapping
color <- rep(c("#9e9ac8", "#2c944c", "#08306b", "#17c0ff", "#A57C00", "#dd1c77"), each = 2)

# Create and display clean table
cd <- tibble(service, raster_name, year, color)

kable(cd, format = "html", caption = "Input Raster datasets 1992–2020") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```


# 3. Extract Zonal Stats per Spatial Unit

Compute Zonal Statistics for each target raster/year 

## 3.1 Global ES Layers for 1992 and 2020


**Note:** Although `exactextractr` is used here to obtain zonal summary statistics—specifically the mean (and optionally the standard deviation) for each polygon—it can also return a list of data frames, each containing the cell values from the input raster for a given feature. This capability is particularly useful for more detailed analyses. Additionally, user-defined functions can be incorporated, which I plan to explore next for the USLE analysis.

We calculated the mean value for each polygon  for each inpt varialble for 1992 and 2020, the mean value of the bi-temporal differences and in the next section added the Potential Sediment Retention service, 
The potential sediment retention is calculated as:

$$
\text{Potential Sediment Retention} = \frac{\text{USLE} - \text{Export}}{\text{USLE}}
$$



**Note** We can extraxt quantiles!!

```{r compute stats for each year, eval=FALSE, message=TRUE, include=FALSE}
# Set the target metrics to extract. Most Standard R summary metrics are available, for more information run ?exactextractr 
target_stats <- c("mean")

rasters_list <- lapply(tiffes, rast)
# Set the number of cores to run on parallel. This approach only works on Unix-based systems (Linux & MacOS). I still have to find the alternative for parallelzing on Windows. 
num.cores <- length(rasters_list) # set as many cores as rasters to be evaluated.  The number of cores to user is also contingent on the amount of RAM available.

#start extracting the summary statisticss
results_list <- mclapply(rasters_list, function(r) {
  # Perform exact extraction
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)
  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)},mc.cores = num.cores)

# Combine results from all evaluated rasters into a single data frame
zonal_df <- do.call(rbind, results_list)

# add color and service columns
zonal_df <- left_join(zonal_df,cd)


# Add the year. this uses the original filenames to extract the year, this works here, but might be too hardcoded for generalization 

zonal_df$year <- str_extract(zonal_df$raster_name, "\\d{4}") # i did not use us here for some reason. but keep close 

# Pivot wider to have all the attributes in Different columns
zonal_wide <- pivot_wider(zonal_df, id_cols=all_of(cols), names_from=c(service, year), values_from = mean)
# adjust column names #

# Join the extracted data to the polygon files
poly <- left_join(poly, zonal_wide, by= cols)

st_write(poly, here('vector', set), append = FALSE) # save optional
#write.csv(zonal_df, here('output_data', paste0('zonal_df_',col, '.csv')))

```


### 3.2. Zonal stats for the differences (between 1992 and 2020).  


Although it’s technically possible to process all datasets in a single run, inconsistencies in raster file naming make this difficult to manage. File names often don’t follow a consistent pattern, requiring manual reordering to ensure accurate comparisons.

There are two main processing scenarios: one involves handling multiple sets of layers (e.g., one per year), and the other uses a single set (such as difference layers). While this workflow could be streamlined and better structured, it hasn't been a priority yet and will need to be revisited later.

Ideally, the entire pipeline would be processed at once, but inconsistencies across years currently prevent that. This section documents the approach taken so far and serves as a reference for further refinement. I plan to integrate it as a single extraction, but the number of special cases can make it difficult to manage. I am working towards improving this

```{r compute stats diff, eval=FALSE, message=TRUE, include=FALSE}

# set the path and load the rasters with the calcualted 1992-2020 differences 
inpath <- paste0(inpath, "/", 'change_calc')
#inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/input_ES/change_calc'
# load the rasters with the calculated differences 
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)

#load rasters
rasters_list <- lapply(tiffes, rast)

# This line is to adjust the orderr of the input data. 
service <- c("Coastal Protection", "Nature Access","Nitrogen Export","Pollination","Sediment Export", "USLE")
service <- gsub(" ", "_", service)

filename<- as_tibble(cbind(service,filename))

# We'll compute mean 
target_stats <- "mean"

# Iterate function over the rasters
num.cores <- length(rasters_list)
results_list <- lapply(rasters_list, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)

  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)})#,mc.cores = num.cores) 

# Combine results from all rasters into one data frame
zonal_df_diff <- do.call(rbind, results_list)

#Adjust column so we can assemble the whole thing (yearly and difference values) in the same dataframe. Waht we are doing here is just creating a primary key to perform a join, what we need ios just the name of the "Service".

cd <- cd %>% mutate(year='diff')_df

zonal_df_diff <- left_join(zonal_df_diff,cd)

zonal_df <- zonal_df %>% filter(!is.na())


zonal_wide <- pivot_wider(zonal_df, id_cols=any_of(cols), names_from=c(service, year), values_from = mean)
# adjust column names 
colnames(zonal_wide)[colnames(zonal_wide) != cols] <- paste0(colnames(zonal_wide)[colnames(zonal_wide) != cols], "_diff")

poly <- left_join(poly, zonal_wide, by= cols)


layer1 <- poly

st_write(layer1, here("vector", "hydrosheds_lv6.gpkg"), layer = "layer1", append = FALSE)


```


### 3.3 Get amount of change (in % )


The next step calculates the percentage change in ecosystem service provision between 1992 and 2020, based on the zonal summary statistics. Two approaches were compared: (1) calculating differences directly from the extracted summary statistics, and (2) extracting statistics from pre-computed difference rasters. While both methods yield similar results, subtle differences were observed. At this stage, extracting from the difference rasters appears more robust and likely accounts for pixel-level masking more accurately. However, it is also more computationally demanding, and the benefits over direct differencing may not always justify the extra cost. This will be explored further in the next iteration of the workflow.


```{r extract change in %, eval=FALSE, include=FALSE}

# Function to compute percentage change for all relevant variables in an sf object
# There is plenty of room for optimization here.
# 1. Extract year columns only (excluding the ID column)
value_cols <- names(zonal_wide)[-1]  # assuming first column is the ID
years <- str_extract(value_cols, "\\d{4}") |> as.numeric()
years <- sort(years)

# 2. Loop over year pairs and calculate % change
for (i in seq_along(years)[-1]) {
  y_prev <- years[i - 1]
  y_curr <- years[i]
  
  col_prev <- paste0("n_export_", y_prev)
  col_curr <- paste0("n_export_", y_curr)
  new_col <- paste0("pct_ch_", y_curr, "_", y_prev)
  
  zonal_wide[[new_col]] <- (zonal_wide[[col_curr]] - zonal_wide[[col_prev]]) / 
                            zonal_wide[[col_prev]] * 100
}



calculate_percentage_change <- function(sf_obj) {
  # Identify all variables that exist in both 1992 and 2020
  base_vars <- service
  
  # Iterate through each variable to compute percentage change
  for (var in base_vars) {
    col_1992 <- paste0(var, "_1992")  # Column name for 1992
    col_2020 <- paste0(var, "_2020")  # Column name for 2020
    col_pct  <- paste0(var, "_pct_ch")   # New percentage change column
    
    # Check if the required columns exist in the sf object
    if (all(c(col_1992, col_2020) %in% names(sf_obj))) {
      sf_obj <- sf_obj %>%
        mutate(!!col_pct := ((!!sym(col_2020) - !!sym(col_1992)) / !!sym(col_1992)) * 100)
    }
  }
  return(sf_obj)
}

# Apply function to all sf objects in the list
poly <- calculate_percentage_change(poly)


# get pct as standalone data to add to the table
plt <- st_drop_geometry(poly)
plt  <- plt %>%
  select(1, contains("pct"))
################## THIS IS VERY IMPPORTAnt. Instead of struggling  with the multiple dataframes, it is easier to load the vector file swith all the column and pivot longer as necessary. Easier to manage, adjust on the fly!
plt <- plt %>%
  pivot_longer(
    cols = ends_with("pct_ch"),  # Select all columns ending with "pct_ch"
    names_to = "service",        # New column to store the service names
    values_to = "pct_ch"         # New column to store the percentage change values
  )


#write.csv(plt, here('output_data', paste0('pct_chg_',cols, '.csv')))
#st_write(poly, here('vector', set), append = FALSE) 
```


### 3.4 Potential Retention and Export


```{r extract pot sed Retenion, eval=FALSE, include=FALSE}

# set the path and load the rasters with the differences that we calcualted in step 3
#build paths to files
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/input_ES/Pot_Sed_retention'
inpath <- here("input_ES", "Pot_sed_retention")
# load the rasters with the calculated differences 
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)

rasters_list <- lapply(tiffes, rast)
rasters_list <- Map(function(r,new_name){
  names(r) <- new_name
  return(r)
  }, rasters_list, filename)

diff<- c("diff", 2020,1992)
# create a table with the content to fill the table
color <- c("#08606b", "#08606b", "#68606b")
cd <- as_tibble(cbind(filename, color,year))
rm(filename)

target_stats <- "mean"

# Iterate function over the rasters
num.cores <- length(rasters_list)
results_list <- lapply(rasters_list, function(r) {
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)

  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)})#,mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df_diff <- do.call(rbind, results_list)

zonal_df_diff <- left_join(zonal_df_diff,cd)
zonal_wide <- pivot_wider(zonal_df_diff, id_cols=all_of(cols), names_from=c(raster_name), values_from = mean)

#Caluclate change potential for SDR Service
zonal_wide <- zonal_wide %>%
  mutate(Pot_Sed_ret_pct_ch = (Pot_Sed_ret_2020 - Pot_Sed_ret_1992) / Pot_Sed_ret_1992 * 100)

# add the attributes

poly <- left_join(poly, zonal_wide, by= cols)
st_write(poly, here('vector', set), append = FALSE) 
write.csv(zonal_df_diff, here('output_data', paste0('pot_sed_ret_df_', col, '.csv')))
```







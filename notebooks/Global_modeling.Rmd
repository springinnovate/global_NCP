---
title: "Global ES Extraction"
output:
  html_document:
    df_print: paged
---

####################################################################### 

# Explore Data.

The ecorregions Raster is not suited, 830 different values. Could be
done but does not make a lot of sense
# 1. Prepare Environemnt (Load necessary libraries)

```{r load libraries, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats', 'foreign')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
```

Key aspect:

Global NCP change maps: country breakdowns -> income groups; geographic regions (IPBES? Or just continents); individual countries that have changed the most
zHow much land cover change did you get, how much biophysical change did you get, how much service change?
Which specific transitions are service change attributable to? (e.g., forest to cropland vs. forest to grassland)

# Goal
Global NCP change maps: country breakdowns -> income groups; geographic regions (IPBES? Or just continents); individual countries that have changed the most
How much land cover change did you get, how much biophysical change did you get, how much service change?
Which specific transitions are service change attributable to? (e.g., forest to cropland vs. forest to grassland)





```{r explore ecoregions, eval=FALSE, include=FALSE}

ecoregions <- rast(here("Downloaded_data_ES", "Ecoregions2017_compressed_md5_316061.tif"))
ec_uniqe <- unique(ecoregions)
```

Use vector file, just the main biomes.

# Load Data

By World Bank Region
```{r filter out PAs 1, eval=FALSE, include=FALSE}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
<<<<<<< HEAD
#poly <- poly %>% filter(region_wb!="Antarctica")# %>% filter(region_wb!="East Asia & Pacific")
nam <- sort(unique(poly$iso3))
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
nam <- gsub("__", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$iso3)
```

By Income level
```{r filter out PAs 2, eval=FALSE, include=FALSE}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
nam <- sort(unique(poly$income_grp))
poly <- poly %>% filter(region_wb!="Antarctica")
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$income_grp)
=======
poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
nam <- sort(unique(poly$region_wb))
poly <- poly %>% filter(region_wb!="Antarctica")
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$region_wb)
>>>>>>> dev
```

By Country
```{r filter out PAs 3, eval=FALSE, include=FALSE}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
poly <- poly %>% group_by(iso3) %>% summarize(geom =st_union(geom)) %>% ungroup()
#Exclude countries/territories too far North/South
exclude_iso3 <- c("ATA", "FRO", "ISL", "FIN", "GRL", "NOR")
# Filter the sf object
poly <- poly %>% filter(!iso3 %in% exclude_iso3)
nam <- sort(unique(poly$iso3))
poly <- poly%>%split(.$iso3)
```



```{r select data to process}

inpath <- "/home/jeronimo/OneDrive/GlobalNatCAP/Global_ES_mapping/Downloaded_data" 
outpath <- "/home/jeronimo/OneDrive/GlobalNatCAP/Global_ES_mapping/Income_level"
# do not include data with a differnet crs here, we will deal with that on the next chunk.

tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))
#Remove points, risk reduction baseline
<<<<<<< HEAD
#iffes <- tiffes[-c(2,4)]
=======
tiffes <- tiffes[-c(2,4)]
>>>>>>> dev
tiffes <- tiffes[4]

tf <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5_*", "", filename)
  sub(".tif.*", "", filename)
}
tf <- clean_filename(basename(tf))
```

This runs the same process in batch. Memory was optimized to not load all the data at thesame time, but process and write each raster individually before going to the next one.  

<<<<<<< HEAD
This is way faster than the second approach (below). 
```{r change_map_calc, eval=FALSE, include=FALSE}
esat <- c(here("Downloaded_data_ES", "ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif"),here("Downloaded_data_ES", "ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif"))
=======


```{r iterate cropMask, eval=FALSE, include=FALSE}
clean_filename <- function(filename){
  #sub("_md5.*", "", filename)
  sub(".tif.*", "", filename)
}

# Loop through each geotiff file
for (i in seq_along(tiffes)) {
  # Get the current tiff file
  tiff_file <- tiffes[i]
  
  # Load the current geotiff (don't load all at once)
  access <- rast(tiff_file)
  
  # Apply crop and mask operations to each polygon
  cropped <- map(1:length(poly), function(x) crop(access, poly[[x]]))
  masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(tiff_file))
  #base_filename <- gsub("[[:punct:]]", "", base_filename) # Remove punctuation
#base_filename <- gsub(" ", "_", base_filename) # Replace spaces with underscores
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) {
    writeRaster(masked[[x]], paste0(outpath,"/", base_filename, "_", nam[x], ".tif"), overwrite = TRUE)
  })
  # Optionally, clear the variable to free up memory
  rm(access, cropped, masked)
  gc()  # Garbage collection to clear memory
}
```


# By Income level
# Region Wb
```{r cropmask}
rs <- lapply(tiffes, rast) 
# Pending to add: run this in parallel!
# Apply cropping and masking recursively using lapply and map

num_cores <- 10
# Parallelize the outer loop using mclapply
rs_msk <- mclapply(rs, function(raster) {
  # Use lapply for the inner loop
  lapply(poly, function(polygon) {
    # Crop and mask the raster
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
}, mc.cores = num_cores) 

# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})
```


```{r}
summary_df <- map2_dfr(seq_along(rs_msk), rs_msk, function(i, inner_list) {
  
  # i corresponds to the index of the outer list (e.g., 1, 2, 3...)
  outer_label <- paste0("Outer_", i)  # Label for the outer list
  
  # Loop through each SpatRaster in the inner list
  map_dfr(inner_list, function(raster) {
    extract_raster_summary(raster, outer_label)
  })
})


for (i in seq_along(rs_msk)) {
  sublist <- rs_msk[[i]]  # Access each sublist
  
  for (j in seq_along(sublist)) {
    raster <- sublist[[j]]  # Access each SpatRaster
    raster_name <- names(raster)  # Get the name slot of the SpatRaster
    
    # Build the output filename
    output_filename <- paste0(outpath, '/', raster_name, "_", nam[j], ".tif")
    
    # Save the raster to a .tif file
    writeRaster(raster, filename = output_filename, overwrite = TRUE)
  }
}


```
ok I need you help drafting an email to my dissertation committee that I really struggle with 
# Split by Countries/continents


# mask by protected areas (all togeter, chategories I-VI)

# Reclassify Land Cover maps.

# Change Maps

```{r change_map_calc}
esat <- c('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif','/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif')

>>>>>>> dev

esas <- lapply(esat,rast)
nam <- c(2020,1992)

# Define the reclassification rules in one operation
reclass_table <- data.frame(
  from = c(10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100,
           110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180,
           190, 200, 201, 202, 210, 220),
  to = c(1, 1, 1, 1, 1, 1,   # Cultivated
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # Forests
         3, 3, 3, 3, 3,   # Grasses and Shrubs
         4, 4, 4, 4, 4,   # Sparse Vegetation
         5, 5, 5,   # Mangroves
         6,   # Urban
         7, 7, 7,   # Bare
         8,   # Water
         9)   # Ice
)
esas <- subst(esas, from = reclass_table$from, to = reclass_table$to)
writeRaster(esas, paste0(here("ESA_LC",'ESA_LC_1992_2020.tif')))
```

```{r iterate cropMask, eval=FALSE, include=FALSE}
# Define input and output paths
inpath <- here("ESA_LC")
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)), pattern = '.tif$'))
tiffes <- tiffes[4]  # Adjust this to include all or specific files
outpath <- here("ESA_LC", "country")
dir.create(outpath, showWarnings = FALSE)  # Ensure the output directory exists

# Define a function to clean filenames
clean_filename <- function(filename) {
  sub(".tif.*", "", filename)
}

# Function to process a single raster with the list of polygons
process_raster <- function(tiff_file, poly, nam, outpath) {
  # Debugging: Print the raster file being processed
  message("Processing raster: ", tiff_file)
  
  # Function to process each polygon in the list
  process_polygon <- function(idx, tiff_file, poly, nam, outpath) {
    tryCatch({
      message("Processing polygon group: ", nam[idx])
      
      # Load the raster inside the function
      access <- rast(tiff_file)
      if (!inherits(access, "SpatRaster")) stop("Failed to load raster: ", tiff_file)
      
      # Convert the polygon group from `sf` to `SpatVector`
      poly_group <- vect(poly[[idx]])  # Convert sf to SpatVector
      if (!inherits(poly_group, "SpatVector")) stop("Failed to convert polygon: ", nam[idx])
      
      # Perform crop and mask
      cropped_masked <- mask(crop(access, poly_group), poly_group)
      
      # Define the output filename
      filename <- paste0(outpath, "/", "ESA_LC_", nam[idx], ".tif")
      
      # Write the raster
      writeRaster(cropped_masked, filename, overwrite = TRUE)
      message("Successfully processed: ", nam[idx])
      return(filename)
    }, error = function(e) {
      message("Error processing: ", nam[idx], " - ", e$message)
      return(NULL)
    })
  }
  
  # Use parallel processing for the list of polygon groups
  cl <- makeCluster(detectCores() - 1)  # Use all but one core
  clusterExport(cl, c("tiff_file", "poly", "nam", "outpath", "mask", "crop", "writeRaster", "vect", "rast"))
  clusterEvalQ(cl, library(terra))  # Load terra in workers
  
  # Process all polygons in parallel
  results <- parLapply(cl, seq_along(poly), function(idx) {
    process_polygon(idx, tiff_file, poly, nam, outpath)
  })
  
  # Clean up cluster
  stopCluster(cl)
  return(results)
}

results <- process_raster(tiffes, poly, nam, outpath)

```

## Prodcue Change Maps/Run Similairty Analysis


# Change Analysis

# Pending to test. leave here fopr the moment 
```{r cropmask LC_maps, eval=FALSE, include=FALSE}
esas <- rast(here("ESA_LC",'ESA_LC_1992_2020.tif'))


poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
#poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
poly <- poly %>% filter(region_wb!="Antarctica")
nam <- sort(unique(poly$region_wb))
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
nam <- gsub("__", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$region_wb)
# # Extract forests
# esas <- lapply(esas, function(r) {
#   subst(r, from = 2, to =1, others=NA)
# })
# 
# writeRaster(esas, paste0(here("LC", "ESA_FORESTS_1992-2020.tif")))


lapply(poly, function(polygon) {
    # Crop and mask the raster
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})


# Pending to add: run this in parallel!
# Apply cropping and masking recursively using lapply and map
outpath <- here("ESA_LC")
num_cores <- 10
# Parallelize the outer loop using mclapply
rs_msk <- mclapply(poly, function(p) {
    r <- crop(esas, p)
    r <- mask(r,p)
    return(r)
  },mc.cores = num_cores) 

map(1:length(rs_msk),function(x) writeRaster(rs_msk[[x]], paste0(here("ESA_LC", "wb_region"),'/', "LC_", nam[x], ".tif")))
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j){
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})

```


## Divide the global maps in manageable datasets. 
World Bank Regions seems like a manageable approach  
```{Cropmask landcovers}

tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))
#Remove points, risk reduction baseline
#iffes <- tiffes[-c(2,4)]
tiffes <- tiffes[4]

tf <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5_*", "", filename)
  sub(".tif.*", "", filename)
}
tf <- clean_filename(basename(tf))
```
##Produce change maps, and run similarity Analysis.

Produce and refernece bi-temporal land cover maps followed by similarity analysis/systematic change assessment. 


```{r chabge map and change analyis}

chg <- ch_mapR(esas[[1]],esas[[2]])
freq(chg)

mat_1 <- crosstabm(esas[[1]],esas[[2]])

diffs_p <-  diffTablej(mat, digits=3, analysis = "change")


```

# Get Change Maps 

```{r calculate change maps}

inpath <- '~/Documents/MN_Applied_Economics/Global_ES_mapping/ESA_LC'
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))

es <- lapply(tiffes, rast)
ch <- ch_mapR(es)
```


# Get Sankey Diagrams

```{r Sankey Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```


ok we are almost there , but we need to refine. right now on the axis, all the bands are included in each box plot, event if there is only one appearing on the plot

I am trying to synchronize my onedrive folder from linux.however, in my workplace they are quite annoying and you need to log in again all the time, and I am getting this error: The refresh token has expired or is invalid due to sign-in frequency checks by conditional access. and "ERROR: You will need to issue a --reauth and re-authorise this client to obtain a fresh auth token.". show me how to do it
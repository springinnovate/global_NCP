---
title: "Hotspot Extraction Pipeline"
subtitle: "v1.0.1"
author: "Jerónimo Rodríguez Escobar"
date: last-modified
date-format: "YYYY-MM-DD HH:mm zzz"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
  pdf:
    toc: true
    number-sections: true
    geometry: margin=1in
editor: source
---

```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
library(tidyr)
library(forcats)
library(scales)
library(RColorBrewer)
library(htmltools)
library(leaflet)
library(devtools)
library(reticulate)
library(exactextractr)
library(httpgd)
load_all()

# set venv for Python
use_virtualenv("/home/jeronimo/venvs/coastal_snap_env", required = TRUE)

# Centralize data roots (set GLOBAL_NCP_DATA in ~/.Renviron)
data_root <- data_dir()
interim_dir <- data_interim()
processed_dir <- data_processed()

processed_tables_dir <- file.path(processed_dir, "tables")
processed_hotspots_dir <- file.path(processed_dir, "hotspots")
processed_coastal_dir <- file.path(processed_dir, "coastal")

# Source unified hotspot/change helpers
source(here::here("R", "utils_hotspot.R"))

# Legacy locations (kept for tracking):
# inpath <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data"
# inpath <- "/home/jeronimo/OneDrive/PROJECTS/Global_NCP/Final_outputs"
inpath <- processed_dir
# This maps raw column names from synth outputs to canonical names.
rename_list <- list(
  c(
    usle_1992_mean = "global_usle_marine_mod_ESA_1992_mean",
    usle_2020_mean = "global_usle_marine_mod_ESA_2020_mean",
    nature_access_1992_mean = "nature_access_lspop2019_ESA1992_mean",
    nature_access_2020_mean = "nature_access_lspop2019_ESA2020_mean",
    n_ret_ratio_1992_mean = "N_ret_ratio_1992_mean",
    n_ret_ratio_2020_mean = "N_ret_ratio_2020_mean",
    sed_ret_ratio_1992_mean = "Sed_ret_ratio_1992_mean",
    sed_ret_ratio_2020_mean = "Sed_ret_ratio_2020_mean",
    coastal_protection_Rt_1992_mean = "coastal_protection_Rt_1992_mean",
    coastal_protection_Rt_2020_mean = "coastal_protection_Rt_2020_mean",
    coastal_protection_Rt_nohab_all_1992_mean = "coastal_protection_Rt_nohab_all_1992_mean",
    coastal_protection_Rt_nohab_all_2020_mean = "coastal_protection_Rt_nohab_all_2020_mean",
    coastal_protection_Rt_ratio_1992_mean = "coastal_protection_Rt_ratio_1992_mean",
    coastal_protection_Rt_ratio_2020_mean = "coastal_protection_Rt_ratio_2020_mean"
  ),
  c(
    n_export_1992_sum = "global_n_export_tnc_esa1992_sum",
    n_export_2020_sum = "global_n_export_tnc_esa2020_sum",
    n_retention_1992_sum = "global_n_retention_ESAmar_1992_fertilizer_sum",
    n_retention_2020_sum = "global_n_retention_ESAmar_2020_fertilizer_sum",
    sed_export_1992_sum = "global_sed_export_marine_mod_ESA_1992_sum",
    sed_export_2020_sum = "global_sed_export_marine_mod_ESA_2020_sum",
    pollination_1992_sum = "realized_polllination_on_ag_ESA1992_sum",
    pollination_2020_sum = "realized_polllination_on_ag_ESA2020_sum"
  )
)

```

::: callout-warning
**TODO / cleanup (run heavy chunks only when inputs are ready)**

- Paths are centralized via `data_dir()`; keep `GLOBAL_NCP_DATA` set.
- Keep expensive consolidation chunks `eval: false` until the latest `10k_grid_synth_{serv,benef,coastal}_*.gpkg` files are in `interim/`.
- Use this file as the living pipeline record; promote stable text to README/methods.
- Maintain the AOO equal-area grid as canonical; use the 4326 clean copy for raster extraction, then wrap dateline for display/output hygiene.
:::

# Pipeline Overview

For a detailed description of the full pipeline methodology, see README_pipeline.md.

This notebook focuses on the **consolidation** step: synthesizing zonal stats, calculating change, and preparing the canonical dataset.

::: {.callout-tip icon="true"}
## Future Tasks & Ideas

Here are some ideas and future tasks for this analysis:

1.  **Adapt analysis for multi-temporal data:** Adapt analysis to handle updated modeled ES layers and multiple points in time (beyond bi-temporal T0, T1). Strategize for incorporating multi-temporal data.
2.  **Quantify hotspot vs. non-hotspot change:** Develop a method to quantify and visualize the share of total change (from bar plots) that occurs within hotspots versus outside of them, possibly using stacked bar plots.
:::

## Data Inputs

Global 10 km grid covering terrestrial areas worldwide (polygon-based template).

### Canonical 10 km grid (IUCN AOO, land-only)

We do **not** generate the grid from scratch. We use the published IUCN
Area of Occupancy (AOO) 10×10 km equal-area grid and subset it to land
by intersecting with the correspondence polygons. We keep the full grid
cells that touch land (no clipping) and append attributes used for
subregional aggregation (country/continent/region/biome).

```{r prep-grid-aoo-land, eval=FALSE}
library(sf)
library(dplyr)

grid_raw <- st_read(data_vectors("AOOGrid_10x10kmShp/AOOGrid_10x10km.shp"))
land_polys <- st_read(
  data_vectors("cartographic_ee_ee_r264_correspondence.gpkg"),
  layer = "ee_r264_correspondence"
)

if (st_crs(grid_raw) != st_crs(land_polys)) {
  land_polys <- st_transform(land_polys, st_crs(grid_raw))
}

# Keep grid cells that intersect any land polygon
grid_land <- st_filter(grid_raw, land_polys, .predicate = st_intersects)

# Attach attributes for aggregation (spatial join or fid-based join)
land_attrs <- land_polys |>
  dplyr::select(
    iso3, nev_name, continent, region_un, region_wb,
    income_grp, WWF_biome
  )
grid_land <- st_join(grid_land, land_attrs, join = st_intersects, left = TRUE)

st_write(
  grid_land,
  data_vectors("AOOGrid_10x10km_land.gpkg"),
  delete_layer = TRUE
)
```

```{python}
#| label: make-grid-valid-shapely
#| eval: false
#| message: false
#| warning: false

# Optional: create a Shapely-validated grid copy to avoid GEOS errors in Python.
# Requires geopandas/shapely in the Quarto Python environment.
import os
import geopandas as gpd

data_root = os.environ.get("GLOBAL_NCP_DATA", "")
grid_in = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land.gpkg")
grid_out = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land_shpvalid.gpkg")

gdf = gpd.read_file(grid_in)
gdf["geometry"] = gdf.geometry.make_valid()
gdf = gdf[gdf.geometry.notna() & gdf.is_valid]

gdf.to_file(grid_out, driver="GPKG")
print(f"wrote {len(gdf):,} features -> {grid_out}")
```

```{python}
#| label: grid-4326-clean
#| eval: false
#| message: false
#| warning: false

# Create a 4326 version for raster extraction (avoids to_crs errors at runtime).
# This keeps the same cells, only expressed in EPSG:4326.
import geopandas as gpd
from shapely.ops import transform
from pyproj import Transformer

grid_in = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land_shpvalid.gpkg")
grid_out = os.path.join(data_root, "vector_basedata", "AOOGrid_10x10km_land_4326_clean.gpkg")

gdf = gpd.read_file(grid_in)
transformer = Transformer.from_crs(gdf.crs, "EPSG:4326", always_xy=True)

bad = 0
new_geoms = []
for geom in gdf.geometry:
    try:
        g = transform(transformer.transform, geom)
        if g.is_empty or not g.is_valid:
            bad += 1
            new_geoms.append(None)
        else:
            new_geoms.append(g)
    except Exception:
        bad += 1
        new_geoms.append(None)

gdf["geometry"] = new_geoms
gdf = gdf[gdf.geometry.notna()]
gdf = gdf.set_crs("EPSG:4326", allow_override=True)
gdf.to_file(grid_out, driver="GPKG")
print(f"dropped {bad:,} invalid geometries; wrote -> {grid_out}")
```

**Canonical grids used downstream**

- **Equal-area canonical grid**: `AOOGrid_10x10km_land.gpkg`
- **Extraction grid (EPSG:4326)**: `AOOGrid_10x10km_land_4326_clean.gpkg`

The 4326 file is only for raster extraction; the cell geometry is the same
10×10 km equal-area grid expressed in a different CRS.

Coastal protection model outputs (1992 and 2020) originally as points spaced \~500 m along shorelines.

Global raster datasets for nitrogen retention/export/ratio, sediment retention/export/USLE, pollination, and nature access (1992 and 2020).

Country polygon dataset for spatial aggregation and attribution.


# Data processing

## Consolidate Synthesis Data

Assemble the final gridded Change/Beneficiary dataset by joining the interim synthesis files (Services, Beneficiaries, Coastal).

```{r}
#| label: assemble-final-grid
#| eval: true
#| include: false
source(here::here("R", "paths.R"))
interim_dir <- data_interim()
processed_dir <- data_processed()
processed_coastal_dir <- file.path(processed_dir, "coastal")

latest_interim <- function(pattern) {
  files <- list.files(interim_dir, pattern = pattern, full.names = TRUE)
  if (!length(files)) stop("No interim files match pattern: ", pattern)
  files[which.max(file.mtime(files))]
}

inpath <- processed_dir
# Optional: export name indexes to CSV
write_index <- FALSE
write_base_synth <- TRUE

# New workflow: means + sums are in the same synth output.
serv_path  <- latest_interim("^10k_grid_services(.*)?\\.gpkg$")
ben_path   <- latest_interim("^10k_grid_benef(.*)?\\.gpkg$")
coast_path <- latest_interim("^10k_grid_coastal(.*)?\\.gpkg$")


S <- st_read(serv_path)
B <- st_read(ben_path)
CP <- st_read(coast_path)

if (!"fid" %in% names(S)) S$fid <- seq_len(nrow(S))
if (!"fid" %in% names(B)) B$fid <- seq_len(nrow(B))
if (!"fid" %in% names(CP)) CP$fid <- seq_len(nrow(CP))

sf_f <- S |>

# Immediately normalize variable names after joining
S_tbl <- st_drop_geometry(S)
B_tbl <- st_drop_geometry(B)
CP_tbl <- st_drop_geometry(CP)

sf_f <- S |>
  left_join(B_tbl, by = "fid") |>
  left_join(CP_tbl, by = "fid")

# Canonical renaming: apply once, early, and only here
rename_map <- c(rename_list[[1]], rename_list[[2]])
sf_f <- dplyr::rename(sf_f, !!!rename_map)


# Ensure c_fid is present (critical for downstream hotspot extraction)
if (!"c_fid" %in% names(sf_f)) {
  sf_f$c_fid <- sf_f$fid
}

# Optional: save the base synthesis (raw 1992/2020 + beneficiaries + coastal)
if (isTRUE(write_base_synth)) {
  st_write(sf_f, file.path(processed_dir, "10k_grid_synth_all.gpkg"), delete_dsn = TRUE)
}

 # Services-only synth lives in interim (10k_grid_services.gpkg) if needed for QA.

```

### next
```{r compute-bitemporal-change}
#| eval: true
#| include: false
# Assumes `sf_f` is already created in the consolidation chunk above.
# If not, load the latest base synthesis from processed.
# This keeps the change calculation resumable without rerunning consolidation.
if (!exists("processed_dir")) {
  if (!exists("data_processed")) source(here::here("R", "paths.R"))
  processed_dir <- data_processed()
}
if (!exists("compute_change")) {
  devtools::load_all(here::here(), quiet = TRUE)
}

if (!exists("sf_f")) {
  synth_path <- file.path(processed_dir, "10k_grid_synth_all.gpkg")
  sf_f <- st_read(synth_path, quiet = TRUE)
}
# Calculate bi-temporal change in % and absolute terms.
# pct_chg uses the earliest year as baseline: ((t1 - t0) / t0) * 100.
# Division by zero yields Inf/NaN; handle downstream if needed.
# Use pct_mode = "symm" to avoid counter-intuitive sign flips in percent change
# when baseline values are negative. The symmetric formula ensures that the
# sign of the percentage change is always consistent with the sign of the
# absolute change.
sf_f <- compute_change(
  sf_f,
  suffix = c("_sum", "_mean"),
  change_type = "both",
  drop_columns = FALSE,
  pct_mode = "symm"
)

# NaN are divisions by 0, NA is when values are NA
write_full_change <- TRUE

# Expanded change synthesis (raw + change) for QA/validation
if (isTRUE(write_full_change)) {
  st_write(
    sf_f,
    file.path(processed_dir, "10k_grid_ES_change_benef.gpkg"),
    layer = "10k_grid_ES_change_benef",
    delete_dsn = TRUE
  )
}


```
# ============================================================================
# AUTHORITATIVE GRID + GROUPING + CHANGE DATASET
# ============================================================================
# This chunk produces 10k_change_calc.gpkg, the canonical input for:
#   - analysis/hotspot_extraction.qmd
#   - analysis/KS_tests_hotspots.qmd
#   - Any downstream aggregation/plotting
#
# Grid attributes (fid, c_fid, iso3, continent, region_wb, income_grp,
# region_un, subregion, WWF_biome, BIOME) are attached via Consolidation.qmd
# and NOT re-derived elsewhere.
#
# If this file is missing or stale, regenerate by running the full
# Consolidation.qmd workflow.
# ============================================================================
```{r filter-canonical-change-prep}
#| eval: true
#| include: false
# STEP 1: Prepare all column lists and perform an initial slim-down of sf_f.
# This avoids doing too many complex operations in a single chunk.

if (!"fid" %in% names(sf_f)) {
  sf_f$fid <- seq_len(nrow(sf_f))
}

# Define service rename mapping (same as in consolidation chunk).

keep_fixed <- c(
  "fid", "c_fid", "iso3", "continent", "region_un", "region_wb",
  "income_grp", "subregion", "WWF_biome", "BIOME"
)

benef_pattern <- "^(GHS_|GlobPOP_|hdi_|rast_gdpTot_|rast_adm1_gini_|fields_mehrabi_)"

# Drop raw/base-year service columns; we only keep change metrics here.
message("Dropping raw year columns to reduce memory footprint...")
year_cols <- grep("1992|2020", names(sf_f), value = TRUE)
benef_cols <- grep(benef_pattern, names(sf_f), value = TRUE)
drop_year_cols <- setdiff(year_cols, benef_cols)

if (length(drop_year_cols) > 0) {
  sf_f <- sf_f[, !names(sf_f) %in% drop_year_cols]
  # free memory after the drop
  rm(year_cols, benef_cols, drop_year_cols)
  gc()
}
message("Initial column drop complete.")

# Now, define the final set of columns to keep.
chg_cols <- grep("_(abs|pct)_chg$", names(sf_f), value = TRUE)
if (!length(chg_cols)) {
  message("No change columns found in sf_f; probing full dataset file I/O (n_max=1)...")
  # quick probe to ensure the file is accessible and not stuck on IO
  try({
    sf::st_read(file.path(processed_dir, "10k_grid_ES_change_benef.gpkg"), n_max = 1, quiet = TRUE)
    message('Probe read OK; performing full read...')
  }, silent = TRUE)
  sf_f <- st_read(file.path(processed_dir, "10k_grid_ES_change_benef.gpkg"), quiet = FALSE)
  if (!"fid" %in% names(sf_f)) {
    sf_f$fid <- seq_len(nrow(sf_f))
  }
  chg_cols <- grep("_(abs|pct)_chg$", names(sf_f), value = TRUE)
}

# Ensure c_fid is preserved/normalized before final selection
# TODO: Clarify legacy usage of c_fid. Originally used for non-spatial country joins.
# Currently serves as a stable unique identifier for grid cells (often == fid).
# In future refactors, consider unifying strictly on 'fid' if c_fid is redundant.
if (!"c_fid" %in% names(sf_f)) {
  if ("c_fid.x" %in% names(sf_f)) sf_f <- dplyr::rename(sf_f, c_fid = c_fid.x)
  else if ("c_fid.y" %in% names(sf_f)) sf_f <- dplyr::rename(sf_f, c_fid = c_fid.y)
  else if ("id" %in% names(sf_f))      sf_f <- dplyr::rename(sf_f, c_fid = id)
}
if (!"c_fid" %in% names(sf_f)) {
  sf_f$c_fid <- sf_f$fid
}

# Define canonical services and beneficiaries to keep
svc_cols <- names(c(rename_list[[1]], rename_list[[2]]))
svc_base <- unique(stringr::str_replace(svc_cols, "_(1992|2020)_", "_"))

canonical_change <- unique(c(
  paste0(svc_base, "_abs_chg"),
  paste0(svc_base, "_pct_chg")
))
chg_cols <- intersect(chg_cols, canonical_change)

benef_keep <- intersect(
  c(
    paste0(c("fields_mehrabi_2017", "GHS_BUILT_S_E2020", "hdi_raster_predictions_2020", "rast_adm1_gini_disp_2020"), "_mean"),
    paste0(c("GHS_POP_E2020_GLOBE", "GlobPOP_Count_30arc_2020", "rast_gdpTot_1990_2020_30arcsec_2020"), "_sum")
  ),
  names(sf_f)
)

# This is the final list of columns for our canonical dataset
keep_cols <- unique(c(
  keep_fixed,
  benef_keep,
  chg_cols
))
keep_cols <- setdiff(keep_cols, c("RasterVal", "id_2"))

message("Beneficiary columns to keep (n = ", length(benef_keep), "):")
print(benef_keep)

svc_meta <- tibble::tibble(col = svc_cols, stat = stringr::str_match(svc_cols, "_(mean|sum)$")[, 2], base = stringr::str_replace(svc_cols, "_(1992|2020)_", "_")) |>
  dplyr::distinct(base, stat)
message("Service stats kept (canonical):")
print(svc_meta)
message("Change columns to keep (n = ", length(chg_cols), "):")
print(head(chg_cols, 25))
message('Freeing temporary objects before heavy selection...')
rm(svc_meta); gc()

```

```{r filter-canonical-change-select}
#| eval: true
#| include: false
# STEP 2: Perform the main, heavy selection operation.
# This is isolated to prevent memory hangs.
message("Creating canonical subset by selecting final columns...")
# attempt a small test of selection to detect schema issues quickly
message('Testing select on a small subset (first 10 rows)')
try({ tmp_test <- sf_f[seq_len(min(10, nrow(sf_f))), ] |> dplyr::select(any_of(keep_cols)); rm(tmp_test) }, silent = TRUE)
gc()
t0 <- Sys.time();
cols_to_keep <- intersect(keep_cols, names(sf_f))
sf_canon <- sf_f[, cols_to_keep]
rm(sf_f); gc()
message('Canonical subset created. Elapsed (mins): ', round(as.numeric(difftime(Sys.time(), t0, units = 'mins')), 2))
```

```{r filter-canonical-change-join}
#| eval: true
#| include: false
# STEP 3: Join missing grid attributes if necessary and finalize schema.
grid_attrs <- st_read(
  file.path(data_interim(), "..", "vector_basedata", "AOOGrid_10x10km_land_4326_clean.gpkg"),
  quiet = TRUE
) |>
  st_drop_geometry() |>
  dplyr::select(any_of(keep_fixed))

if (!"fid" %in% names(grid_attrs)) {
  grid_attrs$fid <- seq_len(nrow(grid_attrs))
}

# If fixed grouping columns are missing after the select, join them back.
if (!all(keep_fixed %in% names(sf_canon))) {
  message("Fixed grouping columns are missing, joining them back...")
  gc(); t_join <- Sys.time()
  sf_canon <- sf_canon |>
    left_join(grid_attrs, by = "fid")
  message('Join complete. Elapsed (secs): ', round(as.numeric(difftime(Sys.time(), t_join, units = 'secs')), 1))

  # Clean up potential join artifacts if c_fid existed in both
  if ("c_fid.x" %in% names(sf_canon)) {
    sf_canon$c_fid <- dplyr::coalesce(sf_canon$c_fid.x, sf_canon$c_fid.y)
    sf_canon$c_fid.x <- NULL
    sf_canon$c_fid.y <- NULL
  }
}

# Re-apply selection to ensure only intended columns remain after the join
sf_canon <- sf_canon |> dplyr::select(any_of(keep_cols))
message("Final column schema enforced.")
```

```{r write-canonical-change}
#| eval: true
#| include: false
# Clean service change names only (drop _mean/_sum for canonical services).
rename_change <- function(nm) {
  stringr::str_replace(nm, "_(mean|sum)_(abs|pct)_chg$", "_\\2_chg")
}
svc_change <- c(
  paste0(svc_base, "_abs_chg"),
  paste0(svc_base, "_pct_chg")
)
svc_change <- intersect(svc_change, names(sf_canon))
if (length(svc_change)) {
  sf_canon <- dplyr::rename_with(sf_canon, rename_change, .cols = all_of(svc_change))
}

st_write(
  sf_canon,
  file.path(processed_dir, "10k_change_calc.gpkg"),
  layer = "10k_change_calc",
  delete_dsn = TRUE
)
```

Note: coastal protection is prepared upstream (join → ratios → rasterize → summary)
via `Python_scripts/coastal_protection_join.py`, `Python_scripts/rasterize_coastal.py`,
and `analysis_configs/c_protection_synth.yaml`. This document consumes the resulting
coastal synthesis GPKG from `data_interim()`.

## Add Updated Access Data
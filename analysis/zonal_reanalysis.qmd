---
---
title: "Zonal Reanalysis and Change Detection"
format: html
---

# Setup

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)

# Load project paths
if (file.exists(here("R", "paths.R"))) {
  source(here("R", "paths.R"))
} else {
  stop("R/paths.R not found. Please ensure you are in the project root.")
}

# Define I/O paths
input_dir <- file.path(data_dir(), "interim")
output_dir <- file.path(data_dir(), "processed")
plot_dir <- here("outputs", "plots", "zonal_summary")

dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
dir.create(plot_dir, recursive = TRUE, showWarnings = FALSE)

# Define a single source of truth for name cleaning
clean_service_names <- function(column_names) {
  column_names %>%
    str_remove_all("_md5_[a-z0-9]+|_[0-9]{8}_[0-9]{6}") %>%
    str_remove_all("_tnc|_esa|_value|_compressed|_mar|mar|_lspop2019|_ESA|ine_mod_ESA") %>%
    str_replace_all("_Rt_change_coastal_risk.*", "_coastal_risk") %>%
    str_replace_all("_Service_change_coastal_risk.*", "_coastal_service") %>%
    str_replace_all("_nature_access_.*", "_nature_access") %>%
    str_replace_all("_Nitrogen_Export_diff", "_nitrogen_export") %>%
    str_replace_all("_Pollination_diff", "_pollination") %>%
    str_replace_all("_Sediment_Export_diff", "_sediment_export") %>%
    str_replace_all("__+", "_") %>%
    str_replace_all("_$", "") %>%
    tolower()
}
```

# Data Ingestion

```{r load-data, message=FALSE}
file_list <- list.files(input_dir, pattern = "\\.csv$", full.names = TRUE) %>%
  .[!str_detect(., "data._")]

tt_combined <- map_df(file_list, ~{
  df <- read_csv(.x, show_col_types = FALSE)
  filename <- basename(.x)
  grp_name <- str_remove(filename, "_[0-9]{8}_[0-9]{6}\\.csv$")

  # Robust renaming of the first column to 'unit'
  colnames(df)[colnames(df) == "unit"] <- "unit_original"
  colnames(df)[1] <- "unit"

  df %>%
    mutate(grouping = grp_name, unit = as.character(unit)) %>%
    select(grouping, unit, everything())
})

write_csv(tt_combined, file.path(output_dir, "zonal_stats_combined.csv"))
```

# Filtering and Cleaning

```{r clean-data}
# Keep only relevant summary stats and change variables
tt_filtered <- tt_combined %>%
  select(grouping, unit, starts_with(c("mean_", "stdev_", "valid_count_"))) %>%
  filter(unit != "Antarctica")

# Select only the "change" or "diff" columns
tt_ch <- tt_filtered %>%
  select(grouping, unit, contains("change"), contains("diff"))

# Apply the unified cleaning function
names(tt_ch) <- clean_service_names(names(tt_ch))

write_csv(tt_ch, file.path(output_dir, "change_variables_cleaned.csv"))
```

# Analysis

```{r analysis}
# Reshape to long format to calculate SE across all services
tt_analysis <- tt_ch %>%
  select(-matches("X1|unnamed|source_file")) %>%
  pivot_longer(
    cols = -c(grouping, unit),
    names_to = c(".value", "service"),
    names_pattern = "(mean|stdev|valid_count)_(.*)"
  ) %>%
  mutate(se = stdev / sqrt(valid_count)) %>%
  filter(!is.na(mean), mean != 0) # Focus on meaningful changes

# Save wide version for Becky/Rich (Mean and SE columns)
tt_final_wide <- tt_analysis %>%
  select(grouping, unit, service, mean, se) %>%
  pivot_wider(
    names_from = service,
    values_from = c(mean, se),
    names_glue = "{.value}_{service}"
  )

write_csv(tt_final_wide, file.path(output_dir, "final_ES_change_analysis.csv"))
```

# Visualization

```{r visualization}
# We use a more specific name 'target_group' to avoid any confusion with column names
generate_es_plot <- function(target_group) {

  # Use .env$ to explicitly pull 'target_group' from the function argument
  data_subset <- tt_analysis %>%
    filter(grouping == .env$target_group)

  # Skip if no data for this group
  if(nrow(data_subset) == 0) {
    message(paste("No data found for", target_group))
    return(NULL)
  }

  # Calculate height based on number of units (0.22 inches per unit)
  # This ensures long lists like Countries are readable
  num_units <- length(unique(data_subset$unit))
  calc_height <- max(6, num_units * 0.22)

  message(paste("Generating plot for:", target_group, "(Units:", num_units, ")"))

  p <- ggplot(data_subset, aes(x = mean, y = reorder(unit, mean), fill = service)) +
    geom_col(alpha = 0.7, show.legend = FALSE) +
    geom_errorbar(aes(xmin = mean - se, xmax = mean + se), width = 0.3, color = "grey30") +
    geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
    facet_wrap(~service, scales = "free_x") +
    theme_minimal() +
    labs(
      title = paste("Ecosystem Service Change:", target_group),
      subtitle = "Bars = Mean Change | Error Bars = +/- 1 SE (1992-2020)",
      x = "Mean Change Value",
      y = NULL
    ) +
    theme(
      strip.text = element_text(face = "bold", size = 11),
      axis.text.y = element_text(size = 7)
    )

  # Save with limitsize = FALSE to handle the long country plots
  ggsave(
    filename = file.path(plot_dir, paste0("ES_plot_", target_group, ".png")),
    plot = p,
    width = 14,
    height = calc_height,
    limitsize = FALSE
  )
}

# Get the list of groupings and run the function for each
unique_groupings <- unique(tt_analysis$grouping)

# walk() is like a loop that doesn't print messy output
walk(unique_groupings, generate_es_plot)
```
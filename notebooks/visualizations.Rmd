---
title: "Clean Mapping Data"
output: html_notebook
---


1. Organize output Dataframes (make sure that they are complete and correclty named)


```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
# source the helper functions
source(here::here("R", "utils_lcc_metrics.R"))
source(here("R","utils_pct_change.R"))
```


```{r load polygons, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Read spatial polygon dataset

inpath <- '/Users/sputnik/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/vector'
inpath <- here("vector")
sets <- file.path(inpath, list.files(inpath, pattern= 'gpkg$'))

#cols <- c("id", "continent", "income_grp", "region_wb", "WWF_biome", "HYBAS_ID", "HYBAS_ID")
#sets <- cbind(sets, cols)
# Set an index to select the desired dataset
t <- 10
# Select the target dataset from the list 
set <- sets[t]
# # load polygons
lyr <- st_layers(set)
poly <- st_read(set)

#get the name of the column with the unique ids of the input vector
col <- colnames(poly[1])[1]
```


```{r load and reshape processed geometries, eval=FALSE, include=FALSE}

# get pct as standalone data to add to the table
poly <- st_read(here("vector", "hydrosheds_lv6_synth.gpkg"))
poly <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_ES_TimeSeries/vector/hydrosheds_lv6_synth.gpkg')

plt <- st_drop_geometry(poly)

plt <- as_tibble(plt)
#filter all thins smaller than 150.000 ha (takes away the 54 smallest features) 

#this needs to be updated. 
service <- c("Coastal_Protection", "N_export", "Sed_export", "Usle", "Pollination", "Sed_Ret_R")
color <- c("#9e9ac8", "#2c944c", "#08306b", "#17c0ff", "#dd1c77", "#8C510A")
cd <- as_tibble(cbind(service,color))

################## THIS IS VERY IMPPORTAnt. Instead of struggling  with the multiple dataframes, it is easier to load the vector file swith all the column and pivot longer as necessary. Easier to manage, adjust on the fly!
plt <- as_tibble(plt %>%
  pivot_longer(
    cols = c(ends_with("pct_ch")),  # Select all columns ending with "pct_ch"
    names_to = "service",        # New column to store the service names
    values_to = "pct_ch"         # New column to store the percentage change values
  ))

plt <- plt[-c(2:6,8:10,12,13,23:34)]

# Adjust Dataframe and labels
plt <- plt %>%
  mutate(service = str_remove(service, "_pct_ch$"))

plt <- plt %>% mutate(service = case_when(
  service == "n_export" ~ "N_export",
  service == "sed_export" ~ "Sed_export",
  service == "Sed_retention_ratio" ~ "Sed_Ret_R",
  TRUE ~ service
  ))


plt <- left_join(plt,cd)
#Remove invalid records
plt <- plt%>% filter(!is.na(pct_ch))
# Remove Inf values
plt <- plt %>%  plt %>% filter(pct_ch != Inf).  ## Double check here. Why is Inf here? No problem. Service in period 0 was o, so $ of chng from 0 to any value is inf

write.csv(plt, file= here('output_data', "metrics_change_hb_lev_6.csv"))
```



```{r box whikers, fig.height=8, fig.width=14}

plt <- as_tibble(read.csv(here('output_data', "metrics_change_hb_lev_6.csv")))
plt[1] <- NULL 
# apply filter (5 or 2%) # Remove the smalles basins. That help. Also, it seems that some biomes are more prone to artifacts than others!!!
area_threshold <- quantile(plt$SUB_AREA, probs = 0.05, na.rm = TRUE)
# Filter the dataset
df <- df1 %>% filter(SUB_AREA > area_threshold)


t_5 <- ggplot(df, aes(x = service, y = pct_ch, fill = color)) +
  geom_boxplot() +  # or geom_violin()
  scale_fill_identity() +
  facet_wrap(~ service, scales = "free_y") +
  labs(
    title = "Distribution of % Change by Service (Excl. Smallest 5% of Basins)",
    x = NULL,
    y = "Percentage Change"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    strip.text = element_text(face = "bold"),
    legend.position = "none"
  )

library(ggplot2)
library(dplyr)

# Custom pseudo-log transformation
pseudo_log <- function(x) sign(x) * log1p(abs(x))

# Apply to the filtered dataframe
df <- df %>% 
  mutate(pct_ch_trans = pseudo_log(pct_ch))

# Plot with transformed values
p <- ggplot(df, aes(x = service, y = pct_ch_trans, fill = color)) +
  geom_violin(trim = FALSE, scale = "width") +
  scale_fill_identity() +
  facet_wrap(~ service, scales = "free_y") +
  labs(
    title = "Pseudo-log Scaled % Change by Service",
    x = NULL,
    y = "Transformed % Change"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    strip.text = element_text(face = "bold"),
    legend.position = "none"
  )
p
```
## 5. Here, add plots with the differences 

```{r plot chg 8, fig.height=8, fig.width=14}

plot_es_changes <- function(data, label_col = col, 
                            filter_type = "top_bottom", filter_val = 10) {
  label_sym <- sym(label_col)
  
  # Step 2: Apply filtering based on the chosen type
  if (filter_type == "top_bottom") {
    # Top/bottom n observations per service
    top_bottom <- data %>%
      group_by(service) %>%
      slice_max(pct_ch, n = filter_val, with_ties = FALSE) %>%
      bind_rows(
        data %>%
          group_by(service) %>%
          slice_min(pct_ch, n = filter_val, with_ties = FALSE)
      ) %>%
      ungroup()
    
  } else if (filter_type == "quantile") {
    # Top/bottom quantile per service (e.g., top/bottom 10%)
    top_bottom <- data %>%
      group_by(service) %>%
      filter(pct_ch >= quantile(pct_ch, 1 - filter_val, na.rm = TRUE) |
             pct_ch <= quantile(pct_ch, filter_val, na.rm = TRUE)) %>%
      ungroup()
    
  } else if (filter_type == "all") {
    top_bottom <- data
  } else {
    stop("Invalid `filter_type`. Use 'top_bottom', 'quantile', or 'all'.")
  }
  
  # Step 3: Reorder labels per service
  top_bottom <- top_bottom %>%
    mutate(temp_label = reorder_within(!!label_sym, -pct_ch, service))
  
  # Step 4: Plot
  ggplot(top_bottom, aes(x = temp_label, y = pct_ch, fill = color)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    scale_fill_identity() +
    scale_x_reordered() +
    facet_wrap(~ service, scales = "free", ncol = 3) +
    labs(
      #title = paste("% Change 1992â€“2020 By", cols, sep= " "),
      x = NULL,
      y = "% Change"
    ) +
    theme_bw() +
    theme(
      strip.text = element_text(face = "bold", size = 10),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
    )
}

# Top/bottom 10 countries per service
plot_es_changes(df, label_col = col, filter_type = "top_bottom", filter_val = 10)

# Top/bottom 5% per service
#plot_es_changes(df, label_col = col, filter_type = "quantile", filter_val = 0.1)

# Show all values
#plot_es_changes(plt, filter_type = "all")
```


## Scatterplots

```{r scatterplots LC change}


# List of LC metrics to analyze
lc_metrics <- c("Gain_2", "Persistence_2", "Loss_2", "dir_ch_2")

# Loop over each LC metric and generate a faceted scatterplot
for (metric in lc_metrics) {
  p <- plt %>%
    ggplot(aes_string(x = metric, y = "pct_ch", color = "color")) +
    geom_point(alpha = 0.5, size = 1.2) +
    facet_wrap(~ service, scales = "free") +
    scale_color_identity() +
    labs(
      title = paste("Relationship Between", metric, "and ES % Change"),
      x = metric,
      y = "% Change in Ecosystem Service"
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(face = "bold"),
      plot.title = element_text(hjust = 0.5),
      axis.text = element_text(size = 9)
    )
  
  print(p)
}
```


# Scatterplots change 2

```{r scatterplots LC change2, fig.height=8, fig.width=14  }
library(dplyr)
library(ggplot2)

# LC metrics of interest
lc_metrics <- c("Gain_2", "Persistence_2", "Loss_2", "dir_ch_2")

# Step 1: Remove top 2% outliers for each service
filtered_plt <- plt %>%
  group_by(service) %>%
  mutate(threshold = quantile(pct_ch, 0.98, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(pct_ch <= threshold)

# Step 2: Apply pseudo-log transformation
filtered_plt <- filtered_plt %>%
  mutate(pct_ch_log = sign(pct_ch) * log1p(abs(pct_ch)))

# Step 3: Loop through each LC metric and create faceted plots
for (metric in lc_metrics) {
  p <- ggplot(filtered_plt, aes_string(x = metric, y = "pct_ch_log", color = "color")) +
    geom_point(alpha = 0.5, size = 1.2) +
    facet_wrap(~ service, scales = "free") +
    scale_color_identity() +
    labs(
      title = paste("Log-Transformed % Change vs", metric),
      x = metric,
      y = "Log(% Change in ES)"
    ) +
    theme_minimal() +
    theme(
      strip.text = element_text(face = "bold"),
      plot.title = element_text(hjust = 0.5),
      axis.text = element_text(size = 9)
    )
  
  print(p)
}
```


# Get Sankey Diagrams

```{r Sankey Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```


*Note* This is on;y necessary fof WWF Biomnes, because the names are too long and are hard to fit inside the charts

```{r biomes short}

biome_labels <- c(
  "Temperate Grasslands, Savannas & Shrublands" = "Temperate Grasslands",
  "Tropical & Subtropical Moist Broadleaf Forests" = "Moist Broadleaf Forests",
  "Temperate Conifer Forests" = "Temperate Conifers",
  "Tropical & Subtropical Coniferous Forests" = "Tropical Conifers",
  "Rock & Ice" = "Rock & Ice",
  "Boreal Forests/Taiga" = "Boreal Forests",
  "Montane Grasslands & Shrublands" = "Montane Grasslands",
  "Lakes" = "Lakes",
  "Flooded Grasslands & Savannas" = "Flooded Grasslands",
  "Mediterranean Forests, Woodlands & Scrub" = "Mediterranean Forests",
  "Tropical & Subtropical Dry Broadleaf Forests" = "Dry Broadleaf Forests",
  "Temperate Broadleaf & Mixed Forests" = "Temperate Broadleaf",
  "Tundra" = "Tundra",
  "Tropical & Subtropical Grasslands, Savannas & Shrublands" = "Tropical Grasslands",
  "Mangroves" = "Mangroves"
)
#Only for biomes, t make names shorter
plt <- plt %>%
  mutate(biome_short = recode(WWF_biome, !!!biome_labels))
```





```{r ggplot 2020, echo=FALSE, fig.height=8, fig.width=14}
#

plot_ecosystem_services <- function(data, var, col) {
  col_sym <- sym(col)  # Convert column name to symbol for dplyr
  # Step 1: Prepare Data (Remove NA values and zero mean values, then reorder names)
  data_prepped <- data %>%
    filter(!is.na(mean) & mean > 0 & year == !!year) %>%  # Exclude cases where mean is 0 and filter year
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))  

  # Step 2: Compute min/max values for selected column
  service_range <- data_prepped %>%
    group_by(service) %>%
    summarize(
      min_val = min(pct_chg, na.rm = TRUE),
      max_val = max(pct_chg, na.rm = TRUE),
      .groups = "drop"
    )

  # Step 3: Create "invisible" data for min/max range
  range_data <- service_range %>%
    pivot_longer(cols = c(min_val, max_val), names_to = "range_type", values_to = "mean") %>%
    mutate(year = !!year, temp_col = "dummy")

  # Step 4: Extract top 10 and bottom 10 per selected column
  top_10 <- data_prepped %>%
    group_by(service) %>%
    slice_max(order_by = mean, n = 10, with_ties = TRUE) 

  bottom_10 <- data_prepped %>%
    group_by(service) %>%
    slice_min(order_by = mean, n = 10, with_ties = TRUE)

  # Step 5: Combine only top 10 and bottom 10
  filtered_data <- bind_rows(top_10, bottom_10) %>%
    arrange(service, desc(mean))

  # Step 6: Reorder selected column to appear correctly
  filtered_data <- filtered_data %>%
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))

  # Step 7: Plot the filtered data
  p <- ggplot(filtered_data, aes(x = temp_col, y = mean, fill = color)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    scale_fill_identity() +
    facet_wrap(~ service, scales = "free") +
    scale_x_reordered() +  
    labs(
      title = paste("Mean Ecosystem Service Values,", year),
      x = col,
      y = "Mean Value"
    ) +
    theme_bw() +
    theme(
      strip.text = element_text(face = "bold", size = 12),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 5)
    )
  
  return(p)
}

# Generate plots for different years using a specified column
p_1992 <- plot_ecosystem_services(zonal_df, 1992, col)
p_2020 <- plot_ecosystem_services(zonal_df, 2020, col)

# Display the plots
print(p_1992)
print(p_2020)


```



```{r timeseries plot}


inpath <- here('output_data')
zonal_csv <- file.path(inpath, list.files(paste0(inpath),pattern= 'TS'))

n <- 4
zonal_df <- as_tibble(read.csv(zonal_csv[n]))


# Only for biomes, t make names shorter
 # zonal_df <- zonal_df %>%
 #   mutate(biome_short = recode(WWF_biome, !!!biome_labels))

library(ggplot2)

#zonal_df <- zonal_df %>% filter(WWF_biome!="Lakes")

# Create the line plot
ggplot(zonal_df, aes(x = year, y = mean, color = biome_short, group = biome_short)) +
  geom_line(size = 1) +          # Add lines for each subregion
  geom_point(size = 2) +         # Add points for visibility
  labs(
    title = paste("Ecosystem Service Trends by ", col, sep= ""),
    x = "Year",
    y = "Mean Value"
  ) +
  theme_minimal() +
  theme(legend.position = "right")  # Adjust legend position
```

```{r faceted plot stdev, fig.height=18, fig.width=18}

zonal_df <- zonal_df %>% filter(Sub_Region!="Antarctica")

ggplot(zonal_df, aes(x = year, y = stdev, group = Sub_Region)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 2, color = "red") +
  facet_wrap(~ Sub_Region, scales = "free_y", ncol = 3) +
  labs(
    title = paste("NDR - st_dev", col, sep= ", "),
    x = "Year",
    y = "Mean Value"
  ) +
  theme(
    legend.position = "right",
    text = element_text(size = 14),  # Reduce overall text size
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),  # Adjust x-axis labels
    axis.text.y = element_text(size = 10),  # Adjust y-axis labels
    legend.text = element_text(size = 8),  # Reduce legend font size
    legend.title = element_text(size = 0.1),  # Adjust legend title size
    strip.text = element_text(size = 12)  # Adjust facet labels (if using facets)
  )


```

```{r chartTS country filter}

# Filter to exclude NA values and "Lakes"

# Filter top 5 and bottom 5 per service
top_bottom_df <- zonal_df %>%
  group_by(service, year) %>%
  slice_max(mean, n = 5, with_ties = FALSE) %>%
  bind_rows(
    zonal_df %>%
      group_by(service, year) %>%
      slice_min(mean, n = 5, with_ties = FALSE)
  ) %>%
  ungroup()

ggplot(top_bottom_df, aes(x = year, y = mean, color = name_long, group = name_long)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = paste("Ecosystem Service Trends by ", col, sep = ""),
    x = "Year",
    y = "Mean Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 8)
  )
```


```{r faceted plot count, fig.height=18, fig.width=18}


# 1. Filter valid records
filtered <- zonal_df %>%
  filter(!is.na(mean), !is.na(name_long))

# 2. Compute average change per country (across years) per service
ranked <- filtered %>%
  group_by(service, name_long) %>%
  summarize(avg_mean = mean(mean, na.rm = TRUE), .groups = "drop")

# 3. Extract top and bottom 5 countries per service
top_bottom <- ranked %>%
  group_by(service) %>%
  slice_max(avg_mean, n = 5, with_ties = FALSE) %>%
  bind_rows(
    ranked %>% 
      group_by(service) %>% 
      slice_min(avg_mean, n = 5, with_ties = FALSE)
  ) %>%
  ungroup()

# 4. Filter original data to only those countries and prepare labels
plot_data <- filtered %>%
  filter(name_long %in% top_bottom$name_long) %>%
  mutate(label = reorder_within(name_long, -mean, service))  # reorder per facet

# 5. Plot
ggplot(plot_data, aes(x = year, y = mean, group = name_long, color = name_long)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  facet_wrap(~ service, scales = "free_y") +
  scale_color_viridis_d(option = "C", begin = 0.2, end = 0.9) +
  scale_x_continuous(breaks = unique(zonal_df$year)) +
  scale_y_continuous() +
  labs(
    title = "Top and Bottom 5 Country Values per Ecosystem Service",
    x = "Year",
    y = "Mean Value"
  ) +
  scale_color_discrete(guide = guide_legend(title = "Country")) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 8)
  )

```

```{r ttt}
library(dplyr)
library(ggplot2)

# Step 1: Filter valid rows
filtered <- zonal_df %>%
  filter(!is.na(mean), !is.na(name_long))

# Step 2: Calculate average mean per country across all years, by service
ranked <- filtered %>%
  group_by(service, name_long) %>%
  summarize(avg_mean = mean(mean, na.rm = TRUE), .groups = "drop")

# Step 3: Select top 5 and bottom 5 countries per service
top_bottom <- ranked %>%
  group_by(service) %>%
  slice_max(avg_mean, n = 10, with_ties = FALSE) %>%
  bind_rows(
    ranked %>%
      group_by(service) %>%
      slice_min(avg_mean, n = 0, with_ties = FALSE)
  ) %>%
  ungroup()

# Step 4: Filter zonal_df to keep only those countries
zonal_top_bottom <- filtered %>%
  filter(name_long %in% top_bottom$name_long)

# Optional: Add a shortened country label if needed
zonal_top_bottom <- zonal_top_bottom %>%
  mutate(country_short = name_long)  # you can shorten names if needed

# Step 5: Plot (same structure as your original one, just updated to use countries)
ggplot(zonal_top_bottom, aes(x = year, y = mean, group = name_long)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 2, color = "red") +
  facet_wrap(~ name_long, scales = "free_y", ncol = 4) +
  labs(
    title = paste("NDR - Means,", "Top 10 Countries", sep= ", "),
    x = "Year",
    y = "Mean Value"
  ) +
  theme(
    legend.position = "right",
    text = element_text(size = 14),
    axis.text.x = element_text(size = 8, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 8),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 8),
    strip.text = element_text(size = 10)
  )

```

```{r plot chg2, fig.height=8, fig.width=14}

# 1. Filter out "Nature_Access"
filtered_plt <- plt %>%
  filter(service != "Nature_Access" & !is.na(pct_ch))

# 2. Get top 10 and bottom 10 by service
top_bottom <- filtered_plt %>%
  group_by(service) %>%
  slice_max(pct_ch, n = 10, with_ties = FALSE) %>%
  bind_rows(
    filtered_plt %>%
      group_by(service) %>%
      slice_min(pct_ch, n = 10, with_ties = FALSE)
  ) %>%
  ungroup()

# 3. Reorder country names *within each service*
top_bottom <- top_bottom %>%
  mutate(name_long = reorder_within(name_long, -pct_ch, service))  # descending order


ggplot(top_bottom, aes(x = name_long, y = pct_ch, fill = color)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_identity() +
  scale_x_reordered() +  # <<-- THIS enables correct facet ordering
  facet_wrap(~ service, scales = "free", ncol = 3) +
  labs(
    title = "% Change 1992-2020 By Income Group
    x = NULL,
    y = "% Change"
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
  )
```





# Get Sankey Diagrams

```{r Sankey 2 Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```


write.csv(plt, here('output_data', paste0('pct_chg_',cols, '.csv')))




Percentage of Change. This is the key. I almot have it for everything! 


Variables: 

Pot. Sediment Retention for Each Grouping (Initial/ Final)

######################################################################

Time Series!!!

% percentage of change in the charts.... amount.






I extract change stats (stdev + mean) for 

- Subregions (repeat for stdev)
- Biome (running right now, lost connection to the internet, but just adusting to add the new columns to the vector file)
- Country. Here to ee those with the most change (%) It's the same, its a ratio, so we don't need to put too much effort normalizing or stuff
- Income
 
 
Get all the differentials and include % change in service and in export (so retention and export)....make sure i know which inputs am i using.

Paper: Review question to make sure that i

I extract change stats (stdev + mean) for 

 
Get all the differentials and include % change in service and in export (so retention and export)....make sure i know which inputs am i using.




Pending:

Incorporate the Fertilier EASA database to improve Fertilizer (N) mode.

M<ake sure i m using the right dstasert and olumn! Wrtite tomorrow to justin!!!

Make sure i m using the right dstasert and olumn! Wrtite tomorrow to justin!!!


Connecting LC analysis -> is it possible to identify s relationship betwe them?

How do the amoung of change between nwture tono nature relatw with the amount of change in the provision. This is zero banal


Get emtrics for Retention, export and the calcualtion (in short repeat the whole thing. again)

Get metrics for Retention, export and the calcualtion (in short repeat the whole thing. again)


Re run the export part

```{r create list of columns}

poly <- st_read('/Users/rodriguez/Global_ES_TS/global_NCP/vector/vector_f/Continent.gpkg')

serv <- names(poly)
serv <- serv[-c(1,30)]
serv


#Get filenames (add paths to stored data)





```

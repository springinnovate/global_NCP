---
title: "Globale ES Extraction"
output: html_notebook
---

####################################################################### 

# Explore Data.

The ecorregions Raster is not suited, 830 different values. Could be
done but does not make a lot of sense
# 1. Prepare Environemnt (Load necessary libraries)

```{r load libraries, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats', 'foreign')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
```

Key aspect:

Global NCP change maps: country breakdowns -> income groups; geographic regions (IPBES? Or just continents); individual countries that have changed the most
How much land cover change did you get, how much biophysical change did you get, how much service change?
Which specific transitions are service change attributable to? (e.g., forest to cropland vs. forest to grassland)

# Goal
Global NCP change maps: country breakdowns -> income groups; geographic regions (IPBES? Or just continents); individual countries that have changed the most
How much land cover change did you get, how much biophysical change did you get, how much service change?
Which specific transitions are service change attributable to? (e.g., forest to cropland vs. forest to grassland)





```{r explore ecoregions, eval=FALSE, include=FALSE}

ecoregions <- rast(here("Downloaded_data_ES", "Ecoregions2017_compressed_md5_316061.tif"))
ec_uniqe <- unique(ecoregions)
```

Use vector file, just the main biomes.

# Load Data

By World Bank Region
```{r filter out PAs}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
#poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
poly <- poly %>% filter(region_wb!="Antarctica")
nam <- sort(unique(poly$region_wb))
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
nam <- gsub("__", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$region_wb)
```

By Income level
```{r filter out PAs, eval=FALSE, include=FALSE}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
nam <- sort(unique(poly$income_grp))
poly <- poly %>% filter(region_wb!="Antarctica")
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$income_grp)
```

By Country
```{r filter out PAs, eval=FALSE, include=FALSE}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
poly <- poly %>% group_by(iso3) %>% summarize(geom =st_union(geom)) %>% ungroup()
#Exclude countries/territories too far North/South
exclude_iso3 <- c("ATA", "FRO", "ISL", "FIN", "GRL", "NOR")
# Filter the sf object
poly <- poly %>% filter(!iso3 %in% exclude_iso3)
nam <- sort(unique(poly$iso3))
poly <- poly%>%split(.$iso3)
```



```{r select data to process}

inpath <- "/home/jeronimo/OneDrive/GlobalNatCAP/Global_ES_mapping/Downloaded_data" 
outpath <- "/home/jeronimo/OneDrive/GlobalNatCAP/Global_ES_mapping/Income_level"
# do not include data with a differnet crs here, we will deal with that on the next chunk.

tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))
#Remove points, risk reduction baseline
#iffes <- tiffes[-c(2,4)]
tiffes <- tiffes[4]

tf <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5_*", "", filename)
  sub(".tif.*", "", filename)
}
tf <- clean_filename(basename(tf))
```

This runs the same process in batch. Memory was optimized to not load all the data at thesame time, but process and write each raster individually before going to the next one.  

This is way faster than the second approach (below). 

```{r iterate cropMask, eval=FALSE, include=FALSE}
clean_filename <- function(filename){
  #sub("_md5.*", "", filename)
  sub(".tif.*", "", filename)
}

# Loop through each geotiff file
for (i in seq_along(tiffes)) {
  # Get the current tiff file
  tiff_file <- tiffes[i]
  
  # Load the current geotiff (don't load all at once)
  access <- rast(tiff_file)
  
  # Apply crop and mask operations to each polygon
  cropped <- map(1:length(poly), function(x) crop(access, poly[[x]]))
  masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(tiff_file))
  #base_filename <- gsub("[[:punct:]]", "", base_filename) # Remove punctuation
#base_filename <- gsub(" ", "_", base_filename) # Replace spaces with underscores
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) {
    writeRaster(masked[[x]], paste0(outpath,"/", base_filename, "_", nam[x], ".tif"), overwrite = TRUE)
  })
  # Optionally, clear the variable to free up memory
  rm(access, cropped, masked)
  gc()  # Garbage collection to clear memory
}
```

# Region Wb

I should stop using this apprach. The code looks well, but it is very inefficient.
Check agains the other method. Also start thinking about covnerting all this into python.
```{r cropmask}
rs <-  rast(here("ESA_LC", "ESA_LC_1992_2020.tif")) 

num_cores <- 6
# Parallelize the outer loop using mclapply
rs_msk <- mclapply(poly, function(polygon) {
    # Crop and mask the raster
    r<- crop(rs, polygon)
    r <- mask(r, polygon)
    return(masked_raster)
  }, mc.cores = num_cores) 

# Use lapply to iterate over each list in nested_list
  map2(rs_msk, nam, function(r) {
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(here("ESA_LC", "wb_region"),"/",  "ESA_LC_1992_2020_", nam[i], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})
```


```{r}
summary_df <- map2_dfr(seq_along(rs_msk), rs_msk, function(i, inner_list) {
  
  # i corresponds to the index of the outer list (e.g., 1, 2, 3...)
  outer_label <- paste0("Outer_", i)  # Label for the outer list
  
  # Loop through each SpatRaster in the inner list
  map_dfr(inner_list, function(raster) {
    extract_raster_summary(raster, outer_label)
  })
})


for (i in seq_along(rs_msk)) {
  sublist <- rs_msk[[i]]  # Access each sublist
  
  for (j in seq_along(sublist)) {
    raster <- sublist[[j]]  # Access each SpatRaster
    raster_name <- names(raster)  # Get the name slot of the SpatRaster
    
    # Build the output filename
    output_filename <- paste0(outpath, '/', raster_name, "_", nam[j], ".tif")
    
    # Save the raster to a .tif file
    writeRaster(raster, filename = output_filename, overwrite = TRUE)
  }
}


```


# mask by protected areas (all togeter, chategories I-VI)


# Change Analysis

## Reclassify ESA maps. Lower number of classes. 
Move fro mthe original ESA LC maps to 9 Mroader Classes to facilitate anlysis. 
Classes: 



```{r change_map_calc}
esat <- c(here("Downloaded_data_ES", "ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif"),here("Downloaded_data_ES", "ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif"))

esas <- lapply(esat,rast)
nam <- c(2020,1992)

# Define the reclassification rules in one operation
reclass_table <- data.frame(
  from = c(10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100,
           110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180,
           190, 200, 201, 202, 210, 220),
  to = c(1, 1, 1, 1, 1, 1,   # Cultivated
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # Forests
         3, 3, 3, 3, 3,   # Grasses and Shrubs
         4, 4, 4, 4, 4,   # Sparse Vegetation
         5, 5, 5,   # Mangroves
         6,   # Urban
         7, 7, 7,   # Bare
         8,   # Water
         9)   # Ice
)

esas <- subst(esas, from = reclass_table$from, to = reclass_table$to)

writeRaster(esas, paste0(here("ESA_LC",'ESA_LC_1992_2020.tif')))



```


```{r cropmask LC_maps}
esas <- rast(here("ESA_LC",'ESA_LC_1992_2020.tif'))


poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
#poly <- st_transform(poly, crs = crs(tmp))
rm(tmp)
poly <- poly %>% filter(region_wb!="Antarctica")
nam <- sort(unique(poly$region_wb))
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
nam <- gsub("__", "_", nam) # Replace spaces with underscores
poly <- poly%>%split(.$region_wb)
# # Extract forests
# esas <- lapply(esas, function(r) {
#   subst(r, from = 2, to =1, others=NA)
# })
# 
# writeRaster(esas, paste0(here("LC", "ESA_FORESTS_1992-2020.tif")))


lapply(poly, function(polygon) {
    # Crop and mask the raster
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})








# Pending to add: run this in parallel!
# Apply cropping and masking recursively using lapply and map
outpath <- here("ESA_LC")
num_cores <- 10
# Parallelize the outer loop using mclapply
rs_msk <- mclapply(poly, function(p) {
    r <- crop(esas, p)
    r <- mask(r,p)
    return(r)
  },mc.cores = num_cores) 

map(1:length(rs_msk),function(x) writeRaster(rs_msk[[x]], paste0(here("ESA_LC", "wb_region"),'/', "LC_", nam[x], ".tif")))
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j){
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})

```


## Divide the global maps in manageable datasets. 
World Bank Regions seems like a manageable approach  
```{Cropmask landcovers}

tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))
#Remove points, risk reduction baseline
#iffes <- tiffes[-c(2,4)]
tiffes <- tiffes[4]

tf <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5_*", "", filename)
  sub(".tif.*", "", filename)
}
tf <- clean_filename(basename(tf))
```
##Produce change maps, and run similarity Analysis.

Produce and refernece bi-temporal land cover maps followed by similarity analysis/systematic change assessment. 


```{r chabge map and change analyis}
tiffes <- 


# Change MapsProduce





chg <- ch_mapR(esas[[1]],esas[[2]])
freq(chg)

mat_1 <- crosstabm(esas[[1]],esas[[2]])

diffs_p <-  diffTablej(mat, digits=3, analysis = "change")


```
git remote set-url origin git@github.com:springinnovate/nbs_op.git
# Get Change Maps 

```{r calculate 
change maps}

inpath <- '~/Documents/MN_Applied_Economics/Global_ES_mapping/ESA_LC'
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))

es <- lapply(tiffes, rast)
ch <- ch_mapR(es)
```
# Get Sankey Diagrams

```{r Sankey Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```


ok we are almost there , but we need to refine. right now on the axis, all the bands are included in each box plot, event if there is only one appearing on the plot

I am trying to synchronize my onedrive folder from linux.however, in my workplace they are quite annoying and you need to log in again all the time, and I am getting this error: The refresh token has expired or is invalid due to sign-in frequency checks by conditional access. and "ERROR: You will need to issue a --reauth and re-authorise this client to obtain a fresh auth token.". show me how to do it
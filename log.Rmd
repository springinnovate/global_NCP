---
title: "Clean Mapping Data"
output: html_notebook
---



Here, write down the output of Yesterday meeting and what i need to do. 

1. Organize output Dataframes (make sure that they are complete and correclty named)

Here, write down the output of Yesterday meeting and what i need to do. 

1. Organize output Data frames (make sure that they are complete and correctly named)


2. Make sure all the Vector files have the necessary attributes.

a. Subregion (IPBES) *done*
b. Countries top/less change *running to complete*
c. Biomes 
d. Income level *done*
e. regioin_wb *done*
f. Continent *done*

Clean the data 


# LOAD TTHE DATA


```{r load input polygon data}

#Builds a list of files in the vector directory
inpath <- here("vector")
sets <- file.path(inpath, list.files(inpath, pattern = "hydrosheds.*\\.gpkg$"))

# Set and select an index to load the desired dastaset
t <- 1
set <- sets[t]

lyr <- st_layers(set)
# # load polygons
print(lyr)
```


```{r refine columns}
basin <- st_read(set, layer= "hydrosheds_lev6") 
basin <- basin[c(1:13)]
poly <- st_read(set, layer= "layer1") 
bas1 <- st_drop_geometry(poly)
bas1 <- bas1[c(1,14,15,17)]
poly <- st_read(set, layer= "ch_92_2020") 
bas2 <- st_drop_geometry(poly)
bas2 <- bas2[c(1,14:15,23,22,17,18,19,20,24)]
poly <- st_read(set)

#poly <- st_read(set, layer= "ch_92_2020") 
#poly
basin <- left_join(basin, bas1)
basin <- left_join(basin,bas2)
basin <- basin[c(1:13,17:20,14,15,21,22,23,25,16,24)]
st_write(basin, here('vector', "hydrosheds_lv6_tmp.gpkg"))
```

```{r load input polygon data cumaribo}

#Builds a list of files in the vector directory
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/vector'
sets <- file.path(inpath, list.files(inpath, pattern = "hydrosheds.*\\.gpkg$"))

# Set and select an index to load the desired dastaset
t <- 2
set <- sets[t]

lyr <- st_layers(set)
# # load polygons
print(lyr)
```

```{r refine columns cumaribo}
basin2 <- st_read(set)#, layer= "hydrosheds_lev6") 
t2 <- st_drop_geometry(basin)
t2 <- t2[c(1,14:21,24,25,22,23,26)]
basin <- st_read(set)#, layer= "hydrosheds_lev6") 
t3 <- st_drop_geometry(basin)
t3 <- t3[c(1,14:17,20,21,22:25)]
t3 <- t3[-10]

basin <- st_read(set, layer= "ch_92_2020") 
t4 <- st_drop_geometry(basin)
t4 <- t4[-c(2:16)]
t4 <- t4[-c(2,3,4,5,6,9)]

t2 <- t2 %>% mutate(HYBAS_ID = as.character(HYBAS_ID))
t3 <- t3 %>% mutate(HYBAS_ID = as.character(HYBAS_ID))
t2 <- left_join(t2,t3)
t4 <- t4 %>% mutate(HYBAS_ID = as.character(HYBAS_ID))
t2 <- left_join(t2,t4)
t2 <- t2[c(1:9,15,16,17,18,19,20,12,13,21,22,14,23)]
#poly <- st_read(set, layer= "ch_92_2020") 
#poly
basin <- left_join(basinf, t2)
basin <- left_join(basin,bb2)

names(basin) <- ifelse(
  endsWith(names(basin), "_2020_1992"),
  gsub("_2020_1992$", "", names(basin)),
  names(basin))
st_write(basin, paste0('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/vector', '/', "hydrosheds_lv6_synthesis.gpkg"), append=FALSE)

```

## Add broader infor spatial units 
Here, we will add the names of the broadrr sptial units:

country
WWF biome
Ecoregion

```{r join spatial unit names}

inpath <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/vector"
inpath <- here('vector')
basins <- st_read(paste0(inpath, "/", "hydrosheds_lv6_cp.gpkg"))

basins <- basins[c(1:13)]
basins <- st_make_valid(basins)

st_write(basins, here('vector', 'hydrosheds_lv_6_tmp.gpkg'), append=FALSE)




cnt <- st_read(here('vector', 'cartographic_ee_ee_r264_correspondence.gpkg'))
cnt <- cnt[c(1,12,22)]

# Reproject both layers
cnt <- st_transform(cnt, crs = "EPSG:6933")
bas_eq <- st_transform(basins, crs = "EPSG:6933")

template <- terra::rast(bas_eq, resolution = 300)

# Rasterize the country polygons
r_country <- terra::rasterize(vect(cnt), template, field = "id")

# Step 2: Extract coverage fraction for each basin
country_weights <- exactextractr::exact_extract(r_country, basins, weights = "area")

# Step 3: For each basin, find the country with max coverage
dominant_country <- lapply(country_weights, function(tbl) {
  tbl <- tbl %>% group_by(value) %>% summarise(weight = sum(weight, na.rm = TRUE))
  tbl <- tbl %>% slice_max(weight, n = 1)
  return(tbl$value)
})

# Step 4: Map back to country name
dominant_country <- unlist(dominant_country)
basins$id <- dominant_country


basins <- left_join(basins, cnt %>% st_drop_geometry(), by = "id")
st_write(basins, here("vector", "basins_ct.gpkg"))
#######################################

basins <- st_read(paste0(inpath, "/", "hydrosheds_lv_tmp.gpkg"), layer = 'clipped')
bas_eq <- st_transform(basins, crs = "EPSG:6933")

template <- terra::rast(bas_eq, resolution = 300)
wwf_biome <- st_read(here("vector", "Biomes_WWF", "wwf_terr_ecos.shp"))
wwf_biome <-  st_transform(wwf_biome, crs = "EPSG:6933")


wwf_biome <- wwf_biome[c(1,4,6)]
r_bm <- terra::rasterize(vect(wwf_biome), template, field = "BIOME")
bm_weights <- exactextractr::exact_extract(r_bm, basins, weights = "area")

#  For each basin, find the unit with max coverage
dominant_bm <- map_dbl(bm_weights, function(tbl) {
  tbl %>%
    group_by(value) %>%
    summarise(weight = sum(weight, na.rm = TRUE), .groups = "drop") %>%
    slice_max(order_by = weight, n = 1, with_ties = FALSE) %>%
    pull(value)
})

basins <- st_read(here("vector", "basins_ct.gpkg"))
#dominant_bm.<- unlist(dominant_bm)
basins$BIOME <- dominant_bm
# Step 4: Map back to country name
lyrng <- st_drop_geometry(basins)

basins <- st_read()
basins <- left_join(basins,lyr %>% st_drop_geometry(), by ="BIOME")

basins <- basins %>% mutate(HYBAS_ID = as.character(HYBAS_ID))
lyr <- lyr %>% mutate(HYBAS_ID = as.character(HYBAS_ID))

basins <- left_join(basins, lyr  %>% st_drop_geometry(), by = "HYBAS_ID")
st_write(basins, here("vector", "basins_bm.gpkg"), append=FALSE)

basins <- left_join(basins, wwf_biome %>% st_drop_geometry(), by = "id")
st_write(basins, here("vector", "basins_ct.gpkg"))

```



```{r get amounts of lc chnage}
```



```{r biomes short}
biome_labels <- c(
  "Temperate Grasslands, Savannas & Shrublands" = "Temperate Grasslands",
  "Tropical & Subtropical Moist Broadleaf Forests" = "Moist Broadleaf Forests",
  "Temperate Conifer Forests" = "Temperate Conifers",
  "Tropical & Subtropical Coniferous Forests" = "Tropical Conifers",
  "Rock & Ice" = "Rock & Ice",
  "Boreal Forests/Taiga" = "Boreal Forests",
  "Montane Grasslands & Shrublands" = "Montane Grasslands",
  "Lakes" = "Lakes",
  "Flooded Grasslands & Savannas" = "Flooded Grasslands",
  "Mediterranean Forests, Woodlands & Scrub" = "Mediterranean Forests",
  "Tropical & Subtropical Dry Broadleaf Forests" = "Dry Broadleaf Forests",
  "Temperate Broadleaf & Mixed Forests" = "Temperate Broadleaf",
  "Tundra" = "Tundra",
  "Tropical & Subtropical Grasslands, Savannas & Shrublands" = "Tropical Grasslands",
  "Mangroves" = "Mangroves"
)
```

```{r get amounts of lc change}


# get pct as standalone data to add to the table
poly <- st_transform(poly, crs = 5880)
poly$area_ha <- as.numeric(st_area(poly))/10000

plt <- st_drop_geometry(poly)


plt <- as_tibble(plt)
#filter all thins smaller than 150.000 ha (takes away the 54 smallest features) 
plt  <- plt %>% select(1,2, area_ha,contains("pct"))%>% filter(area_ha>400000)


service <- c("Coastal_Protection", "Nitrogen_Export", "Sediment_Export", "Usle", "Nature_Access", "Pollination", "Pot_Sed_ret")
color <- c("#9e9ac8", "#2c944c", "#08306b", "#17c0ff", "#A57C00", "#dd1c77", "#8C510A")
cd <- as_tibble(cbind(service,color))

################## THIS IS VERY IMPPORTAnt. Instead of struggling  with the multiple dataframes, it is easier to load the vector file swith all the column and pivot longer as necessary. Easier to manage, adjust on the fly!
plt <- as_tibble(plt %>%
  pivot_longer(
    cols = c(ends_with("pct_ch")),  # Select all columns ending with "pct_ch"
    names_to = "service",        # New column to store the service names
    values_to = "pct_ch"         # New column to store the percentage change values
  ))

plt <- plt %>%
  mutate(service = str_remove(service, "_pct_ch$"))
plt <- left_join(plt,cd)
plt <- plt%>% filter(!is.na(pct_ch))

# Only for biomes, t make names shorter
# plt <- plt %>%
#   mutate(biome_short = recode(WWF_biome, !!!biome_labels))
 
```

```{r ggplot 2020, echo=FALSE, fig.height=8, fig.width=14}
#

plot_ecosystem_services <- function(data, var, col) {
  col_sym <- sym(col)  # Convert column name to symbol for dplyr
  
  # Step 1: Prepare Data (Remove NA values and zero mean values, then reorder names)
  data_prepped <- data %>%
    filter(!is.na(mean) & mean > 0 & year == !!year) %>%  # Exclude cases where mean is 0 and filter year
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))  

  # Step 2: Compute min/max values for selected column
  service_range <- data_prepped %>%
    group_by(service) %>%
    summarize(
      min_val = min(pct_chg, na.rm = TRUE),
      max_val = max(pct_chg, na.rm = TRUE),
      .groups = "drop"
    )

  # Step 3: Create "invisible" data for min/max range
  range_data <- service_range %>%
    pivot_longer(cols = c(min_val, max_val), names_to = "range_type", values_to = "mean") %>%
    mutate(year = !!year, temp_col = "dummy")

  # Step 4: Extract top 10 and bottom 10 per selected column
  top_10 <- data_prepped %>%
    group_by(service) %>%
    slice_max(order_by = mean, n = 10, with_ties = TRUE) 

  bottom_10 <- data_prepped %>%
    group_by(service) %>%
    slice_min(order_by = mean, n = 10, with_ties = TRUE)

  # Step 5: Combine only top 10 and bottom 10
  filtered_data <- bind_rows(top_10, bottom_10) %>%
    arrange(service, desc(mean))

  # Step 6: Reorder selected column to appear correctly
  filtered_data <- filtered_data %>%
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))

  # Step 7: Plot the filtered data
  p <- ggplot(filtered_data, aes(x = temp_col, y = mean, fill = color)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    scale_fill_identity() +
    facet_wrap(~ service, scales = "free") +
    scale_x_reordered() +  
    labs(
      title = paste("Mean Ecosystem Service Values,", year),
      x = col,
      y = "Mean Value"
    ) +
    theme_bw() +
    theme(
      strip.text = element_text(face = "bold", size = 12),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 5)
    )
  
  return(p)
}

# Generate plots for different years using a specified column
p_1992 <- plot_ecosystem_services(zonal_df, 1992, col)
p_2020 <- plot_ecosystem_services(zonal_df, 2020, col)

# Display the plots
print(p_1992)
print(p_2020)


```

## 5. Here, add plots with the differences 

```{r plot chg 8, fig.height=8, fig.width=14}
library(dplyr)
library(ggplot2)
library(tidytext)
library(rlang)

#For biomes
#col <- "biome_short"

plot_es_changes <- function(data, label_col = col, 
                            filter_type = "top_bottom", filter_val = 10) {
  
  label_sym <- sym(label_col)
  
  # Step 1: Filter problematic service and NAs
  filtered_data <- data %>%
    filter(service != "Nature_Access", !is.na(pct_ch))
  
  # Step 2: Apply filtering based on the chosen type
  if (filter_type == "top_bottom") {
    # Top/bottom n observations per service
    top_bottom <- filtered_data %>%
      group_by(service) %>%
      slice_max(pct_ch, n = filter_val, with_ties = FALSE) %>%
      bind_rows(
        filtered_data %>%
          group_by(service) %>%
          slice_min(pct_ch, n = filter_val, with_ties = FALSE)
      ) %>%
      ungroup()
    
  } else if (filter_type == "quantile") {
    # Top/bottom quantile per service (e.g., top/bottom 10%)
    top_bottom <- filtered_data %>%
      group_by(service) %>%
      filter(pct_ch >= quantile(pct_ch, 1 - filter_val, na.rm = TRUE) |
             pct_ch <= quantile(pct_ch, filter_val, na.rm = TRUE)) %>%
      ungroup()
    
  } else if (filter_type == "all") {
    top_bottom <- filtered_data
  } else {
    stop("Invalid `filter_type`. Use 'top_bottom', 'quantile', or 'all'.")
  }
  
  # Step 3: Reorder labels per service
  top_bottom <- top_bottom %>%
    mutate(temp_label = reorder_within(!!label_sym, -pct_ch, service))
  
  # Step 4: Plot
  ggplot(top_bottom, aes(x = temp_label, y = pct_ch, fill = color)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    scale_fill_identity() +
    scale_x_reordered() +
    facet_wrap(~ service, scales = "free", ncol = 3) +
    labs(
      title = paste("% Change 1992â€“2020 By", cols, sep= " "),
      x = NULL,
      y = "% Change"
    ) +
    theme_bw() +
    theme(
      strip.text = element_text(face = "bold", size = 10),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 8)
    )
}

# Top/bottom 10 countries per service
#plot_es_changes(plt, label_col = "name_long", filter_type = "top_bottom", filter_val = 10)

# Top/bottom 5% per service
#plot_es_changes(plt, label_col = "iso3", filter_type = "quantile", filter_val = 0.05)

# Show all values
plot_es_changes(plt, filter_type = "all")


```

```{r timeseries plot}


inpath <- here('output_data')
zonal_csv <- file.path(inpath, list.files(paste0(inpath),pattern= 'TS'))

n <- 4
zonal_df <- as_tibble(read.csv(zonal_csv[n]))


# Only for biomes, t make names shorter
 # zonal_df <- zonal_df %>%
 #   mutate(biome_short = recode(WWF_biome, !!!biome_labels))

library(ggplot2)

#zonal_df <- zonal_df %>% filter(WWF_biome!="Lakes")

# Create the line plot
ggplot(zonal_df, aes(x = year, y = mean, color = biome_short, group = biome_short)) +
  geom_line(size = 1) +          # Add lines for each subregion
  geom_point(size = 2) +         # Add points for visibility
  labs(
    title = paste("Ecosystem Service Trends by ", col, sep= ""),
    x = "Year",
    y = "Mean Value"
  ) +
  theme_minimal() +
  theme(legend.position = "right")  # Adjust legend position
```

```{r faceted plot stdev, fig.height=18, fig.width=18}

zonal_df <- zonal_df %>% filter(Sub_Region!="Antarctica")

ggplot(zonal_df, aes(x = year, y = stdev, group = Sub_Region)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 2, color = "red") +
  facet_wrap(~ Sub_Region, scales = "free_y", ncol = 3) +
  labs(
    title = paste("NDR - st_dev", col, sep= ", "),
    x = "Year",
    y = "Mean Value"
  ) +
  theme(
    legend.position = "right",
    text = element_text(size = 14),  # Reduce overall text size
    axis.text.x = element_text(size = 10, angle = 45, hjust = 1),  # Adjust x-axis labels
    axis.text.y = element_text(size = 10),  # Adjust y-axis labels
    legend.text = element_text(size = 8),  # Reduce legend font size
    legend.title = element_text(size = 0.1),  # Adjust legend title size
    strip.text = element_text(size = 12)  # Adjust facet labels (if using facets)
  )


```

```{r chartTS country filter}

# Filter to exclude NA values and "Lakes"

# Filter top 5 and bottom 5 per service
top_bottom_df <- zonal_df %>%
  group_by(service, year) %>%
  slice_max(mean, n = 5, with_ties = FALSE) %>%
  bind_rows(
    zonal_df %>%
      group_by(service, year) %>%
      slice_min(mean, n = 5, with_ties = FALSE)
  ) %>%
  ungroup()

ggplot(top_bottom_df, aes(x = year, y = mean, color = name_long, group = name_long)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = paste("Ecosystem Service Trends by ", col, sep = ""),
    x = "Year",
    y = "Mean Value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 8)
  )
```


```{r faceted plot count, fig.height=18, fig.width=18}


# 1. Filter valid records
filtered <- zonal_df %>%
  filter(!is.na(mean), !is.na(name_long))

# 2. Compute average change per country (across years) per service
ranked <- filtered %>%
  group_by(service, name_long) %>%
  summarize(avg_mean = mean(mean, na.rm = TRUE), .groups = "drop")

# 3. Extract top and bottom 5 countries per service
top_bottom <- ranked %>%
  group_by(service) %>%
  slice_max(avg_mean, n = 5, with_ties = FALSE) %>%
  bind_rows(
    ranked %>% 
      group_by(service) %>% 
      slice_min(avg_mean, n = 5, with_ties = FALSE)
  ) %>%
  ungroup()

# 4. Filter original data to only those countries and prepare labels
plot_data <- filtered %>%
  filter(name_long %in% top_bottom$name_long) %>%
  mutate(label = reorder_within(name_long, -mean, service))  # reorder per facet

# 5. Plot
ggplot(plot_data, aes(x = year, y = mean, group = name_long, color = name_long)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  facet_wrap(~ service, scales = "free_y") +
  scale_color_viridis_d(option = "C", begin = 0.2, end = 0.9) +
  scale_x_continuous(breaks = unique(zonal_df$year)) +
  scale_y_continuous() +
  labs(
    title = "Top and Bottom 5 Country Values per Ecosystem Service",
    x = "Year",
    y = "Mean Value"
  ) +
  scale_color_discrete(guide = guide_legend(title = "Country")) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
    axis.text.y = element_text(size = 8),
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 8)
  )

```

```{r ttt}
library(dplyr)
library(ggplot2)

# Step 1: Filter valid rows
filtered <- zonal_df %>%
  filter(!is.na(mean), !is.na(name_long))

# Step 2: Calculate average mean per country across all years, by service
ranked <- filtered %>%
  group_by(service, name_long) %>%
  summarize(avg_mean = mean(mean, na.rm = TRUE), .groups = "drop")

# Step 3: Select top 5 and bottom 5 countries per service
top_bottom <- ranked %>%
  group_by(service) %>%
  slice_max(avg_mean, n = 10, with_ties = FALSE) %>%
  bind_rows(
    ranked %>%
      group_by(service) %>%
      slice_min(avg_mean, n = 0, with_ties = FALSE)
  ) %>%
  ungroup()

# Step 4: Filter zonal_df to keep only those countries
zonal_top_bottom <- filtered %>%
  filter(name_long %in% top_bottom$name_long)

# Optional: Add a shortened country label if needed
zonal_top_bottom <- zonal_top_bottom %>%
  mutate(country_short = name_long)  # you can shorten names if needed

# Step 5: Plot (same structure as your original one, just updated to use countries)
ggplot(zonal_top_bottom, aes(x = year, y = mean, group = name_long)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 2, color = "red") +
  facet_wrap(~ name_long, scales = "free_y", ncol = 4) +
  labs(
    title = paste("NDR - Means,", "Top 10 Countries", sep= ", "),
    x = "Year",
    y = "Mean Value"
  ) +
  theme(
    legend.position = "right",
    text = element_text(size = 14),
    axis.text.x = element_text(size = 8, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 8),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 8),
    strip.text = element_text(size = 10)
  )

```

```{r plot chg2, fig.height=8, fig.width=14}

# 1. Filter out "Nature_Access"
filtered_plt <- plt %>%
  filter(service != "Nature_Access" & !is.na(pct_ch))

# 2. Get top 10 and bottom 10 by service
top_bottom <- filtered_plt %>%
  group_by(service) %>%
  slice_max(pct_ch, n = 10, with_ties = FALSE) %>%
  bind_rows(
    filtered_plt %>%
      group_by(service) %>%
      slice_min(pct_ch, n = 10, with_ties = FALSE)
  ) %>%
  ungroup()

# 3. Reorder country names *within each service*
top_bottom <- top_bottom %>%
  mutate(name_long = reorder_within(name_long, -pct_ch, service))  # descending order


ggplot(top_bottom, aes(x = name_long, y = pct_ch, fill = color)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_fill_identity() +
  scale_x_reordered() +  # <<-- THIS enables correct facet ordering
  facet_wrap(~ service, scales = "free", ncol = 3) +
  labs(
    title = "% Change 1992-2020 By Income Group
    x = NULL,
    y = "% Change"
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 7)
  )
```




write.csv(plt, here('output_data', paste0('pct_chg_',cols, '.csv')))




Percentage of Change. This is the key. I almot have it for everything! 


Variables: 

Pot. Sediment Retention for Each Grouping (Initial/ Final)

######################################################################

Time Series!!!

% percentage of change in the charts.... amount.






I extract change stats (stdev + mean) for 

- Subregions (repeat for stdev)
- Biome (running right now, lost connection to the internet, but just adusting to add the new columns to the vector file)
- Country. Here to ee those with the most change (%) It's the same, its a ratio, so we don't need to put too much effort normalizing or stuff
- Income
 
 
Get all the differentials and include % change in service and in export (so retention and export)....make sure i know which inputs am i using.

Paper: Review question to make sure that i

I extract change stats (stdev + mean) for 

 
Get all the differentials and include % change in service and in export (so retention and export)....make sure i know which inputs am i using.




Pending:

Incorporate the Fertilier EASA database to improve Fertilizer (N) mode.

M<ake sure i m using the right dstasert and olumn! Wrtite tomorrow to justin!!!

Make sure i m using the right dstasert and olumn! Wrtite tomorrow to justin!!!


Connecting LC analysis -> is it possible to identify s relationship betwe them?

How do the amoung of change between nwture tono nature relatw with the amount of change in the provision. This is zero banal


Get emtrics for Retention, export and the calcualtion (in short repeat the whole thing. again)

Get metrics for Retention, export and the calcualtion (in short repeat the whole thing. again)


Re run the export part

```{r create list of columns}

poly <- st_read('/Users/rodriguez/Global_ES_TS/global_NCP/vector/vector_f/Continent.gpkg')

serv <- names(poly)
serv <- serv[-c(1,30)]
serv


#Get filenames (add paths to stored data)





```

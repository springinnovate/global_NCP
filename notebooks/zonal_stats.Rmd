---
title: "Summary ES"
author: "Jeronimo Rodriguez-Escobar"
date: "2025-02-12"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

  
1.	Perform zonal statistics (using exactextractr).
3.	Create faceted bar plots (using ggplot2) for each raster statistic.
4.	Generate choropleth maps (using ggplot2 and sf) to visualize spatial distributions.

Compute mean and median for each polygon. Evaluate other metrics

At a country level, this was done using the *fids* for the polygons. Using country names is problematic because of geopolitics.
Pending to do: Think how to deal with the dependencies and complex cases (GB, France, others), but i will take care of this at a later point.

```{r setup, include=FALSE}
# Load required libraries
library(sf)             # For reading and working with vector data
library(dplyr)          # For data manipulation
library(terra)          # For reading raster data (SpatRasters)
library(exactextractr)  # For zonal statistics
library(ggplot2)        # For plotting
library(forcats)        # For factor reordering
library(tidytext)
library(here)
library(patchwork)
library(tidyr)
library(parallel)
```

## 1. Load Polygons

```{r load polygons, eval=FALSE, include=FALSE}

# Replace with the actual path to your polygon file (e.g., a shapefile)

<<<<<<< HEAD
# By country
set <- 'global_poly_means.gpkg'
# By World Bank Region
set <- 'wb_poly_means.gpkg'
# By Income gropup !!! This one has some isse, and is not running properly for some reason. 
set <- 'income_grp_f.gpkg'
# By Continent
set <- 'continent_poly.gpkg' #change to poly_means or just poly everythingh else 
# By Subregion
set <- 'subregion_poly_.gpkg'

#poly <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/vector/global_poly_means.gpkg')
# Countries
poly <- st_read(here('vector', set)) 
=======
poly <- st_read(here('vector', 'global_poly_means.gpkg')) 
#poly <- st_read(here('vector','poly_wb.geojson'))
#poly <- st_read(here('vector','income_group.gpkg'))
#poly <- st_read(here('vector','income_group.gpkg'))
poly <- st_read(here('vector','income_g.gpkg'))
>>>>>>> 6ee0eb4 (check some error in the order. whyyyy)
```


## 2. Load a List of GeoTIFF Rasters

Following products were assessed:
```{r load raster es, echo=FALSE}
# Vector of raster file paths
inpath <- here('input_ES')
#inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/input_ES' # use this path when working locally. 
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)
# Use terra::rast() to load each file as a SpatRaster object
rasters_list <- lapply(tiffes, rast)
# add the names of the services. This is slightly annoying but necessary. We need to make sure that the order is the same of the input data, which depends on the names of the input tiff files and that can change. Check that BEFORE running.
service <- rep(c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination"), each=2)
filename<- as_tibble(cbind(service,filename))
print(filename)
```

# 3. Calcualte Differences (check with original) 

 This has been already done, no need to do again.
```{r get differences, eval=FALSE, include=FALSE}
outpath <- here("input_ES")
m_index <- length(rasters_list) %/% 2
lab <- unique(service)
l1 <- rasters_list[seq(1,length(rasters_list), by=2)]
l2 <- rasters_list[seq(2,length(rasters_list), by=2)]
 # test with future_map2 next time to get this to run faster. 

diffs <- map2(l1,l2, function(l1,l2){
  return(l1-l2)
})
map(1:length(diffs), function(x) writeRaster(diffs[[x]], paste0(outpath, "/", 'change_calc', '/', lab[x], '_diff.tif')))
```

## 3. Compute Zonal Statistics with exactextractr for each year

```{r compute stats 1, eval=FALSE, include=FALSE}
#load the data (this assumes 'tiffs' is the list of paths to the files, and that they are in the desired order. Make sure tha the dc dataframe with the labels matches) There is room for improvement here. 

# Compute zonal stats. Mean for now 
target_stats <- c("mean")
# this is the column in the vector file that we are going to use. 
cols <- 'income_grp'
<<<<<<< HEAD
col <- "income_grp" # this is for the join. Not great but gets the work done
=======

>>>>>>> 6ee0eb4 (check some error in the order. whyyyy)
# Number of CPU cores to use

num.cores <- length(rasters_list) # set as many cores as rasters to be evaluated.  sequentially, and there is no progress bar available 
# here, set a way to change the input poly and columns externally instead of editing inside of the functions. That is cumbersome and prone to errors. 

res1 <- exact_extract(rasters_list[[1]], poly, fun = target_stats,append_cols = cols)
res2 <- exact_extract(rasters_list[[2]], poly, fun = target_stats,append_cols = cols)
res3 <- exact_extract(rasters_list[[3]], poly, fun = target_stats,append_cols = cols)
res4 <- exact_extract(rasters_list[[4]], poly, fun = target_stats,append_cols = cols)
res5 <- exact_extract(rasters_list[[5]], poly, fun = target_stats,append_cols = cols)
res6 <- exact_extract(rasters_list[[6]], poly, fun = target_stats,append_cols = cols)
res7 <- exact_extract(rasters_list[[7]], poly, fun = target_stats,append_cols = cols)
res8 <- exact_extract(rasters_list[[8]], poly, fun = target_stats,append_cols = cols)
res9 <- exact_extract(rasters_list[[9]], poly, fun = target_stats,append_cols = cols)
res10 <- exact_extract(rasters_list[[10]], poly, fun = target_stats,append_cols = cols)

results_list <- lapply(rasters_list, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)
  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)})#,mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df <- do.call(rbind, results_list)

# create a table with the content to fill the table
raster_name <- unique(zonal_df$raster_name)
service <- rep(c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination"), each = 2)
color <- rep(c("#9e9ac8", "#2c944c", "#08306b", "#A57C00", "#dd1c77"), each=2)
cd <- as_tibble(cbind(raster_name, service, color))


# add color ands service columns
zonal_df <- left_join(zonal_df,cd)
# Add the year. this uses the original filenames to extract the year, but miht be too hardcoded for generalization 

zonal_df$year <- ifelse(grepl("1992", zonal_df$raster_name), 
                          1992, 
                          ifelse(grepl("2020", zonal_df$raster_name), 2020, NA))


zonal_wide <- pivot_wider(zonal_df, id_cols=all_of(col), names_from=c(service, year), values_from = mean)
# adjust column names 

poly <- left_join(poly, zonal_wide, by= col)
st_write(poly, here('vector', set), append = FALSE) 
write.csv(zonal_df, here('output_data', paste0('zonal_df_',cols, '.csv')))

```


## This mightr not be necessary anymore either. 

```{r create maps, eval=FALSE, include=FALSE}

# Very important. This line adds the new attribnutes to the polygons. I could add all here.

col <- "region_wb"

#zonal_df <-zonal_df %>% select(c(region_wb,mean, service)) # original (most vector files except biomes)
zonal_df <-zonal_df %>% select(c(BIOME,WWF_biome, mean, service, year)) # biomes
zonal_df <- zonal_df %>% rename(diff_mean = mean)

# this is to pivot when there are two dates (each year)
zonal_wide <- pivot_wider(zonal_df, id_cols=!!sym(col), names_from=c(service,year), values_from = c(mean, median))



# this is for when there i only one value (eg differences)
zonal_wide <- pivot_wider(zonal_df, id_cols=!!sym(col), names_from= service, values_from = diff_mean)


#rename the columns to avoid confusions
zonal_wide <- zonal_wide %>%
  rename_with(~ paste0("diff_", .), -1) 

poly <- left_join(poly, zonal_wide, by = col)

st_write(poly,  here('vector', 'wb_poly_means.gpkg'), append=FALSE) 

```

## 4. Now do the differences  

In theory, i could get everything done together but it somehow gets too complicated. The names of the input rasters don't follow the same sequence, so i need to reorganize the whole thing. Also in one case there are two years, in the differences, thats absent. There are just too many things to cosnider that it gets overly complicated. Maybe someone will be able to take from here.

I will have to review this part. Ideally, i should get everything done together but the year issue made it tricky. Easier to do each on its owrn, the problem is that there is still too much manual handling. Eventually this should be generalized.

```{r compute stats diff, eval=FALSE, include=FALSE}

# load the polygons with the new attributes to add new 
#inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/input_ES'
inpath <- here('input_ES')
inpath <- paste0(inpath, "/", 'change_calc')

# load the rasters with the calculated differences 
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)

# Use terra::rast() to load each file as a SpatRaster object
rast_list <- lapply(tiffes, rast)
service <- c("Coastal Protection", "Nature Access","Nitrogen Export","Pollination","Sediment Export")
filename<- as_tibble(cbind(service,filename))
#colnames(prod) <- c("Service", "Filename")
print(filename)
# We'll compute mean and median as an example
target_stats <- "mean"
cols <- 'region_wb'
# exact_extract can append columns from the polygon layer;
# Number of CPU cores to use
num.cores <- 5 #detectCores() - 2  # Leave 4 cores free. Adjust depending on available resources. 

# res1 <- exact_extract(rast_list[[1]], poly, fun = target_stats,append_cols = cols)
# res2 <- exact_extract(rast_list[[2]], poly, fun = target_stats,append_cols = cols)
# res3 <- exact_extract(rast_list[[3]], poly, fun = target_stats,append_cols = cols)
# res4 <- exact_extract(rast_list[[4]], poly, fun = target_stats,append_cols = cols)
# res5 <- exact_extract(rast_list[[5]], poly, fun = target_stats,append_cols = cols)
# for some reason "income level"is givin all kinds of issues becasue fuck me, of course. 
results_list <- mclapply(rast_list, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = cols)
  r_name <- names(r)

  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)},mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df <- do.call(rbind, results_list)

raster_name <- unique(zonal_df$raster_name)
service <- rep(c("Coastal Protection", "Nature Access","Nitrogen Export", "Pollination","Sediment Export"), each = 1)
color <- c("#9e9ac8", "#A57C00", "#2c944c",  "#dd1c77", "#08306b")
cd <- as_tibble(cbind(raster_name, service, color))

# Very important. This line adds the new attribnutes to the polygons. I could add all here.
zonal_df <- left_join(zonal_df,cd)
#zonal_df <- zonal_df[-c(4,6)]
zonal_wide <- pivot_wider(zonal_df, id_cols=all_of(cols), names_from=c(service), values_from = mean)
# adjust column names 
colnames(zonal_wide)[colnames(zonal_wide) != cols] <- paste0(colnames(zonal_wide)[colnames(zonal_wide) != cols], "_diff")

poly <- left_join(poly, zonal_wide, by= cols)
st_write(poly, here('vector', 'biome_poly_means.gpkg'), append = FALSE) 

# I don't thik this is needed anymore.         
#save(zonal_df, file=here('output_data', 'diffs_wb.RData'))
write.csv(zonal_df, here('output_data', "diffs_biomes_.csv"))
```


## 5 . Get the Barplots

```{r ggplot 2020, echo=FALSE, fig.height=8, fig.width=14}
#zonal_df <- as_tibble(read.csv(here('output_data', 'zonal_df_global.csv')))
#zonal_df <- zonal_df %>% select(id, mean, service, color, year, name_long)
zonal_df <- zonal_df %>% select(BIOME, WWF_biome, mean, service, color, year)
zonal_df <- zonal_df %>% select(id, mean, service, color, year, name_long)

plot_ecosystem_services <- function(data, year, col) {
  col_sym <- sym(col)  # Convert column name to symbol for dplyr
  
  # Step 1: Prepare Data (Remove NA values and zero mean values, then reorder names)
  data_prepped <- data %>%
    filter(!is.na(mean) & mean > 0 & year == !!year) %>%  # Exclude cases where mean is 0 and filter year
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))  

  # Step 2: Compute min/max values for selected column
  service_range <- data_prepped %>%
    group_by(service) %>%
    summarize(
      min_val = min(mean, na.rm = TRUE),
      max_val = max(mean, na.rm = TRUE),
      .groups = "drop"
    )

  # Step 3: Create "invisible" data for min/max range
  range_data <- service_range %>%
    pivot_longer(cols = c(min_val, max_val), names_to = "range_type", values_to = "mean") %>%
    mutate(year = !!year, temp_col = "dummy")

  # Step 4: Extract top 10 and bottom 10 per selected column
  top_10 <- data_prepped %>%
    group_by(service) %>%
    slice_max(order_by = mean, n = 10, with_ties = TRUE) 

  bottom_10 <- data_prepped %>%
    group_by(service) %>%
    slice_min(order_by = mean, n = 10, with_ties = TRUE)

  # Step 5: Combine only top 10 and bottom 10
  filtered_data <- bind_rows(top_10, bottom_10) %>%
    arrange(service, desc(mean))

  # Step 6: Reorder selected column to appear correctly
  filtered_data <- filtered_data %>%
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))

  # Step 7: Plot the filtered data
  p <- ggplot(filtered_data, aes(x = temp_col, y = mean, fill = color)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    scale_fill_identity() +
    facet_wrap(~ service, scales = "free") +
    scale_x_reordered() +  
    labs(
      title = paste("Mean Ecosystem Service Values,", year),
      x = col,
      y = "Mean Value"
    ) +
    theme_bw() +
    theme(
      strip.text = element_text(face = "bold", size = 12),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 5)
    )
  
  return(p)
}

# Generate plots for different years using a specified column
p_1992 <- plot_ecosystem_services(zonal_df, 1992, col)
p_2020 <- plot_ecosystem_services(zonal_df, 2020, col)

# Display the plots
print(p_1992)
print(p_2020)


```

## 5. Here, add plots with the differences 

```{r plot diffs, fig.height=8, fig.width=14}

zonal_df <- as_tibble(read.csv(here('output_data', 'diffs_biomes_.csv')))

plot_ecosystem_services1 <- function(data, col) {
  col_sym <- sym(col)  # Convert column name to symbol for dplyr
  
  # Step 1: Prepare Data (Remove NA values and zero mean values, then reorder names)
  data_prepped <- data %>%
    filter(!is.na(mean)) %>%  # Exclude cases where mean is 0
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))  

  # Step 2: Compute min/max values for selected column
  service_range <- data_prepped %>%
    group_by(service) %>%
    summarize(
      min_val = min(mean, na.rm = TRUE),
      max_val = max(mean, na.rm = TRUE),
      .groups = "drop"
    )

  # Step 3: Create "invisible" data for min/max range
  range_data <- service_range %>%
    pivot_longer(cols = c(min_val, max_val), names_to = "range_type", values_to = "mean") %>%
    mutate(temp_col = "dummy")

  # Step 4: Extract top 10 and bottom 10 per selected column
  top_10 <- data_prepped %>%
    group_by(service) %>%
    slice_max(order_by = mean, n = 10, with_ties = TRUE) 

  bottom_10 <- data_prepped %>%
    group_by(service) %>%
    slice_min(order_by = mean, n = 10, with_ties = TRUE)

  # Step 5: Combine only top 10 and bottom 10
  filtered_data <- bind_rows(top_10, bottom_10) %>%
    arrange(service, desc(mean))

  # Step 6: Reorder selected column to appear correctly
  filtered_data <- filtered_data %>%
    mutate(temp_col = reorder_within(!!col_sym, -mean, service))

  # Step 7: Plot the filtered data
  p <- ggplot(filtered_data, aes(x = temp_col, y = mean, fill = color)) +
    geom_bar(stat = "identity", show.legend = FALSE) +
    scale_fill_identity() +
    facet_wrap(~ service, scales = "free") +
    scale_x_reordered() +  
    labs(
      title = paste("Difference Mean Ecosystem Service Values, 1992-2020"),
      x = col,
      y = "Mean Diff Value"
    ) +
    theme_bw() +
    theme(
      strip.text = element_text(face = "bold", size = 12),
      axis.text.x = element_text(angle = 45, hjust = 1, size = 5)
    )
  
  return(p)
}

# Generate plots for different years using a specified column
p_diff <- plot_ecosystem_services1(zonal_df, col)


# Display the plots
print(p_diff)

```

## 5. Generate  Maps

Use ggplot2 and the spatial geometry from our sf object to color each polygon by its statistic of interest. Faceting again by each raster is useful to compare spatial patterns.
```{r create maps, eval=FALSE, include=FALSE}

# Very important. This line adds the new attribnutes to the polygons. I could add all here.

col <- "region_wb"

zonal_df <-zonal_df %>% select(c(region_wb,mean, service)) 
zonal_df <- zonal_df %>% rename(diff_mean = mean)

# this is to pivot when there are two dates (each year)
zonal_wide <- pivot_wider(zonal_df, id_cols=!!sym(col), names_from=c(service,year), values_from = c(mean, median))



# this is for when there i only one value (eg differences)
zonal_wide <- pivot_wider(zonal_df, id_cols=!!sym(col), names_from= service, values_from = diff_mean)


#rename the columns to avoid confusions
zonal_wide <- zonal_wide %>%
  rename_with(~ paste0("diff_", .), -1) 

poly <- left_join(poly, zonal_wide, by = col)

st_write(poly,  here('vector', 'wb_poly_means.gpkg'), append=FALSE) 

```
####################################################

```{r mapping f}
# First, merge the zonal_df results back into the polygon sf object.
# This ensures each polygon gets a corresponding mean/median value.

# We need a unique identifier. 
# Assuming 'country' is unique per polygon, we can do a left_join:
map_data <- poly %>%
  left_join(zonal_df, by = "id")

map_1992 <- map_data %>% filter(year == 1992)
services <- unique(map_1992$service)
map_2020 <-  map_data %>% filter(year == 2020)


# 2) Identify unique services
services <- unique(map_data$service)

# 3) For each service, find min/max across both years
service_range <- map_data %>%
  group_by(service) %>%
  summarize(
    min_val = min(mean, na.rm = TRUE),
    max_val = max(mean, na.rm = TRUE),
    unique_color = unique(color)  # we assume 1 color per service
  )

plot_list_1992 <- list()
plot_list_2020 <- list()

for (s in services) {
  # Extract global range (across both years) for service s
  row_s <- filter(service_range, service == s)
  s_min <- row_s$min_val
  s_max <- row_s$max_val
  s_color <- row_s$unique_color  # The single "unique" color for that service
  
  # Subset to year=1992, that service
  df_1992 <- map_data %>%
    filter(service == s, year == 1992)
  
  # Subset to year=2020, that service
  df_2020 <- map_data %>%
    filter(service == s, year == 2020)
  
  # Build the 1992 map
  p_1992 <- ggplot(df_1992) +
    geom_sf(aes(fill = mean), color = NA) +
    scale_fill_gradientn(
      colors   = c("white", s_color),  # White -> "unique_color"
      limits   = c(s_min, s_max),      # Force the same scale min/max
      na.value = "gray40"
    ) +
    labs(
      title = paste0(s, " (1992)"),
      fill  = "Mean"
    ) +
    theme_minimal()
  
  # Build the 2020 map
  p_2020 <- ggplot(df_2020) +
    geom_sf(aes(fill = mean), color = NA) +
    scale_fill_gradientn(
      colors   = c("white", s_color),
      limits   = c(s_min, s_max),
      na.value = "gray40"
    ) +
    labs(
      title = paste0(s, " (2020)"),
      fill  = "Mean"
    ) +
    theme_minimal()
  
  # Add them to the lists
  plot_list_1992[[s]] <- p_1992
  plot_list_2020[[s]] <- p_2020
}
```

```{r eval=FALSE, fig.height=12, fig.width=14, include=FALSE}

# Combine the 1992 plots in a row (or multi-row if you prefer).
combined_1992 <- wrap_plots(plot_list_1992, ncol = 3)

# Combine the 2020 plots similarly.
combined_2020 <- wrap_plots(plot_list_2020, ncol = 3)

combined_1992
combined_2020


'#9e9ac8'
'#2c944c'
```

Next steps:  

Run for: Biomes, Continents, income group, wb_region, subregion might be more interesting than continent?



Land cover change -> assess change/area In other words what do i **exactly** need to do here?  What is my target variable? LC change? 

Will have to do it using DiffeR and iterate over all countries, then assemble the dataframes and get percentage of change distributed among the main components. This is not going to be easy but i can get it. However, keep in mind that with all the original classes this becomes unmanageable. Can we produce successive no change maps and check what happens among the change (alloation, swaps and shifts)

Take care of the beneficiaries. 









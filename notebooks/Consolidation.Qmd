---
title: "Current Status"
output: html_notebook
---

```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
library(tidyr)
library(forcats)
library(scales)
library(RColorBrewer)
library(htmltools)
library(leaflet)
library(devtools)
library(reticulate)
load_all()
#source the helper functions
# set venv for Python
use_virtualenv("~/venvs/coastal_snap_env", required = TRUE)
```

# 1. Clean and Organize Synthesis Data

Make sure that they are complete and correclty named)

Right now, 24/04/2025 the problem is on the way the dataframe with the class colors and names is built. The process is still too manual (writing the table) and is easy to get it wrong when it should not be big deal. Ideally, there should be a dialogue box to fill this instead. This should take care of the class names, values and colors neatl Also, the order of the factors needs to be set, and new classes have to be added. Again, i don't how to do it easily. I mean, it is easy, but annoying to do and easy to get wrong.

```{r consolidate final outputs}

#Builds a list of files in the vector directory
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data'
sets <- file.path(inpath, list.files(inpath, pattern = "gpkg$"))
# Set and select an index to load the desired dastaset

sets1 <- sets[c(4,5)] # 10 km grid, zonals stats for ES services 1992, 2020), sums and means
sets2 <- sets[c(2,3)] # 10 km grid, zonal stats for beneficiaries  , sums and means (depends on the variable)

# Step 1. Load vectors with the summary stats objects
sf_a <- lapply(sets1, st_read)

# Step 2: Add fid to each (all have same row count & order)
sf_a <- lapply(sf_a, function(x) dplyr::mutate(x, fid = dplyr::row_number()))

# Update: include the coastal protection data:
sf_cp <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/summary_pipeline_workspace/grid_10k_synth_zonal_2025_07_20_23_45_27.gpkg')
# Define name replacements per object. Had to do it manually, there is just too much variability to automatize, did it by hand, but this is the kind of thing an LLM helps to write easily. Still, there must be a smarter, more flexible way to do it (maybe the best approach is to keep a consistent naming convention for the modeled rasters, this has been a PIA since i started this work)

# Update 2. Include corrected Access Data
rename_list <- list(
  c(
    usle_1992_mean = "global_usle_marine_mod_ESA_1992_mean",
    usle_2020_mean = "global_usle_marine_mod_ESA_2020_mean",
    nature_access_1992_mean = "nature_access_lspop2019_ESA1992_mean",
    nature_access_2020_mean = "nature_access_lspop2019_ESA2020_mean",
    n_ret_ratio_1992_mean = "N_ret_ratio_1992_mean",
    n_ret_ratio_2020_mean = "N_ret_ratio_2020_mean",
    sed_ret_ratio_1992_mean = "Sed_ret_ratio_1992_mean",
    sed_ret_ratio_2020_mean = "Sed_ret_ratio_2020_mean"
  ),
  c(
    n_export_1992_sum = "global_n_export_tnc_esa1992_sum",
    n_export_2020_sum = "global_n_export_tnc_esa2020_sum",
    n_retention_1992_sum = "global_n_retention_ESAmar_1992_fertilizer_sum",
    n_retention_2020_sum = "global_n_retention_ESAmar_2020_fertilizer_sum",
    sed_export_1992_sum = "global_sed_export_marine_mod_ESA_1992_sum",
    sed_export_2020_sum = "global_sed_export_marine_mod_ESA_2020_sum",
    pollination_1992_sum = "realized_polllination_on_ag_ESA1992_sum",
    pollination_2020_sum = "realized_polllination_on_ag_ESA2020_sum"
  )
)
sf_a <- Map(function(x, renames) {
  dplyr::rename(x, !!!renames)
}, sf_a, rename_list)

# Calculate bitemporal change in % and absolute terms. Drop original columns
sf_a <- lapply(sf_a, function(x) compute_change(x, suffix = c("_sum", "_mean"), change_type= "both", drop_columns = TRUE))

# Join into a single sf object
sf_f <- sf_a[[1]]

# Step 2: Loop over the rest and join by 'fid'
for (i in 2:length(sf_a)) {
  sf_f <- dplyr::left_join(sf_f, sf_a[[i]] %>% sf::st_drop_geometry(), by = "fid")
}
rm(sf_a)
 # NaN are divisions by 0, NA is when values are NA

# load beneficiaries data. There is no change going on here right now, but maybe in the future. The workflow can handle this
sf_ben <- lapply(sets2,st_read)
# Add fid (primary key)
sf_ben <- lapply(sf_ben, function(x) dplyr::mutate(x, fid = dplyr::row_number()))

sf_benf <- sf_ben[[1]]

# Step 2: Loop over the rest and join by 'fid'
for (i in 2:length(sf_ben)) {
  sf_benf <- dplyr::left_join(sf_benf, sf_ben[[i]] %>% sf::st_drop_geometry(), by = "fid")
}
rm(sf_ben) # remove not needed anymore
# join beneficiary data to the ES change data
sf_f <- dplyr::left_join(sf_f, sf_benf %>% sf::st_drop_geometry(), by = "fid")
rm(sf_benf) 

# export output

st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```


### 1.1.1 Include new columns (C.Protection)


Not RUN. This is is an earlier approach that Becky told me ( i don't remember the exact reason) was not great). Rasterizing to the ESA resoluton and the using exactextractr. That's why i did not use that at the end. 

```{r add c protection}
#| eval: false
#| include: false
# Update: include the coastal protection data:
sf_cp <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/summary_pipeline_workspace/grid_10k_synth_zonal_2025_07_20_23_45_27.gpkg')

# Define name replacements per object. Had to do it manually, there is just too much variability to automatize, did it by hand, but this is the kind of thing an LLM helps to write easily. Still, there must be a smarter, more flexible way to do it (maybe the best approach is to keep a consistent naming convention for the modeled rasters, this has been a PIA since i started this work)
sf_cp <- sf_cp %>% mutate(fid = dplyr::row_number())
# Calculate bitemporal change in % and absolute terms. Drop original columns
sf_cp <- compute_variable_change(sf_cp, suffix = c("_mean"), change_type= "both", drop_columns = TRUE)

sf_cp <- st_drop_geometry(sf_cp)
# load previous version
sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'))
sf_f <-  sf_f %>% mutate(fid = dplyr::row_number()) # not sure why fid is missing. Nevermind, added here 
sf_f <- left_join(sf_f, sf_cp)
# reorganize columns:

sf_f <- sf_f %>%
  relocate(
    coastal_protection_Rt_abs_chg,
    coastal_protection_Rt_pct_chg,
    .before = GHS_BUILT_S_E2020_mean
  )
```


### 1.1.2 Include Coastal Protection (QGIS) 
This approach was not great, because I did the calculation on QGIS 
Here are the instructions:

Option 2: QGIS (no coding)

Use the “Join attributes by location (summary)” tool:

    Input layer: your cp2

    Join layer: your points

    Geometric predicate: intersects (or within, if points are entirely inside)

    Choose the column to summarize

    Statistics to calculate: check Mean and Max

    Run and save the result



Quite easy, but doing this leaves no record, abd ui just wasted two days trying to remember where the hell was I, so it may be better to run it here so we have ALL the steps on. single location

## 1.2 Include Coastal Protection (final)
```{r include coastal points}
# Load grid (nothing special, just a template)
 cp2 <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg')
 cp2$fid <- as.integer(cp2$fid)
# st_write(cp2, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg', append=FALSE)

# load cleaned coastal protection data:
cp_points <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/Spring/Inspring/coastal_risk_tnc_esa1992_2020_ch.gpkg')

# Ensure same CRS (optional) Add a conditional here that checks before trying to reproject!
cp_points <- st_transform(cp_points, st_crs(cp2))

# Join and summarize
###### I WILL HAVE TO RUN THIS IN PYTHON BECAUSE IT IS TOO SLOW IN R. MAYBE I COULD CHUNCK HERE, BUT NO POINT OF LEANRING NEW TOOLS ON R WHNE PYTHON DOES IT FINE AND I CAN KEEP TRANSITIONING THERE 
cp2_joined <- st_join(cp2, cp_points) %>%
  group_by(fid) %>% # use the fid or any other unique identifier fro mthe polygon dataset. I should set here if i want the max or minimum.
  summarise(
    Rt_1992_mean = mean(Rt_1992, na.rm = TRUE),
    Rt_1992_max = max(Rt_1992, na.rm = TRUE),
    Rt_nohab_1992_mean = mean(Rt_nohab_all_1992, na.rm = TRUE),
    Rt_nohab_1992_max = max(Rt_nohab_all_1992, na.rm = TRUE),
    Rt_service_1992_mean = mean(Rt_service_1992, na.rm = TRUE),
    Rt_service_1992_max = max(Rt_service_1992, na.rm = TRUE),
    Rt_2020_mean = mean(Rt_2020, na.rm = TRUE),
    Rt_2020_max = max(Rt_2020, na.rm = TRUE),
    Rt_nohab_2020_mean = mean(Rt_nohab_all_2020, na.rm = TRUE),
    Rt_nohab_2020_max = max(Rt_nohab_all_2020, na.rm = TRUE),
    Rt_service_2020_mean = mean(Rt_service_2020, na.rm = TRUE),
    Rt_service_2020_max = max(Rt_service_2020, na.rm = TRUE),
    geometry = first(geometry)
  )

head(cp2_joined)
cp2_joined$fid <- as.integer(cp2_joined$fid)
sapply(cp2_joined, class)

# to separate it from the raw empty template, added this with the calculated data.
st_write(cp2_joined,'/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_calc.gpkg')
```

## 1.3 Join Coastal protectio nand export again

```{r add to the existing synthesis data} 
#to preserve all original polygon columns: (not doing it right now but can be redone)
sf_f <- st_read(paste0(inpath,'/', '10k_grid_ES_change_benef.gpkg'))

# some cleaning of older data. Not necesary anymore 
# sf_f$Rt_mean_pct_chg <- NULL
# sf_f$Rt_max_pct_chg <- NULL
# sf_f$coastal_protection_Rt_abs_chg <- NULL
# sf_f$coastal_protection_Rt_pct_chg <- NULL

# Calculate change# CalcNULLulate change

cp2 <- compute_change(cp2_joined, suffix = c("_mean", "_max"), drop_columns = TRUE, change_type = "both")
st_write(cp2,'/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_chg.gpkg', append=FALSE)

cp2 <- st_drop_geometry(cp2)
# export output
cp2 <- cp2 %>%
  select(-contains("_max"))

sf_f <- sf_f %>% mutate(fid = row_number())

sf_f <- left_join(sf_f, cp2, by = 'fid')
sf_f <- sf_f %>%
  relocate(
    Rt_mean_abs_chg,
    Rt_mean_pct_chg,
    Rt_nohab_mean_abs_chg,
    Rt_nohab_mean_pct_chg,
    Rt_service_mean_abs_chg,
    Rt_service_mean_pct_chg,
    .before = GHS_BUILT_S_E2020_mean
  )
sf_f$fid <- NULL

st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```


```{python summarize points}
#| eval: false
#| include: false
import geopandas as gpd
import pandas as pd
from shapely import speedups
from pathlib import Path
from tqdm import tqdm

if speedups.available:
    speedups.enable()

# Paths
polygons_fp = "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal.gpkg"
points_fp = "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/Spring/Inspring/coastal_risk_tnc_esa1992_2020_ch.gpkg"
output_fp = "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/grid_10k_coastal_calc_py.gpkg"

# Columns to summarize
cols_to_summarize = [
    "Rt_1992",
    "Rt_nohab_all_1992",
    "Rt_service_1992",
    "Rt_2020",
    "Rt_nohab_all_2020",
    "Rt_service_2020"
]


# Load data
grid = gpd.read_file(polygons_fp).reset_index(drop=True)
grid["fid"] = grid.index.astype(int)
points = gpd.read_file(points_fp)

# Ensure CRS match
if grid.crs != points.crs:
    points = points.to_crs(grid.crs)

# Build spatial index
points_sindex = points.sindex

results = []

# Process polygons in chunks
for _, poly in tqdm(grid.iterrows(), total=len(grid)):
    possible_matches_idx = list(points_sindex.intersection(poly.geometry.bounds))
    possible_matches = points.iloc[possible_matches_idx]
    precise_matches = possible_matches[possible_matches.intersects(poly.geometry)]

    row = {"fid": int(poly["fid"])}
    if precise_matches.empty:
        for col in cols_to_summarize:
            row[f"{col}_mean"] = None
            row[f"{col}_max"] = None
    else:
        for col in cols_to_summarize:
            if col in precise_matches.columns:
                row[f"{col}_mean"] = precise_matches[col].mean(skipna=True)
                row[f"{col}_max"] = precise_matches[col].max(skipna=True)
            else:
                row[f"{col}_mean"] = None
                row[f"{col}_max"] = None

    results.append(row)

summary_df = pd.DataFrame(results)

# Ensure fid column exists
if "fid" not in summary_df.columns:
    raise ValueError("No fid column found in summary_df. Check that results were appended correctly.")

# Merge back with grid
summary_gdf = grid.merge(summary_df, on="fid", how="left", validate="1:1")

# Export
summary_gdf.to_file(output_fp, driver="GPKG")
print(f"✓ Exported summarized file to {output_fp}")
```

## 1.4 Add Updated Access Data

No need to run again, already updated the Access Data. 
```{r add access}

sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg')) # why am I loading this? What is the purpose. This is always the final (most curtrent version). To this one is that ineed to do the adjustments, add new columns


# This is the old output with the calculated means for services (because we want to add the new columns to this output, even if it was created sli
sf_1 <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data/10k_grid_synth_serv_means.gpkg')

sf_1 <- sf_1 %>% mutate(fid = dplyr::row_number())


poly1 <- poly1 %>% rename(nature_access_lspop2019_ESA2020_mean = nature_access_lspop2019_ESA2020._mean)
sf_1 <- sf_1 %>% rename(nature_access_lspop2019_ESA2020_mean = nature_access_lspop2019_ESA2020._mean)
poly1 <- poly1 %>% mutate(fid = dplyr::row_number())

poly1 <- st_drop_geometry(poly1)


sf_1$nature_access_lspop2019_ESA2020_mean <- NULL
sf_1$nature_access_lspop2019_ESA2020_mean <- poly1$nature_access_lspop2019_ESA2020_mean

sf_1 <- sf_1 %>% select(fid, global_usle_marine_mod_ESA_1992_mean, global_usle_marine_mod_ESA_2020_mean,nature_access_lspop2019_ESA1992_mean,nature_access_lspop2019_ESA2020_mean, N_ret_ratio_1992_mean,N_ret_ratio_2020_mean, Sed_ret_ratio_1992_mean, Sed_ret_ratio_2020_mean)

st_write(sf_1, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/output_data/10k_grid_synth_serv_means.gpkg', append=FALSE)


names(sf_f)

names(poly1)

poly1 <- poly1 %>% rename(nature_access_abs_chg=access_abs_chg)
poly1 %>% rename(nature_access_pct_chg = access_pct_chg)

names(sf_1)

names(sf_f)

sf_f$nature_access_abs_chg=poly1$nature_access_abs_chg
sf_f$nature_access_pct_chg=poly1$access_pct_chg
st_write(sf_f, paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'), append=FALSE)
```


# 2 Load Vector Data and pivot



Here, the spatial objects with the additional attributes are loaded and reformatted for analysis and chart preparation - **pivot & tidy**. Add to the documentation , environment or however that's called the vectors to select and order the columns. This should actually be done as a database structure. The columns live somewhere and are summoned upon need from a set of options (list_dir).

```{r pivot, eval=FALSE, include=FALSE}
# 
#  get pct as standalone data to add to the table
# Most recent version of synthesis data:
sf_f <- st_read(paste0(inpath, '/', '10k_grid_ES_change_benef.gpkg'))
sf_f$fid <- seq_len(nrow(sf_f)) 
plt <- st_drop_geometry(sf_f)
 

# get the benef vars (all that are not "chg)
socio_vars <- names(plt)[
  names(plt) != "fid" & 
  !grepl("chg$", names(plt))
]

# Pivot all columns that contain pct_ch, keeping other relevant vars

plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, service, pct_chg, all_of(socio_vars)) %>% 
   mutate(service = str_replace(service, "_pct_chg.*", ""))
  
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_chg)) %>% filter(pct_chg != Inf)  

# remove old coastal protection
#plt_long <- plt_long%>% filter(service != "coastal_protection_Rt") #why?? I will leave it here as one of the things one should not do. If i adjsuted the data, dp it for good, doin;'t leave it like this and then i haven o idea what is going on.


# rename some variables (just for aesthetics)

  plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "n_export" ~ "N_export",
   service == "n_retention" ~ "N_retention",
   service == "nature_access" ~ "Nature_Access",
   service == "pollination" ~ "Pollination",
   service == "usle" ~ "USLE",
   service == "n_ret_ratio" ~ "N_Ret_Ratio",
   service == "sed_ret_ratio" ~ "Sed_Ret_Ratio",
   TRUE ~ service
   ))
```

# 3. Get Hotspots

Need to think on an effective way to change the value (the share/quantile we want to extract and also filter by group!!!!)


So, we need a long table format, then define the threshold the services with gains/losses and then (optional) set the filters by groups. Taht wouls be awesome

Add the combos

```{r get_hotspots}
 #set combos

combo1 <- c("Rt_service_mean", "Rt_nohab_mean","Pollination","Nature_Access")
combo2 <- c("N_Ret_Ratio","N_export","N_retention")
combo3 <- c("Sed_Ret_Ratio","Sed_export", "USLE")
# ------------------------------------------
# Step 0: Define Parameters
# ------------------------------------------
loss_services <- c("Rt_service_mean", "Pollination", "N_Ret_Ratio",
                   "Sed_Ret_Ratio", "Nature_Access", "N_retention")
gain_services <- c("Sed_export", "N_export", "USLE", "Rt_nohab_mean") # decide what to do wit "Rt_mean"
pct_cutoff <- 0.05  # e.g. top/bottom 5%

# ------------------------------------------
# Step 1: Identify Top/Bottom Hotspots per Service
# ------------------------------------------
df_hotspots <- plt_long %>%
  group_by(service) %>%
  mutate(
    n_total = n(),
    n_cut = ceiling(n_total * pct_cutoff),
    rank_high = rank(-pct_chg, ties.method = "first"),
    rank_low = rank(pct_chg, ties.method = "first"),
    hotspot_flag = case_when(
      rank_high <= n_cut ~ "high",
      rank_low <= n_cut ~ "low",
      TRUE ~ NA_character_
    ),
    hotspot_binary = !is.na(hotspot_flag)
  ) %>%
  ungroup()

# ----------------------------------------------------
# Step 2: Keep only NEGATIVE hotspot services
# ----------------------------------------------------
df_hotspots_negative <- df_hotspots %>%
  filter(
    hotspot_binary,
    (service %in% loss_services & hotspot_flag == "low") |
    (service %in% gain_services & hotspot_flag == "high")
  )

# ----------------------------------------------------
# Step 3: Summarize by polygon
# ----------------------------------------------------

hotspot_summary <- df_hotspots_negative %>%
  group_by(fid) %>%
  summarise(
    hotspot_count = n(),
    hotspot_services = list(unique(service))
  ) %>%
  mutate(
    hotspot_services = sapply(hotspot_services, \(x) paste(trimws(as.character(x)), collapse = ", ")),
    hotspot_services_list = strsplit(hotspot_services, ",\\s*"),
    
    # Compute combo counts with final column names
    count_prov_access = sapply(hotspot_services_list, \(x) sum(x %in% combo1)),
    count_N = sapply(hotspot_services_list, \(x) sum(x %in% combo2)),
    count_Sed = sapply(hotspot_services_list, \(x) sum(x %in% combo3)),
    
    hotspot_types = sapply(hotspot_services_list, function(svcs) {
      types <- sapply(svcs, function(s) {
        if (s %in% loss_services) "loss"
        else if (s %in% gain_services) "damage"
        else NA_character_
      })
      paste(na.omit(types), collapse = ", ")
    }),
    hotspot_services_negative = hotspot_services
  ) %>%
  select(
    fid,
    hotspot_count,
    count_prov_access,
    count_N,
    count_Sed,
    hotspot_services,
    hotspot_types,
    hotspot_services_negative
  )
# ----------------------------------------------------
# Step 4: Pivot to wide binary matrix
# ----------------------------------------------------
hotspot_binary_matrix <- hotspot_summary %>%
  select(fid, hotspot_services) %>%
  separate_rows(hotspot_services, sep = ",\\s*") %>%
  mutate(is_hotspot = 1L) %>%
  pivot_wider(names_from = hotspot_services, values_from = is_hotspot, values_fill = 0)

# ----------------------------------------------------
# Step 5: Join with sf object
# Only cp2 that are hotspots for negative services are retained
# ----------------------------------------------------
hotspots_final <- sf_f %>%
  inner_join(hotspot_binary_matrix, by = "fid") %>%
  left_join(hotspot_summary, by = "fid")
# ------------------------------------------
# Step 6: Export to file
# ------------------------------------------
st_write(hotspots_final, file.path(inpath, "10k_grid_hotspots.gpkg"), append = FALSE)

```

# 4. Join Country Data (improve Analysis)

**I still have not been able to make sense of this shit. And it is really annoyingm anbd a huge waste of time. I absolutely need to transcend this.**


```{r add country data, eval=FALSE, include=FALSE}
# load the country data (to get income level and stuff)
hotspots_change <- st_read(paste0(inpath, '/',"hydrosheds_lv_6_hotspots.gpkg"))


# oad contries correpsondence data
cart_corr <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector/cartographic_ee_ee_r264_correspondence.gpkg')
# drop geometry
cart_corr <- st_drop_geometry(cart_corr)
# Drop unnecessary columns
cart_corr <- cart_corr[c(1,21,25,27,30,31,35)]
hotspots_change <- left_join(hotspots_change, cart_corr)
#reorder columns to keep related togetert <- 
hotspots_change <- hotspots_change[c(1:16,38:43,17,18,19:37)]
st_write(hotspots_change, here('vector', "hydrosheds_lv_6_hotspots.gpkg"), append = FALSE)

```




# 5 Scatterplots.


```{r pivot back}

plt <- st_drop_geometry(hotspots_final)


plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_chg"),
    names_to = "service",
    values_to = "pct_chg"
  ) %>%
  select(fid, service, pct_chg, all_of(socio_vars)) %>% 
   mutate(service = str_replace(service, "_pct_chg.*", ""))
  
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_chg)) %>% filter(pct_chg != Inf)  




```

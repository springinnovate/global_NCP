---
title: "DaTa Preparation NBS"    
output: html_notebook
---

I will use this project to peprare and extract insights from the raster data for the polygons in the 5 prioritized areas.

# 1. Prepare Environemnt (Load necessary libraries)
```{r load libraries, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
```
## 1.1 set class names and colors

```{r set symbols, include=FALSE}
#create vector with the country names
nam <- c("BRAZIL","MADAGASCAR","MEXICO","PERU","VIETNAM") 
lb <- data.frame(
  Class <- c("Cultivated", "Forest", "Shrubs and Grasses", "Sparse",
            "Mangroves", "Urban", "Bare", "Water"),
  color=c("#dcf064", "#00b809", "#d29000", "#ffebaf", "#009678", "#c31400", 
            "#fff5d7", "#0046c8"),
  value=c(1:8))
names(lb) <- c("Class", "color", "value")
```


## 2 Crop the Data for the target areas
```{r load target area polygons}
poly <- st_read('/Volumes/Jero_HDD/POSDOC/NBS_OPS/vector/areas_nbs.geojson') 
#Split the vector into a list of individual polygons
#create vector with names
nam <- poly$COUNTR
nam <- sort(nam)
poly <- poly%>%split(.$COUNTR)
```
## 2.1 Test with one raster
This is to process one single raster and ths that require reprojection (access related )
```{r crop rast, eval=FALSE, include=FALSE}

#Prepare Land Covers 
esamod <- rast('/Volumes/Jero_HDD/WWF/global_outputs/ESAmodVCFv2_md5_05407ed305c24604eb5a38551cddb031.tif')
esamod <- map(1:length(poly), function(x) crop(esamod, poly[[x]]))
esamod <- map(1:length(esamod), function(x) mask(esamod[[x]], poly[[x]]))
map(1:length(esamod), function(x) writeRaster(esamod[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'esaLC_', nam[x], '.tif'), overwrite=TRUE))

#Coastal risk reduction This was addewd later. Check if there are some missing that i need to add.
c_risk_red<- rast('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESAmodVCFv2_cv_habitat_value_md5_c01e9b17aee323ead79573d66fa4020d.tif')

c_risk_red <- map(1:length(poly), function(x) crop(c_risk_red, poly[[x]]))
c_risk_red <- map(1:length(c_risk_red), function(x) mask(c_risk_red[[x]], poly[[x]]))
map(1:length(c_risk_red), function(x) writeRaster(c_risk_red[[x]], paste0('/Users/sputnik/Documents/Natural_capital/NBS_OP/data/raster/', 'coastal_risk_reduction', nam[x], '.tif'), overwrite=TRUE))
#remeber that the layer as such, keeps the name in a slot and that we are using that. 
```

## 2.2 Iterate over Restoration   all rasters 

Restoration Data is being processed here I designed this process because i thought it would be better to oad and process each raster individually to avoid memory issues, but now i think i am an idiot because the rasters are not totally loaded anyway. 

```{r iterate cropMask}
wd <- '/Volumes/Jero_HDD/POSDOC/Downloaded_data_ES'
setwd(wd)

#This is for the restoration data I
tiffes <- file.path(wd, list.files(wd, pattern= '2020-1992'))
n <- 3
t <- rast(tiffes[[n]])
clean_filename <- function(filename) {
  sub("_md5.*", "", filename)
}

output_path <- '/Volumes/Jero_HDD/POSDOC/NBS_OPS/cropped_raster_data/'

 cropped <- map(1:length(poly), function(x) crop(t, poly[[x]]))
 masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(paths[[n]]))
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) (writeRaster(masked[[x]], paste0(output_path, base_filename, "_", nam[x], ".tif"), overwrite = TRUE)))

# Loop through each geotiff file
for (i in seq_along(tiffes)) {
  # Get the current tiff file
  tiff_file <- paths[i]
  
  # Load the current geotiff (don't load all at once)
  access <- rast(tiff_file)
  
  # Apply crop and mask operations to each polygon
  cropped <- map(1:length(poly), function(x) crop(access, poly[[x]]))
  masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(tiff_file))
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) {
    writeRaster(masked[[x]], paste0(output_path, base_filename, "_", nam[x], ".tif"), overwrite = TRUE)
  })
  # Optionally, clear the variable to free up memory
  rm(access, cropped, masked)
  gc()  # Garbage collection to clear memory
}

```

## 2.3. Reproj and CropMask Accesibility Raster

```{r reproj access}

# Nature access: number of people within 1 hour travel
acc <- rast('/Volumes/Jero_HDD/WWF/global_outputs/nature_access_lspop2019_esa2020modVCFhab_md5_a6519ebd8b941444921e749da2e645bb.tif')
#Load template. Where did I get it from? 
acc2 <- rast('/Volumes/Jero_HDD/WWF/global_outputs/temp/access_WGS84.tif')
acc <- project(acc,acc2)
writeRaster(acc, '/Volumes/Jero_HDD/WWF/global_outputs/nature_access_lspop2019_esa2020modVCFhab_md5_a6519ebd8b941444921e749da2e645bb_WGS84.tif')


# Nature Accessibility Data Becsuse i have to reproejcxt. check if there are some left to do. 
# Hast to be reprojected, so that is a bit annoying but no big deal, alrady done. I think the issue is that one is still pending
acc<- rast('/Volumes/Jero_HDD/WWF/global_outputs/nature_access_lspop2019_esa2020modVCFhab_md5_a6519ebd8b941444921e749da2e645bb_WGS84.tif')
acc <- map(1:length(poly), function(x) crop(acc, poly[[x]]))
acc <- map(1:length(acc), function(x) mask(acc[[x]], poly[[x]]))
map(1:length(acc), function(x) writeRaster(acc[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'nature_access_ESA_2020', nam[x], '.tif'), overwrite=TRUE))

# Access restoration
acces_rest <- rast('/Volumes/Jero_HDD/WWF/global_outputs/nature_access_diff_Sc3v1_PNVnoag-esa2020.tif')
acces_rest <- project(acces_rest, acc2, method='bilinear')
acces_rest <- map(1:length(poly), function(x) crop(acces_rest, poly[[x]]))
acces_rest <- map(1:length(acces_rest), function(x) mask(acces_rest[[x]], poly[[x]]))
map(1:length(access_nature), function(x) writeRaster(acces_rest[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/extracted/', 'nature_access_diff_Sc3v1_PNVnoag-esa2020_', nam[x], '.tif'), overwrite=TRUE))

acces_rest <- rast('/Volumes/Jero_HDD/WWF/global_outputs/global_people_access_population_2019_60.0m_md5_d264d371bd0d0a750b002a673abbb383.tif')

map(1:length(access_rest), function(x) writeRaster(acces_rest[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'global_people_access_population_2019_60.0m_', nam[x], '.tif'), overwrite=TRUE))
```

##  2.4. Coastal Risk Reduction 
So far here i have loaded, reprojected when necessary and cropmasked the global ecosystem reasters for each one of the study areas. (NBS) and exported them back to map in Qgis. 

# 3 Explore rasters

For summarizing geotiff raster files and comparing values across protected areas or different land covers for an initial assessment, especially for an audience without geospatial expertise, here are some accessible metrics you can use:
1. Basic Statistical Summaries:

## 3.1 Summaries Land cover
```{r basic summaries}

path_lc <- '/Volumes/Jero_HDD/POSDOC/ESA_LC'
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "rec"))
lc <- lapply(tf,rast)

# Set the target CRS to Equal Earth (EPSG:8857)
equal_earth_crs <- "EPSG:8857"
lc <- lapply(lc, function(r){
  r <- project(r, equal_earth_crs, method='near')
})

lsmet <- lapply(lc, function(r){
  mets <- lsm_c_ca(r)
})
lsmet <- lapply(seq_along(lsmet), function(i) {
  lsmet[[i]]$AREA <- nam[i]
  return(lsmet[[i]])
})
lsmet <- do.call(rbind, lsmet)
lsmet <- left_join(lsmet, lb, by= join_by(class == value))
lsmet <- lsmet %>% select(!c(level,id,metric))
lsmet <-  lsmet %>% group_by(AREA, layer) %>%
  mutate(percentage = value / sum(value) * 100) %>%  # Calculate percentage
  ungroup() %>% mutate(year=case_when(
    layer == 1 ~ 1992,
    layer == 2 ~ 2020
  ))
# Create the bar plot using ggplot2
t <- ggplot(lsmet, aes(x = Class, y = percentage, fill = color)) +
  geom_bar(stat = "identity") +  # Use identity since we are plotting pre-calculated values
  scale_fill_identity() +  # Use the exact colors provided in the `color` column
  facet_grid(year ~ AREA) +  # Create a grid with rows for `AREA` and columns for `layer`
  labs(title = "Class Distribution by Area and Year",
       x = "Class",
       y = "Percentage") +
  #theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability

print(t)
```


This is the data that we have ready:

- Land Cover ESA
  - Area Proportion for both years
- Coastal Risk Reduction
- Restoration
  - Nitrogen Export
  - nature access restoration
  - pollination restoration
  - sediment export
- Conservation?



Beneficiaries and coastal protection pop< 10 downstreams



then, go through these change raster (priority 4 and take a look at them and think what can be done here, and show differences for land covers), inside outside protected areas (UICN cat National park Ia , Ib, II, III, IV , V, VI)

# Get summaries

Barplots/distributions outisde inside protected areas/land cover classes as a way to see if protected areas are abnormally represented.
## Get summaries per PA.

```{r get summaries}
#load protected areas
pa = st_read('/Volumes/Jero_HDD/POSDOC/NBS_OPS/vector/wdpa_nbs.gpkg')
#remove those without UICN cathegory
filt <- c("Not Reported", "Not Assigned", "Not Applicable")
pa <- pa %>% filter(!(IUCN_CAT %in% filt))
pa <- pa%>%split(.$COUNTR)

#Get baseline values for ecosystem services. Will use the 2020baseline values
# set inpath
wd <- '/Volumes/Jero_HDD/POSDOC/NBS_OPS/cropped_raster_data'
wd <- '/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/cropped_rasters'



tiffes <- file.path(wd, list.files(wd, pattern="tif"))
#tiffes <-  tiffes[c(36:40, 46:50, 66:70, 86:90)]
tiffes <-  tiffes[c(1:5, 11:15, 21:25, 41:45)]
tiffes <- tiffes[-c(1:5)] # removethe access becasue fucking distances
# lab <- sapply(tiffes, function(x) {
#   basename(tools::file_path_sans_ext(x))
# })


# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  FilePath = tiffes,
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)

# Step 4: Extract the sorted file paths
tiffes <- file_info$FilePath

baseES <- lapply(tiffes, rast)
# Reproject to equal_+earth_crs


baseES <- lapply(baseES, function(r){
  r <- project(r, equal_earth_crs, method='cubic')
})

lc <- lapply(lc, function(r){
  r <- project(r, equal_earth_crs, method = 'near')
})

#   # Extract only the file name without the extension
# tfs <- sapply(tiffes, function(x) {
#   basename(tools::file_path_sans_ext(x))
# })
# tfs <- as_tibble(tfs)


ap <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/pa_nbs.shp')
filt <- c("Not Reported", "Not Assigned", "Not Applicable")
ap <- ap %>% filter(!(IUCN_CAT %in% filt))
ap <- st_transform(ap, equal_earth_crs)
ap <- ap%>%split(.$COUNTR)

# Step 1: Split the rasters list into a list of lists, where each sublist contains 4 SpatRasters maldita sea. Otra vez yo peleando con una puta lista 
baseES <- split(baseES, rep(1:length(ap), each = 3))

lcm <- lapply(lc, function(r){
  r <- app(r, min)
})
lcm <- lapply(lcm,function(r){
  r <- subst(r,from=c(1:10), to =1)
  })

baseES <- map2(
  baseES,   # The list of raster sublists
  lcm,     # The list of country polygons
  function(rasters, bkg) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) merge(raster, bkg))
  }
)


# Step 2: Apply mask for each group of rasters using the corresponding country vector
baseES_ap <- map2(
  baseES,   # The list of raster sublists
  ap,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector))
  }
)

baseES_api <- map2(
  baseES,   # The list of raster sublists
  ap,     # The list of country polygons
  function(rasters, vector) {
    # Apply mask to each raster in the sublist using the corresponding country vector
    lapply(rasters, function(raster) mask(raster, vector, inverse=TRUE))
  }
)

# Step 3: Flatten the list if you want a single list of masked rasters
baseES_ap <- unlist(baseES_ap, recursive = FALSE)
baseES_api <- unlist(baseES_api, recursive = FALSE)
################################

msk_ap <- lapply(baseES_ap, function(r) {
  # Replace all non-NA values with 1 using ifel()
  ifel(!is.na(r), 1, NA)
})

msk_no_ap <- lapply(baseES_api, function(r) {
  # Replace all non-NA values with 1 using ifel()
  r <- ifel(!is.na(r), 1, NA)
})

# Set the target CRS to Equal Earth (EPSG:8857)
equal_earth_crs <- "EPSG:8857"
msk_ap <- lapply(msk_ap, function(r){
  r <- project(r, equal_earth_crs, method='near')
})
msk_api <- lapply(msk_api, function(r){
  r <- project(r, equal_earth_crs, method='near')
})


freq_ap <- lapply(msk_ap, function(r){
  f <- freq(r)
})
freq_ap <- do.call(rbind, freq_ap)

freq_no_ap <- lapply(msk_no_ap, function(r){
  f <- freq(r)
})

freq_no_ap <- do.call(rbind, freq_no_ap)

freq_ap <- freq_ap %>% select(3)
freq_no_ap <- freq_no_ap %>% select(3)
# Here, we just have the number of pixels for each service inside/outside the PA
##################################################################
##################################################################

#calculate ES output
#
#
totSE_ap <-  lapply(baseES_ap, compute_sum)#, mc.cores = detectCores() - 4)
totSE_api <-  lapply(baseES_api, compute_sum)#, mc.cores = detectCores() - 4)

totSE_ap <- as_tibble(unlist(totSE_ap))
totSE_api <- as_tibble(unlist(totSE_api))
totSE <- cbind(file_info[c(2,3)], totSE_api, totSE_ap, freq_no_ap, freq_ap)

names(totSE) <- c("Service", "Country", "WholeArea", "PArea", "pix_count_area", "pix_count_pa")#, "Share_Service", "Share_Area", "Country")
#Calcualte share protected area/total
totSE <- totSE %>% mutate(share_service= (PArea/WholeArea)*100)
totSE <- totSE %>% mutate(share_area= (pix_count_pa/pix_count_area)*100)

names(totSE) <- c("Service", "Country", "WholeArea", "PArea", "pix_count_area", "pix_count_pa", "Share_Service", "Share_Area")

write.csv(totSE, file=here('data', "totSE.csv"))

totSE_l <- totSE %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Service == "global_n_retention_esamod2_compressed" ~ "Nitrogen Retention",
    Service == "global_sed_retention_esamod2_compressed" ~ "Sediment Retention",
    Service == "pollination_ppl_fed_on_ag_10s_esa2020" ~ "Pollination",
    Service == "nature_access_ESA" ~ "Access",
    Service == "access_ESA_2020" ~ "Access",
    TRUE ~ Service  # Keep other names unchanged, if any
  )) %>%
  # Select relevant columns and reshape the data into long format
  select(Service, Share_Service, Share_Area, Country) %>%
  pivot_longer(cols = c(Share_Service, Share_Area),
               names_to = "Metric",
               values_to = "Value")

##########################################################################
##########################################################################

# Get the histograms
baseES_api_v <- lapply(baseES_api, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

baseES_ap_v <- lapply(baseES_ap, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

# waht the fuck does this freaking line do???/ It adds the id for the service
# There is a better way to do it, check the next part 
file_info[1] <- NULL
baseES_api_v <- lapply(seq_along(baseES_api_v), function(i) {
  cbind(baseES_api_v[[i]], file_info[i,])
})

baseES_api_v <- lapply(baseES_api_v, function(r){
  r <- as_tibble(r)
})

baseES_ap_v <- lapply(seq_along(baseES_ap_v), function(i) {
  cbind(baseES_ap_v[[i]], file_info[i,])
})

baseES_ap_v <- lapply(baseES_ap_v, function(r){
  r <- as_tibble(r)
})
new_col <- "Value"
# Use lapply to rename the first column of each tibble
baseES_api_v <- lapply(baseES_api_v, function(tbl) {
  tbl %>%
    rename(!!new_col := 1)  # Rename first column using column position
})

baseES_ap_v <- lapply(baseES_ap_v, function(tbl) {
  tbl %>%
    rename(!!new_col := 1)  # Rename first column using column position
})

##################### do not run inside the Rmd!!!!!!!!
baseES_api_v <- do.call(rbind, baseES_api_v)
baseES_ap_v <- do.call(rbind, baseES_ap_v)
##################### do not run inside the Rmd!!!!!!!!
###########################################################
###########################################################


baseES_v <- baseES_api_v%>% mutate(set = "outside")

baseESap_v <- baseES_ap_v%>% mutate(set = "inside")
baseES_vf <- rbind(baseES_v, baseESap_v)

baseES_vf <- baseES_vf %>% rename(Service = Product)


baseES_vf <- baseES_vf %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Service == "global_n_retention_esamod2_compressed" ~ "Nitrogen Retention",
    Service == "global_sed_retention_esamod2_compressed" ~ "Sediment Retention",
    Service == "pollination_ppl_fed_on_ag_10s_esa2020" ~ "Pollination",
    Service == "nature_access_ESA" ~ "Nature Access",
    Service == "nature_access_ESA_2020" ~ "Nature Access",
    Service == "access_ESA_2020" ~ "Nature Access",
    TRUE ~ Service  # Keep other names unchanged, if any
  )) 


write.csv(baseES_vf, here('data', "distributionServ.csv"))
# Calculate the 98th percentile for each combination of Service and Country
percentile_98 <- baseES_vf %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.98)) %>%
  ungroup()
percentile_95 <- baseES_vf %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.95)) %>%
  ungroup()


# Join the threshold back to the original data and filter values up to the 98th percentile
baseES_vf_filtered <- baseES_vf %>%
  left_join(percentile_98, by = c("Service", "Country")) %>%
  filter(Value <= threshold)

baseES_vf_filtered <- baseES_vf %>%
  left_join(percentile_95, by = c("Service", "Country")) %>%
  filter(Value <= threshold)



Count <- unique(country_names) 

C <- 5

df <- baseES_vf_filtered %>% filter(Country==Count[C]) %>% filter(Service!="Nature Access")


t <- ggplot(df, aes(x = Value, fill = set)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  #facet_wrap(~Service, scales = "free") +
  facet_wrap(~Service, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `set` variable
  labs(title = paste("Density Plot of Services Inside and Outside Protected Areas -", Count[C]),
       x = "Value",
       y = "Density",
       fill = "Set") +
  #scale_y_continuous(trans = 'log10')# Label for the fill legend
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
t
```
# Plot the shares

```{r ggplot, h=8, w=8}

# Step 2: Create the bar plot
p <- ggplot(totSE_l, aes(x = Metric, y = Value, fill = Service)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.7)) +  # Side-by-side bars
  facet_wrap(~ Country, ncol = 5) +  # Create 5 columns (one for each country)
  scale_fill_brewer(palette = "Set3") +  # Use a color palette for services
  labs(title = "Share of the protected areas and of the total ES provided inside them",
       x = "Metric",
       y = "%",
       fill = "Service") +
  #theme_minimal() +
   theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal",  # Arrange legend items horizontally
    legend.spacing.x = unit(0.1, 'cm'),  # Add horizontal spacing between legend items
    legend.title = element_text(size = 12, face = "bold"),  # Adjust legend title size
    legend.text = element_text(size = 10),  # Adjust legend text size
    legend.box.just = "center"  # Center the legend items
  )



```

# Restorastion Areas Griscom

```{r resto}
wd <- '/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/global_products/Restoration'



tiffes <- file.path(wd,list.files(wd, pattern = "\\.tif$", full.names = FALSE))
tiffes <- tiffes[-5]



rest <- lapply(tiffes, rast)
               
               # This function extracts the 1 and 0 for mthe categorical rasters from restoration and corrects the weird values. did this because i had no metadata. Add the function to a new package eventully
process_raster <- function(raster) {
  # Step 1: Extract unique values
  unique_values <- unique(values(raster, na.rm = TRUE))
  unique_values <- as.vector(unique_values)  # Convert to vector

  # Step 2: Get frequency counts of each unique value
  value_frequencies <- freq(raster)
  val_f <- as.numeric(value_frequencies[, 2])  # Get the frequencies

  # Step 3: Convert raster to numeric
  raster_numeric <- as.numeric(raster)

  # Step 4: Substitute the values with the frequencies
  raster_subst <- subst(raster_numeric, from = unique_values, to = val_f)

  # Return the reclassified raster
  return(raster_subst)
}

names <- basename(tiffes)

rest  <- lapply(rest, process_raster)
#reorganize the list by countries, not by products. 
map(1:length(rest), function(x) writeRaster(rest[[x]], here('data', 'griscom_areas', names[x])))

# Step 1: Extract country names from the "source" attribute of each SpatRaster object
get_country <- function(r) {
  # Extract the source file path
  source_name <- sources(r)[1]  # Extract the first source (in case there are multiple)

  # Use a regular expression to extract the country name from the file name
  # Assuming the file name format is "something_COUNTRY.tif"
  country <- sub(".*_([^_]+)\\.tif$", "\\1", source_name)
  return(country)
}

country_names <- unique(sapply(baseES, get_country))  # Get unique country names
country_list <- setNames(vector("list", length(country_names)), country_names)

# Step 3: Group SpatRasters by country and populate the nested list
for (i in seq_along(baseES)) {
  country <- get_country(baseES[[i]])
  country_list[[country]] <- c(country_list[[country]], list(baseES[[i]]))
}

baseES <- country_list
rm(country_list)

baseES_rp <- lapply(seq_along(baseES), function(i) {
  # Get the current sublist of SpatRasters from baseES
  current_sublist <- baseES[[i]]
  # Get the corresponding template SpatRaster for reprojection
  template_raster <- rest[[i]]
  # Use lapply to reproject each SpatRaster in the sublist using the template raster
  reprojected_sublist <- lapply(current_sublist, function(r) {
    # Reproject the SpatRaster to match the CRS and extent of the template raster
    project(r, template_raster, method = "bilinear")
  })
  # Return the reprojected sublist
  return(reprojected_sublist)
})


baseES <- baseES_rp
rm(baseES_rp)

baseES_0 <- lapply(seq_along(baseES), function(i) {
  # Get the current sublist of SpatRasters from baseES
  current_sublist <- baseES[[i]]
  # Get the corresponding template SpatRaster for reprojection
  template_raster <- rest[[i]]
  # Use lapply to reproject each SpatRaster in the sublist using the template raster
  masked_sublist <- lapply(current_sublist, function(r) {
    # Reproject the SpatRaster to match the CRS and extent of the template raster
    mask(r, template_raster, maskvalues=0)
  })
  # Return the reprojected sublist
  return(masked_sublist)
})
# ok so far so good, but of course resolution with freaking access is always an issue
baseES_1 <- lapply(seq_along(baseES), function(i) {
  # Get the current sublist of SpatRasters from baseES
  current_sublist <- baseES[[i]]
  # Get the corresponding template SpatRaster for reprojection
  template_raster <- rest[[i]]
  # Use lapply to reproject each SpatRaster in the sublist using the template raster
  masked_sublist <- lapply(current_sublist, function(r) {
    # Reproject the SpatRaster to match the CRS and extent of the template raster
    mask(r, template_raster, maskvalues=1)
  })
  # Return the reprojected sublist
  return(masked_sublist)
})

 # Step 1: Flatten the nested list into a single list of SpatRaster objects
flattened_list <- unlist(baseES_1, recursive = FALSE)

# Step 2: Extract information for the data frame

# Create a dataframe to store the information
raster_info <- data.frame(
  Country = character(),   # Empty column for country names
  Product = character(),   # Empty column for product names
  stringsAsFactors = FALSE # Prevent automatic conversion to factors
)

# Step 3: Generate the correct country names for each raster
# Calculate the number of rasters per country
rasters_per_country <- length(flattened_list) / length(country_names)

# Repeat each country name for the number of rasters per country
repeated_country_names <- rep(country_names, each = rasters_per_country)

# Step 4: Iterate over the flattened list and extract the relevant information
for (i in seq_along(flattened_list)) {
  # Get the corresponding country name from the repeated country names vector
  country_name <- repeated_country_names[i]
  
  # Get the product name from the `name` slot of the SpatRaster
  product_name <- names(flattened_list[[i]])
  
  # If the product name contains a "~" or "_md5", extract only the part before it
  if (grepl("_md5", product_name)) {
    product_name <- sub("_md5.*$", "", product_name)
  }

  # Step 5: Add the extracted information to the dataframe
  raster_info <- rbind(raster_info, data.frame(Country = country_name, Product = product_name))
}

# Step 5: Optional - If you need to store the raster info back into a named list with country and product
# Create a named list where each raster is named by country and product

# baseES_0 <- setNames(flattened_list, paste(raster_info$Country, raster_info$Product, sep = "_"))
baseES_1 <- setNames(flattened_list, paste(raster_info$Country, raster_info$Product, sep = "_"))

# Get the histograms
baseES_v <- lapply(baseES_0, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})

baseESap_v <- lapply(baseES_1, function(r){
  t <- as_tibble(values(r, na.rm=TRUE))
})


# Adding corresponding rows from `df` to each tibble in `list_of_tibbles`
baseES_v <- map2(baseES_v,                  # First input: List of tibbles
  split(raster_info, seq(nrow(raster_info))),         # Second input: Split the dataframe into individual rows
  ~ mutate(.x, Country = .y$Country, Product = .y$Product)  # Add `Country` and `Product` columns from df to each tibble
)

baseESap_v <- map2(baseESap_v,                  # First input: List of tibbles
  split(raster_info, seq(nrow(raster_info))),         # Second input: Split the dataframe into individual rows
  ~ mutate(.x, Country = .y$Country, Product = .y$Product)  # Add `Country` and `Product` columns from df to each tibble
)


new_col <- "Value"
# Use lapply to rename the first column of each tibble
baseES_v <- lapply(baseES_v, function(tbl) {
  tbl %>%
    rename(!!new_col := 1)  # Rename first column using column position
})

baseESap_v <- lapply(baseESap_v, function(tbl) {
  tbl %>%
    rename(!!new_col := 1)  # Rename first column using column position
})


baseES_v <- do.call(rbind, baseES_v)
baseESap_v <- do.call(rbind, baseESap_v)
##################### do not run inside the Rmd!!!!!!!!
baseES_v <- baseES_v%>% mutate(restoration = "0")
baseESap_v <- baseESap_v%>% mutate(restoration = "1")

baseES_v <- rbind(baseES_v, baseESap_v)

baseES_v <- baseES_v %>%
  # Create a new column with simplified service names
  mutate(Service = case_when(
    Product == "global_n_retention_esamod2_compressed" ~ "Nitrogen Retention",
    Product == "global_sed_retention_esamod2_compressed" ~ "Sediment Retention",
    Product == "pollination_ppl_fed_on_ag_10s_esa2020" ~ "Pollination",
    Product == "access_WGS84" ~ "Nature Access",
    TRUE ~ Product  # Keep other names unchanged, if any
  )) 

write.csv(baseES_v, here('data', "distributionServ_restoration.csv"))
# Calculate the 98th percentile for each combination of Service and Country
percentile_98 <- baseES_v %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.98)) %>%
  ungroup()
percentile_95 <- baseES_v %>%
  group_by(Service, Country) %>%
  summarize(threshold = quantile(Value, 0.95)) %>%
  ungroup()


# Join the threshold back to the original data and filter values up to the 98th percentile
baseES_filtered <- baseES_v %>%
  left_join(percentile_98, by = c("Service", "Country")) %>%
  filter(Value <= threshold)

baseES_filtered <- baseES_v %>%
  left_join(percentile_95, by = c("Service", "Country")) %>%
  filter(Value <= threshold)



C <- 5

df <- baseES_filtered %>% filter(Country==country_names[C]) %>% filter(Service!="Nature Access")

rm(t)
t <- ggplot(df, aes(x = Value, fill = restoration)) +
  geom_density(alpha = 0.4) +  # Use transparency with alpha to see overlapping densities
  facet_wrap(~Service, scales = "free") +
  #facet_wrap(~Country, scales = "free") +
  # Set both x and y axes to be free for each facet
  scale_fill_manual(values = c("blue", "orange")) +  # Set colors for the `set` variable
  labs(title = paste("Density Plot of Services Inside and Outside Griscom restoration Areas -", country_names[C]),
       x = "Value",
       y = "Density",
       fill = "Restoration") +
  #scale_y_continuous(trans = 'log10')# Label for the fill legend
  theme_minimal() +
  theme(
    strip.text = element_text(size = 12),  # Increase facet labels' font size
    legend.position = "bottom",  # Place the legend at the bottom
    legend.box = "horizontal"  # Arrange legend items horizontally
  )
t
```

```{r get prportions restoration}

rest <- lapply(rest, function(r){
  r <- project(r, baseES[[1]])
  })


s1 <- list(baseES[[1]],baseES[[6]],baseES[[11]], baseES[[16]])
s2 <- list(baseES[[2]],baseES[[7]],baseES[[12]], baseES[[17]])
s3 <- list(baseES[[3]],baseES[[8]],baseES[[13]], baseES[[18]])
s4 <- list(baseES[[4]],baseES[[9]],baseES[[14]], baseES[[19]])
s5 <- list(baseES[[5]],baseES[[10]],baseES[[15]], baseES[[20]])

#baseEs_ap <- map(baseES, ap, mask)
s11 <- lapply(s1, function(r){
  r <- mask(r,rest[[1]],maskvalues=0)
})
s12 <- lapply(s1, function(r){
  r <- mask(r,rest[[1]],maskvalues=1)
})
s2 <- lapply(s2, function(r){
  r <- mask(r,ap[[2]])
})
s3 <- lapply(s3, function(r){
  r <- mask(r,ap[[3]])
})
s4 <- lapply(s4, function(r){
  r <- mask(r,ap[[4]])
})
s5 <- lapply(s5, function(r){
  r <- mask(r,ap[[5]])
})

```

# Produce land cover change maps
```{r land cover change}

#path to lc classifications
path_lc <- '/Volumes/Jero_HDD/POSDOC/ESA_LC'
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "all"))
lc <- lapply(tf,rast)
lc <- c(lc[[1]],lc[[2]])

#load study areas
poly <- st_read('/Volumes/Jero_HDD/POSDOC/NBS_OPS/vector/areas_nbs.geojson')
nam <- poly$COUNTR
nam <- sort(nam)
poly <- poly%>%split(.$COUNTR)
lc <-map(1:length(poly), function(x) crop(lc, poly[[x]]))
lc <-map(1:length(poly), function(x) mask(lc[[x]], poly[[x]]))
map(1:length(poly), function(x) writeRaster(lc[[x]], paste0(path_lc, '/', 'ESA_LC_rec_', nam[x], '.tif')))

chmaps <- lapply(lc, function(r){
  ch_mapR(r)
})

map(1:length(chmaps), function(x) writeRaster(chmaps[[x]][[1]], paste0(path_lc, '/', 'ESA_LC_change', nam[x], '.tif')), overwite=TRUE)


# pending: set a rule to deal wit those combinations that have to few pixels, not worth the time and effort,

```


#######################################################################

Second part with justin.

# Explore Data. 

The ecorregiosn Raster is not suited, 830 different values. Could be done but does not make a lot of sense
```{r explore data}

ecoregions <- rast('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/Ecoregions2017_compressed_md5_316061.tif')

ec_uniqe <- unique(ecoregions)
```

Use vector file, just the main biomes. 

# PA 

```{r filter out PAs}

pa <- st_read('/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/global-2024-05-08.gpkg')

att <- unique(pa$IUCN_CAT)

filt <- c("Not Reported", "Not Assigned", "Not Applicable")
pa <- pa %>% filter(!(IUCN_CAT %in% filt))

st_write(pa, '/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/PA_filt.gpkg')



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

pa <- st_transform(pa, crs=st_crs(countries))

```

split the rasters by biomes

```{r split countries}
wd. <- '/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products'

tf <- list.files(wd., pattern='1992')
tf <- clean_filename(tf)

tf <- tf[-c(2,3,5)]


tiffes <- file.path(wd., list.files(wd., pattern='1992'))
rs <- lapply(tiffes, rast) 

rs <- rs[-3]

biomes <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/global_products/biomes_WWF.gpkg')


nam <- sort(biomes$BIOME)

biomes <- biomes %>% split(.$BIOME)

# Apply cropping and masking recursively using lapply and map
rs_msk <- lapply(rs, function(raster) {
  map(biomes, function(polygon) {
    # Crop the raster to the extent of the polygon
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
})

outpath <- '~/Documents/Natural_capital/Global_mapping/rest_masked_biomes'
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})

summary_df <- map2_dfr(seq_along(rs_msk), rs_msk, function(i, inner_list) {
  
  # i corresponds to the index of the outer list (e.g., 1, 2, 3...)
  outer_label <- paste0("Outer_", i)  # Label for the outer list
  
  # Loop through each SpatRaster in the inner list
  map_dfr(inner_list, function(raster) {
    extract_raster_summary(raster, outer_label)
  })
})

```


# Split by Countries/continents
# mask by protected areas (all togeter, chategories I-VI)
# Reclassify Land Cover maps. 


#  Change Maps
```{r change_map_calc}
esat <- c('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif','/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif')


esas <- lapply(esat,rast)


nam <- c(2020,1992)

map(1:2, function(x) writeRaster(esas[[x]], paste0('ESA_LC_', nam[x],'.tif')))
cls <- lapply(esas,freq)


wd <- '~/Documents/Natural_capital/NBS_OP/data/ESA_LC'

tf <- file.path(wd, list.files(wd, pattern='tif'))

tf <- tf[c(2,3)]

esas <- lapply(tf,rast)



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

cont <- countries %>% filter(subregion=='Central America')

# Define the reclassification rules in one operation
reclass_table <- data.frame(
  from = c(10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100,
           110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180,
           190, 200, 201, 202, 210, 220),
  to = c(1, 1, 1, 1, 1, 1,   # Cultivated
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # Forests
         3, 3, 3, 3, 3,   # Grasses and Shrubs
         4, 4, 4, 4, 4,   # Sparse Vegetation
         5, 5, 5,   # Mangroves
         6,   # Urban
         7, 7, 7,   # Bare
         8,   # Water
         9)   # Ice
)

esas <- lapply(esas, function(r) {
  subst(r, from = reclass_table$from, to = reclass_table$to)
})


map(1:2, function(x) writeRaster(esas[[x]], paste0('~/Documents/Natural_capital/NBS_OP/data/ESA_LC/', "Forests_esa", year[x], '.tif')))

# Extract forests
esas <- lapply(esas, function(r) {
  subst(r, from = 2, to =1, others=NA)
})


chg <- ch_mapR(esas[[1]],esas[[2]])
freq(chg)

mat_1 <- crosstabm(esas[[1]],esas[[2]])

diffs_p <-  diffTablej(mat, digits=3, analysis = "change")

```


# Get Sankey Diagrams
```{r Sankey Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```



# Next step
1. Zonal Countries. one per service

mask for the forest 

not sum, average. 

Historic services. 

name(test)


Map 4 maps for each servie for the average change fior that time peiod 


map(1:2, function(x) writeRaster(esas[[x]], paste0('~/Documents/Natural_capital/NBS_OP/data/ESA_LC/', "ESA_LC_Rec_", year[x], '.tif')))



cls
[[1]]
   layer value      count
1      1    10  105009519
2      1    11  102087508
3      1    12    2415511
4      1    20   29796402
5      1    30   48088904
6      1    40   44613941
7      1    50  137316150
8      1    60   84649974
9      1    61   11061590
10     1    62   39391781
11     1    70  113472860
12     1    71   45563958
13     1    72      19882
14     1    80  109865531
15     1    81      55745
16     1    82         21
17     1    90   37417117
18     1   100   58048747
19     1   110   15730987
20     1   120  140408175
21     1   121    3007375
22     1   122   32663372
23     1   130  177774130
24     1   140   42447574
25     1   150  155885143
26     1   151         61
27     1   152    1072086
28     1   153    3664319
29     1   160   11677666
30     1   170    2394625
31     1   180   34499895
32     1   190   10933052
33     1   200  247641877
34     1   201    1572727
35     1   202    1244423
36     1   210 5675137546
37     1   220  871449826


So, now what does this mean?



Add column with the class names with multiple aggregation levels (ESA)

    Mean: The average value within each zone (e.g., inside/outside protected areas or across land cover types). This provides a simple understanding of the central tendency.
    Median: The middle value when data is ordered, useful when your data has outliers that could skew the mean.
    Standard Deviation: Indicates the variation or dispersion from the average, helping show if values are clustered or spread out.
    Min/Max: The range of values, showing the extremes within each zone.

2. Zonal Statistics (Per Category):

    Sum: The total value across a zone, particularly useful for variables like ecosystem services (e.g., total nitrogen retention within protected areas).
    Count of Pixels: The number of pixels in each zone, which could give an idea of the area covered by specific land covers.
    Percentage Area Contribution: For each zone or land cover class, express the variable as a percentage of the total landscape (e.g., “30% of nitrogen retention occurs in protected areas”).

3. Comparisons by Category:

    Inside vs. Outside Protected Areas:
        You can calculate mean, sum, and standard deviation inside and outside protected areas to compare the effectiveness of protection measures in preserving ecosystem services.
    Land Cover Comparison:
        Use zonal means to compare how different land cover types contribute to ecosystem services like sediment retention or distance to beneficiaries.
        Express results as percentages, e.g., “Forests account for 60% of nitrogen retention.”

4. Histogram or Distribution Analysis:

    A frequency distribution showing how the values of each variable are distributed across the entire study area or within protected vs. unprotected zones.
    This can help show if most of the service is concentrated in a few high-value areas or spread out.

5. Change Detection (If Time Series Exists):

    If your rasters cover different time periods, you could show net changes in ecosystem service values (e.g., changes in nitrogen retention) inside and outside protected areas or across different land covers.

6. Normalized or Standardized Scores:

    Convert variables into percentile ranks or Z-scores to facilitate comparisons across variables that might have different units (e.g., how “high” or “low” the ecosystem service is in relation to the mean).

7. Threshold Analysis:

    For variables where a threshold is meaningful (e.g., distance to beneficiaries < 10 km), you can count the number of pixels above or below the threshold in each category (protected areas, land covers).
    For example, "80% of the people dependent on agriculture live within 5 km of high-sediment retention areas."

8. Spatial Autocorrelation (Advanced):

    If you're dealing with spatially explicit relationships, calculating spatial autocorrelation (Moran’s I) might help quantify clustering patterns of ecosystem services inside and outside protected areas.

9. Cumulative Impact Maps:

    Create a composite map where multiple variables are aggregated to show high- vs. low-value areas for ecosystem services. This could simplify complex data into an easily interpretable map for non-specialists.

These summaries will provide enough detail to highlight key insights, while also being interpretable by those without technical expertise. Let me know if you'd like to dive deeper into any specific metric or approach!


Produyces forest loss maps/gain permanences


Country/continent

Biomes. (Köppen)









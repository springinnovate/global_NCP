---
title: "DaTa Preparation NBS"
output: html_notebook
---

I will use this project to peprare and extract insights from the raster data for the polygons in the 5 prioritized areas.


1. Prepare Environemnt (Load necessary libraries)
```{r load libraries}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools')
sapply(packs, require, character.only = TRUE)
```


2. Load The polygons with the NBS areas
```{r}

poly <- st_read('/Volumes/Jero_HDD/WWF/NBS_OPS/vector/areas_nbs.geojson') 
#Split the vector into a list of individual polygons
#create vector with names
nam <- poly$COUNTR
nam <- sort(nam)
poly <- poly%>%split(.$COUNTR)
```


3. Prepare Inventory to organize data 

I am having to many issues with the data ialready have and the one i don't and it gests confusing, so I am workin on a table that shhould make this easier

```{r organzie inventory}

tab <- as_tibble(read.csv('/Volumes/Jero_HDD/POSDOC/Downloaded_data_ES/inventory_global_products.csv'))
library(dplyr)

# Assuming your tibble is called 'data'
tab <- tab %>%
  select(-matches("^X(\\.\\d+)?$"))

# Print the cleaned tibble
print(cleaned_data)

tab <- tab %>% slice(1:218)
tab <- tab %>% mutate(Filename= basename(Link))

write.csv(tab, file= '/Volumes/Jero_HDD/POSDOC/Downloaded_data_ES/inventory_global_products.csv')

```
Test with one raster
```{r load rst}

esamod <- rast('/Volumes/Jero_HDD/WWF/global_outputs/ESAmodVCFv2_md5_05407ed305c24604eb5a38551cddb031.tif')

esamod <- map(1:length(poly), function(x) crop(esamod, poly[[x]]))
esamod <- map(1:length(esamod), function(x) mask(esamod[[x]], poly[[x]]))

map(1:length(esamod), function(x) writeRaster(esamod[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'esaLC_', nam[x], '.tif'), overwrite=TRUE))

access_nature<- rast('/Volumes/Jero_HDD/WWF/global_outputs/nature_access_lspop2019_esa2020modVCFhab_md5_a6519ebd8b941444921e749da2e645bb_WGS84.tif')


access_nature <- map(1:length(poly), function(x) crop(access_nature, poly[[x]]))
access_nature <- map(1:length(access_nature), function(x) mask(access_nature[[x]], poly[[x]]))

map(1:length(access_nature), function(x) writeRaster(access_nature[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'nature_access_ESA_2020', nam[x], '.tif'), overwrite=TRUE))


acces_rest <- rast('/Volumes/Jero_HDD/WWF/global_outputs/nature_access_diff_Sc3v1_PNVnoag-esa2020.tif')
acces_rest <- project(acces_rest, acc2, method='bilinear')

acces_rest <- map(1:length(poly), function(x) crop(acces_rest, poly[[x]]))
acces_rest <- map(1:length(acces_rest), function(x) mask(acces_rest[[x]], poly[[x]]))

map(1:length(access_nature), function(x) writeRaster(acces_rest[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'nature_access_diff_Sc3v1_PNVnoag-esa2020_', nam[x], '.tif'), overwrite=TRUE))

acces_rest <- rast('/Volumes/Jero_HDD/WWF/global_outputs/global_people_access_population_2019_60.0m_md5_d264d371bd0d0a750b002a673abbb383.tif')
acces_rest <- project(acces_rest, acc2, method='bilinear')

acces_rest <- map(1:length(poly), function(x) crop(acces_rest, poly[[x]]))
acces_rest <- map(1:length(acces_rest), function(x) mask(acces_rest[[x]], poly[[x]]))

map(1:length(access_nature), function(x) writeRaster(acces_rest[[x]], paste0('/Volumes/Jero_HDD/WWF/NBS_OPS/raster/', 'global_people_access_population_2019_60.0m_', nam[x], '.tif'), overwrite=TRUE))

    
    
access_nature<- rast('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESAmodVCFv2_cv_habitat_value_md5_c01e9b17aee323ead79573d66fa4020d.tif')

access_nature <- map(1:length(poly), function(x) crop(access_nature, poly[[x]]))
access_nature <- map(1:length(access_nature), function(x) mask(access_nature[[x]], poly[[x]]))

map(1:length(access_nature), function(x) writeRaster(access_nature[[x]], paste0('/Users/sputnik/Documents/Natural_capital/NBS_OP/data/raster/', 'coastal_risk_reduction', nam[x], '.tif'), overwrite=TRUE))



```


# Iterate over all rasters (this has been annoying)

```{r iterate crop}
wd <- '/Volumes/Jero_HDD/WWF/global_outputs'
setwd(wd)

tiffes <- list.files(wd, pattern= 'downstream')
#tiffes <- tiffes[-c(5,7)]
paths <- file.path(wd, tiffes)

clean_filename <- function(filename) {
  sub("_md5.*", "", filename)
}

output_path <- '/Volumes/Jero_HDD/WWF/NBS_OPS/raster/'

# Remove faulty data
#tiffes <- tiffes[-1]

# Loop through each geotiff file
for (i in seq_along(tiffes)) {
  
  # Get the current tiff file
  tiff_file <- paths[i]
  
  # Load the current geotiff (don't load all at once)
  access <- rast(tiff_file)
  
  # Apply crop and mask operations to each polygon
  cropped <- map(1:length(poly), function(x) crop(access, poly[[x]]))
  masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(tiff_file))
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) {
    writeRaster(masked[[x]], paste0(output_path, base_filename, "_", nam[x], ".tif"), overwrite = TRUE)
  })
  # Optionally, clear the variable to free up memory
  rm(access, cropped, masked)
  gc()  # Garbage collection to clear memory
}

```


# Reproj Accesibility Raster

```{r reproj access}
acc <- rast('/Volumes/Jero_HDD/WWF/global_outputs/nature_access_lspop2019_esa2020modVCFhab_md5_a6519ebd8b941444921e749da2e645bb.tif')
acc

acc2 <- rast('/Volumes/Jero_HDD/WWF/global_outputs/temp/access_WGS84.tif')

acc <- project(acc,acc2)
writeRaster(acc, '/Volumes/Jero_HDD/WWF/global_outputs/nature_access_lspop2019_esa2020modVCFhab_md5_a6519ebd8b941444921e749da2e645bb_WGS84.tif')
```
# Explore rasters
(max min values, unique, frequencies)
```{r explore rasters}
sediment <- rast('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/raster/sediment_ESAmod2-Sc3v1_MEXICO.tif')
```

#summaries

For summarizing geotiff raster files and comparing values across protected areas or different land covers for an initial assessment, especially for an audience without geospatial expertise, here are some accessible metrics you can use:
1. Basic Statistical Summaries:

```{r basic summaries}

here('data', 'raster')

lc <- list.files(here('data', 'raster'), pattern= 'LC')
lc <- file.path(here('data', 'raster'),lc)
cls <- lapply(lc, rast)
fr <- lapply(cls, freq)

lsmet <- lapply(cls, function(r){
  mets <- lsm_c_ai(r)
})

lsmet <- lapply(seq_along(lsmet), function(i) {
  lsmet[[i]]$AREA <- nam[i]
  return(lsmet[[i]])
})
```


Beneficiaries and coastal protection pop< 10 dowstreams

```{r load coastal}

wd <- '/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/'

gt <- list.files(wd, pattern='geojson')

gt <- gt[-c(1,2,8)]

gt <- file.path(gt)

cp <- lapply(gt, st_read)

```



45 - 19 (calculate the values) for coastal riske reduction

```{r coastal risk red}

wd <- '/Volumes/Jero_HDD/POSDOC/NBS_OPS/vector'

coastal_risk_reduction_MEXICO.geojson

pat <- file.path(wd, list.files(wd, pattern="coastal_risk"))
pat <- pat[c(1,3,5,7,9)]

vat <- lapply(pat, st_read)
vat <- lapply(vat, function(v){
  v <- v %>% mutate(rt_fin=Rt_nohab_all-Rt)
})


library(sf)
library(janitor)

# Standardize column names and save each sf object
lapply(seq_along(vat), function(i) {
  # Clean column names to ensure compatibility
  vat[[i]] <- clean_names(vat[[i]])
  
  # Save the sf object to the specified path
  st_write(vat[[i]], pat[i], append = FALSE, delete_layer = TRUE)
})


lapply(seq_along(vat), function(i) {
  st_write(vat[[i]], pat[i], append = TRUE)
})


```

Substract these columnns, and map this thing again 
Add legenf (in terms of hig-low)

include access change

then, go throuhg these change rasterd (priority 4 and take al ook at them and think waht can be done here, and show differences for land covers), inside7outside protected areas (UICN cat National park Ia , Ib, II, III, IV , V, VI)



#######################################################################

Second part with justin.

# Explore Data. 

The ecorregiosn Raster is not suited, 830 different values. Could be done but does not make a lot of sense
```{r explore data}

ecoregions <- rast('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/Ecoregions2017_compressed_md5_316061.tif')

ec_uniqe <- unique(ecoregions)
```

Use vector file, just the main biomes. 

# PA 

```{r filter out PAs}

pa <- st_read('/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/global-2024-05-08.gpkg')

att <- unique(pa$IUCN_CAT)

filt <- c("Not Reported", "Not Assigned", "Not Applicable")
pa <- pa %>% filter(!(IUCN_CAT %in% filt))

st_write(pa, '/Users/sputnik/Documents/Natural_capital/NBS_OP/data/vector/PA_filt.gpkg')



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

pa <- st_transform(pa, crs=st_crs(countries))

```

split the rasters by biomes

```{r split countries}
wd. <- '/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products'

tf <- list.files(wd., pattern='1992')
tf <- clean_filename(tf)

tf <- tf[-c(2,3,5)]


tiffes <- file.path(wd., list.files(wd., pattern='1992'))
rs <- lapply(tiffes, rast) 

rs <- rs[-3]

biomes <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/global_products/biomes_WWF.gpkg')


nam <- sort(biomes$BIOME)

biomes <- biomes %>% split(.$BIOME)

# Apply cropping and masking recursively using lapply and map
rs_msk <- lapply(rs, function(raster) {
  map(biomes, function(polygon) {
    # Crop the raster to the extent of the polygon
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
})

outpath <- '~/Documents/Natural_capital/Global_mapping/rest_masked_biomes'
# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})

summary_df <- map2_dfr(seq_along(rs_msk), rs_msk, function(i, inner_list) {
  
  # i corresponds to the index of the outer list (e.g., 1, 2, 3...)
  outer_label <- paste0("Outer_", i)  # Label for the outer list
  
  # Loop through each SpatRaster in the inner list
  map_dfr(inner_list, function(raster) {
    extract_raster_summary(raster, outer_label)
  })
})

```


# Split by Countries/continents
# mask by protected areas (all togeter, chategories I-VI)
# Reclassify Land Cover maps. 


#  Change Maps
```{r change_map_calc}
esat <- c('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif','/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif')


esas <- lapply(esat,rast)


nam <- c(2020,1992)

map(1:2, function(x) writeRaster(esas[[x]], paste0('ESA_LC_', nam[x],'.tif')))
cls <- lapply(esas,freq)


wd <- '~/Documents/Natural_capital/NBS_OP/data/ESA_LC'

tf <- file.path(wd, list.files(wd, pattern='tif'))

tf <- tf[c(2,3)]

esas <- lapply(tf,rast)



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

cont <- countries %>% filter(subregion=='Central America')

# Define the reclassification rules in one operation
reclass_table <- data.frame(
  from = c(10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100,
           110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180,
           190, 200, 201, 202, 210, 220),
  to = c(1, 1, 1, 1, 1, 1,   # Cultivated
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # Forests
         3, 3, 3, 3, 3,   # Grasses and Shrubs
         4, 4, 4, 4, 4,   # Sparse Vegetation
         5, 5, 5,   # Mangroves
         6,   # Urban
         7, 7, 7,   # Bare
         8,   # Water
         9)   # Ice
)

esas <- lapply(esas, function(r) {
  subst(r, from = reclass_table$from, to = reclass_table$to)
})


map(1:2, function(x) writeRaster(esas[[x]], paste0('~/Documents/Natural_capital/NBS_OP/data/ESA_LC/', "Forests_esa", year[x], '.tif')))

# Extract forests
esas <- lapply(esas, function(r) {
  subst(r, from = 2, to =1, others=NA)
})


chg <- ch_mapR(esas[[1]],esas[[2]])
freq(chg)

mat_1 <- crosstabm(esas[[1]],esas[[2]])

diffs_p <-  diffTablej(mat, digits=3, analysis = "change")


classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```
# Next step
1. Zonal Countries. one per service

mask for the forest 

not sum, average. 

Historic services. 

name(test)


Map 4 maps for each servie for the average change fior that time peiod 


map(1:2, function(x) writeRaster(esas[[x]], paste0('~/Documents/Natural_capital/NBS_OP/data/ESA_LC/', "ESA_LC_Rec_", year[x], '.tif')))



cls
[[1]]
   layer value      count
1      1    10  105009519
2      1    11  102087508
3      1    12    2415511
4      1    20   29796402
5      1    30   48088904
6      1    40   44613941
7      1    50  137316150
8      1    60   84649974
9      1    61   11061590
10     1    62   39391781
11     1    70  113472860
12     1    71   45563958
13     1    72      19882
14     1    80  109865531
15     1    81      55745
16     1    82         21
17     1    90   37417117
18     1   100   58048747
19     1   110   15730987
20     1   120  140408175
21     1   121    3007375
22     1   122   32663372
23     1   130  177774130
24     1   140   42447574
25     1   150  155885143
26     1   151         61
27     1   152    1072086
28     1   153    3664319
29     1   160   11677666
30     1   170    2394625
31     1   180   34499895
32     1   190   10933052
33     1   200  247641877
34     1   201    1572727
35     1   202    1244423
36     1   210 5675137546
37     1   220  871449826


So, now what does this mean?



Add column with the class names with multiple aggregation levels (ESA)

    Mean: The average value within each zone (e.g., inside/outside protected areas or across land cover types). This provides a simple understanding of the central tendency.
    Median: The middle value when data is ordered, useful when your data has outliers that could skew the mean.
    Standard Deviation: Indicates the variation or dispersion from the average, helping show if values are clustered or spread out.
    Min/Max: The range of values, showing the extremes within each zone.

2. Zonal Statistics (Per Category):

    Sum: The total value across a zone, particularly useful for variables like ecosystem services (e.g., total nitrogen retention within protected areas).
    Count of Pixels: The number of pixels in each zone, which could give an idea of the area covered by specific land covers.
    Percentage Area Contribution: For each zone or land cover class, express the variable as a percentage of the total landscape (e.g., “30% of nitrogen retention occurs in protected areas”).

3. Comparisons by Category:

    Inside vs. Outside Protected Areas:
        You can calculate mean, sum, and standard deviation inside and outside protected areas to compare the effectiveness of protection measures in preserving ecosystem services.
    Land Cover Comparison:
        Use zonal means to compare how different land cover types contribute to ecosystem services like sediment retention or distance to beneficiaries.
        Express results as percentages, e.g., “Forests account for 60% of nitrogen retention.”

4. Histogram or Distribution Analysis:

    A frequency distribution showing how the values of each variable are distributed across the entire study area or within protected vs. unprotected zones.
    This can help show if most of the service is concentrated in a few high-value areas or spread out.

5. Change Detection (If Time Series Exists):

    If your rasters cover different time periods, you could show net changes in ecosystem service values (e.g., changes in nitrogen retention) inside and outside protected areas or across different land covers.

6. Normalized or Standardized Scores:

    Convert variables into percentile ranks or Z-scores to facilitate comparisons across variables that might have different units (e.g., how “high” or “low” the ecosystem service is in relation to the mean).

7. Threshold Analysis:

    For variables where a threshold is meaningful (e.g., distance to beneficiaries < 10 km), you can count the number of pixels above or below the threshold in each category (protected areas, land covers).
    For example, "80% of the people dependent on agriculture live within 5 km of high-sediment retention areas."

8. Spatial Autocorrelation (Advanced):

    If you're dealing with spatially explicit relationships, calculating spatial autocorrelation (Moran’s I) might help quantify clustering patterns of ecosystem services inside and outside protected areas.

9. Cumulative Impact Maps:

    Create a composite map where multiple variables are aggregated to show high- vs. low-value areas for ecosystem services. This could simplify complex data into an easily interpretable map for non-specialists.

These summaries will provide enough detail to highlight key insights, while also being interpretable by those without technical expertise. Let me know if you'd like to dive deeper into any specific metric or approach!


Produyces forest loss maps/gain permanences


Country/continent

Biomes. (Köppen)


Population.

Acess Population access. With the change un land covers. 

National park Ia , 1b, 2, III, IV , V, VI.







---
title: "Current Status"
output: html_notebook
---


```{r setup, message=FALSE, warning=FALSE}
library(terra)
library(sf)
library(dplyr)
library(ggplot2)
library(glue)
library(tidyr)
library(purrr)
library(diffeR)
library(here)
library(stringr)
library(tidytext)
library(rlang)
library(tidyr)
library(forcats)
library(scales)
library(RColorBrewer)
library(htmltools)
library(leaflet)
library(devtools)
load_all()
#source the helper functions
source(here("R", "hotSpotR.R"))
source(here("R","utils_pct_change.R"))
source(here("R", "utils_lcc_metrics.R"))
source(here("R","utils_pct_change.R"))
source(here("R", "perc_filteR.R"))
```


#  1 Organize output Dataframes (make sure that they are complete and correclty named)

Right now, 24/04/2025 the problem is on the way the dataframe with the class colors and names is built. The process is still too manual (writing the table) and is easy to get it wrong when it should not be big deal. Ideally, there should be a dialogue box to fill this instead. This should take care of the class names, values and colors neatl
Also, the order of the factors needs to be set, and new classes have to be added. Again, i don't how to do it easily. I mean, it is easy, but annoying to do and easy to get wrong.


#######################################################
Here, i need to clean the directory form a lot of old versions. Only the groupings. If pct not there, it is fine, that is easy to calculate now. 
**Finally, syntesis data consolidated into a single dataset**

```{r consolidate final outputs}

#Builds a list of files in the vector directory
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_NCP/data/vector'
sets <- file.path(inpath, list.files(inpath, pattern = "hydrosheds.*\\.gpkg$"))
# Set and select an index to load the desired dastaset
t <- 1
set <- sets[t]

# # load polygons
print(lyr)
test1 <- st_read(set)

lyr <- st_layers(sets[3])
poly2 <- st_read(sets[3],layer= "hydrosheds_lv6_synth")


# 1. Drop geometry from the one you're joining in
test1 <-test1 %>% st_drop_geometry()

# 2. Identify overlapping columns (besides ID or join key)
overlap <- intersect(names(poly2), names(test1))

# Optionally drop duplicates (except for the join key, say "HYBAS_ID")
join_key <- "HYBAS_ID"
to_drop <- setdiff(overlap, join_key)


# 3. Remove those overlapping columns from b_data
b_data_unique <- b_data %>% select(-all_of(to_drop))

# 4. Join the cleaned b_data to a (which retains the geometry)
final <- left_join(a, b_data_unique, by = join_key)

test1 <- test1 %>% select(-all_of(to_drop))
test1 <- test1 %>% mutate(HYBAS_ID = as.character(HYBAS_ID))

poly<- left_join(poly2, test1, by = join_key)
poly <- poly %>% rename(N_Ret_Ratio_pct_ch = N_Ret_Ratio)
# adjust name 
are <- plt[c(1,55)]

poly <- left_join(poly,are)
st_write(poly, paste0(inpath, '/', 'hydrosheds_lv6_synth.gpkg'), append=FALSE)
```

# Load and pivot 

Here, the spatial objects with the additional attributes are loaded and reformatted for analysis and chart preparation - **pivot & tidy**. 
Add to the documentation , environment or however that's called the vectors to select and order the columns. 
This should actually be done as a database structure. The columns live somewhere and are summoned upon need from a set of options (list_dir). 
  
```{r chunk1, eval=FALSE, include=FALSE}
# 
#  get pct as standalone data to add to the table
# Most recent version of synthesis data:
 
 plt <- st_drop_geometry(poly)
 
 # Edit/Adjust column names # not yet
 names(plt) <- stringr::str_replace_all(names(plt), "_2020_1992", "")
 # cakculate POP Density

 

ct <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_ES_TimeSeries/vector/cartographic_ee_ee_r264_correspondence.gpkg')

ct <- ct %>% mutate(area_sqkm_country = as.numeric(st_area(geom)) / 10^6)
ct <- st_drop_geometry(ct)
ct <- ct[c(1,37)]
plt <- left_join(plt, ct)
#. Have a method to name versions or something (maybe?)
write.csv(plt, file= here('output_data', "metrics_change_hb_lev_6.csv"))

############# PIVOT TABLE ##############################
 
 # Define the socioeconomic variables
socio_vars <- c(
  "GHS_BUILT_S_E2020_GLOBE_R2023A_4326_3ss_V1_0_mean",# this is also a problem!!!
  "rast_gdpTot_1990_2020_30arcsec.7_mean",
  #"GHS_POP_E2020_GLOBE_R2023A_4326_3ss_V1_0.1_mean", # this is a problem here!!!!
  #"GlobPOP_Count_30arc_2020_I32.1_sum", we don't need this one anymore, already calc per sq km
  "hdi_raster_predictions_2020.1_mean",
  "farmsize_mehrabi.1_mean",
  "GlobPOP_sqkm"
)
###### #### #### Attention, thos will be reviewd for per sq/km and check pollinayion and USLE, but 
# Pivot all columns that contain pct_ch, keeping other relevant vars
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_ch"),
    names_to = "service",
    values_to = "pct_ch"
  ) %>%
  select(HYBAS_ID, service, pct_ch, all_of(socio_vars)) %>% 
   mutate(service = str_replace(service, "_pct_ch.*", ""))
  

plt_long <- plt_long%>% filter(service != "Pop") %>% filter(service!= "n_export")
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_ch)) %>% filter(pct_ch != Inf)  

  plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "Sed_retention_ratio" ~ "Sed_Ret_Ratio",
   service == "Usle" ~ "USLE",
   TRUE ~ service
   ))
 
```
# Identify Hotspots 

```{r get_hotspots}

# 1. Identify hotspots by service

# Set your cutoff
pct_cutoff <- 0.05  # for top/bottom 1%

df_hotspots <- plt_long %>%
  group_by(service) %>%
  mutate(
    n_total = n(),
    n_cut = ceiling(n_total * pct_cutoff),
    rank_high = rank(-pct_ch, ties.method = "first"),
    rank_low = rank(pct_ch, ties.method = "first"),
    hotspot_flag = case_when(
      rank_high <= n_cut ~ "high",
      rank_low <= n_cut ~ "low",
      TRUE ~ NA_character_
    ),
    hotspot_binary = !is.na(hotspot_flag)
  ) %>%
  ungroup()

# 2. Count how many times each HYBAS_ID is a hotspot
hotspot_summary <- df_hotspots %>%
  filter(hotspot_binary) %>%
  group_by(HYBAS_ID) %>%
  summarise(
    hotspot_count = n(),
    hotspot_services = list(unique(service))
  )
hotspot_summary <- hotspot_summary %>%
  mutate(hotspot_services = lapply(hotspot_services, \(x) trimws(as.character(x))),
         hotspot_services = sapply(hotspot_services, \(x) paste(x, collapse = ", ")))


# remove unnecesary columns (original 92-2020 values, keep)
spring_ES <- poly[c(1:18,22,36:40,45,46:54)]
# reorganize columns
spring_ES <- spring_ES[c(1:19, 20,22,21,25,34,23,24,26:33)]



spring_ES <- left_join(spring_ES, hotspot_summary, by = "HYBAS_ID")
hotspots_change <- spring_ES 

hotspots_change <- left_join(hotspots_change,are)
st_write(hotspots_change, here('vector', "hydrosheds_lv_6_hotspots.gpkg"), append = FALSE)
st_write(hotspots_change, paste0(inpath, '/',"hydrosheds_lv_6_hotspots.gpkg"), append=FALSE)
```
# set class names and colors for plotting 

Here, another dialogue to input the variable names, plot colors and set the factor order.
```{r}

hotspots_change <- st_drop_geometry(hotspots_change)
hot <- hotspots_change[c(1:19,35:37)]

plt_long. <- left_join(plt_long, hot)
# export final database long. 
write.csv(plt_long., here("output_data", "serv_benef_synth_02.csv"))

service <- unique(plt_long$service)
service <- c("Coastal_Protection","Pollination","N_export","N_Ret_Ratio","Sed_export", "Sed_Ret_Ratio", "USLE")#, "Pop_sqkm","GDP")
 #color <- c("#9e9ac8", "#dd1c77","#2c944c", "#2c711c","#08306b", "#02606b", "#d4ac0d", "#85929e")
 service_levels <- service
 
```


```{r final_benef_synth, eval=FALSE, include=FALSE}
# 
#  get pct as standalone data to add to the table
# Most recent version of synthesis data:
 
 plt <- st_drop_geometry(hotspots_change)
 
 # Edit/Adjust column names # not yet
 names(plt) <- stringr::str_replace_all(names(plt), "_2020_1992", "")
 # cakculate POP Density

 

ct <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/global_ES_TimeSeries/vector/cartographic_ee_ee_r264_correspondence.gpkg')

ct <- ct %>% mutate(area_sqkm_country = as.numeric(st_area(geom)) / 10^6)
ct <- st_drop_geometry(ct)
ct <- ct[c(1,37)]
plt <- left_join(plt, ct)
#. Have a method to name versions or something (maybe?)
write.csv(plt, file= here('output_data', "metrics_change_hb_lev_6.csv"))

############# PIVOT TABLE ##############################
 
 # Define the socioeconomic variables
socio_vars <- c(
  "GHS_BUILT_S_E2020_GLOBE_R2023A_4326_3ss_V1_0_mean",# this is also a problem!!!
  "rast_gdpTot_1990_2020_30arcsec.7_mean",
  #"GHS_POP_E2020_GLOBE_R2023A_4326_3ss_V1_0.1_mean", # this is a problem here!!!!
  #"GlobPOP_Count_30arc_2020_I32.1_sum", we don't need this one anymore, already calc per sq km
  "hdi_raster_predictions_2020.1_mean",
  "farmsize_mehrabi.1_mean",
  "GlobPOP_sqkm"
)
###### #### #### Attention, thos will be reviewd for per sq/km and check pollinayion and USLE, but 
# Pivot all columns that contain pct_ch, keeping other relevant vars
plt_long <- plt %>%
  pivot_longer(
    cols = matches("pct_ch"),
    names_to = "service",
    values_to = "pct_ch"
  ) %>%
  select(HYBAS_ID, service, pct_ch, all_of(socio_vars)) %>% 
   mutate(service = str_replace(service, "_pct_ch.*", ""))
  

plt_long <- plt_long%>% filter(service != "Pop") %>% filter(service!= "n_export")
# remove infinite values 
plt_long <- plt_long%>% filter(!is.na(pct_ch)) %>% filter(pct_ch != Inf)  

  plt_long <- plt_long %>% mutate(service = case_when(
   service == "sed_export" ~ "Sed_export",
   service == "Sed_retention_ratio" ~ "Sed_Ret_Ratio",
   service == "Usle" ~ "USLE",
   TRUE ~ service
   ))
 
```

#################################################################

 This could be a dialogue box!!!
```{r load-plt}
# apply filter (5% or 2%) # Remove the smallest basins. 
area_threshold <- quantile(plt$SUB_AREA, probs = 0.05, na.rm = TRUE)
# Filter the dataset
df <- plt %>% filter(SUB_AREA > area_threshold)
# get the filtered locations

```

---
title: "Summary ES"
author: "Jeronimo Rodriguez-Escobar"
date: "2025-02-12"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

  
1.	Perform zonal statistics (using exactextractr).
3.	Create faceted bar plots (using ggplot2) for each raster statistic.
4.	Generate choropleth maps (using ggplot2 and sf) to visualize spatial distributions.

Compute mean and median for each polygon. Evaluate other metrics

At a country level, this was done using the *fids* for the polygons. Using country names is problematic because of geopolitics.
Pending to do: Think how to deal with the dependencies and complex cases (GB, France, others), but i will take care of this at a later point.

```{r setup, include=FALSE}
# Load required libraries
library(sf)             # For reading and working with vector data
library(dplyr)          # For data manipulation
library(terra)          # For reading raster data (SpatRasters)
library(exactextractr)  # For zonal statistics
library(ggplot2)        # For plotting
library(forcats)        # For factor reordering
library(tidytext)
library(here)
library(patchwork)
library(tidyr)
library(parallel)
```

## 1. Load Polygons

```{r load polygons, include=FALSE}

# Replace with the actual path to your polygon file (e.g., a shapefile)
poly <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/global_poly_JJ.gpkg') 
# Load polygons as an sf objectpolygons_sf <- st_read(vector_path)
poly_wb <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/poly_wb.geojson')
# filter by region (optional)
#poly <- poly %>%
#  filter(continent == "South America")
```

I am using the rasters with the modeled ES that represent the same service for 1992 and 2020 in the same units (of course). There were not the ones originally prioritized and yhr ones i have been working on but a different set that is the only one that i can actually use


2. Load a List of GeoTIFF Rasters

Following products were assessed:
```{r load raster es, echo=FALSE}
# Vector of raster file paths
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/Downloaded_data_ES2'
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
tiffes <- tiffes[-c(1,10,13,14,17)]
tiffes <- tiffes[-c(7,8)]
filename<- basename(tiffes)
# Use terra::rast() to load each file as a SpatRaster object
rasters_list <- lapply(tiffes, rast)
service <- rep(c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination"), each=2)
filename<- as_tibble(cbind(service,filename))
#colnames(prod) <- c("Service", "Filename")
print(filename)
```

# 3. Calcualte Differences (check with original) 


```{r get differences}
outpath <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/Downloaded_data_ES2"
m_index <- length(rasters_list) %/% 2
lab <- unique(service)
l1 <- rasters_list[seq(1,length(rasters_list), by=2)]
l2 <- rasters_list[seq(2,length(rasters_list), by=2)]
 # test with future_map2 next time to get this to run faster. 

diffs <- map2(l1,l2, function(l1,l2){
  return(l1-l2)
})

map(1:length(diffs), function(x) writeRaster(diffs[[x]], paste0(outpath, "/", 'change_calc', '/', lab[x], '_diff.tif')))
```

## 3. Compute Zonal Statistics with exactextractr for each one

```{r compute stats 1, eval=FALSE, include=FALSE}
diffs <- lapply(tiffes, rast)

# We'll compute mean and median as an example
target_stats <- c("mean", "median")

# exact_extract can append columns from the polygon layer;
# Number of CPU cores to use
num.cores <- detectCores() - 4  # Leave 4 cores free. Adjust depending on available resources. 
# here, set a way to change the input poly and columns externally instead of editing inside of the functions. That is cumbersome and prone to errors. 
results_list <- mclapply(rasters_list, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = "id")
  r_name <- names(r)
  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)
},mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df <- do.call(rbind, results_list)

raster_name <- unique(zonal_df$raster_name)
service <- rep(c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination"), each = 2)
color <- rep(c("#9e9ac8", "#2c944c", "#08306b", "#A57C00", "#dd1c77"), each=2)
cd <- as_tibble(cbind(raster_name, service, color))

zonal_df <- left_join(zonal_df,cd)
  zonal_df$year <- ifelse(grepl("1992", zonal_df$raster_name), 
                          1992, 
                          ifelse(grepl("2020", zonal_df$raster_name), 2020, NA))


poly_df <- st_drop_geometry(poly) %>% select(id, ee_r264_name,iso3,continent,region_wb,income_grp,name_long)  

zonal_df <- zonal_df %>% left_join(poly_df,by = "id")

# i think we don't need this anymore. Saving the outputs both as RData and csv is overkill and not that necessary  
save(zonal_df, file=here('output_data', 'zonal_df_global.RData'))
write.csv(zonal_df, here('output_data', "zonal_summaries_global.csv"))

```


## 4. Now do the differences  
```{r compute stats diff, eval=FALSE, include=FALSE}

# load the polygons with the new attributes to add new 
poly <- st_read('/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/global_poly_means.gpkg')
inpath <- paste0(outpath, "/", 'change_calc')

# load the rasters with the calculated differences 
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
filename<- basename(tiffes)

# Use terra::rast() to load each file as a SpatRaster object
rasters_list <- lapply(tiffes, rast)
service <- c("Coastal Protection", "Nature Access", "Nitrogen Export","Pollination","Sediment Export")
filename<- as_tibble(cbind(service,filename))
#colnames(prod) <- c("Service", "Filename")
print(filename)
diffs <- lapply(tiffes, rast)
# We'll compute mean and median as an example
target_stats <- c("mean", "median")

# exact_extract can append columns from the polygon layer;
# Number of CPU cores to use
num.cores <- detectCores() - 2  # Leave 4 cores free. Adjust depending on available resources. 

results_list <- mclapply(diffs, function(r) {
  # Perform exact extraction
  # - append_cols = "country" retains the country identifier with each result
  res <- exact_extract(r, poly,
                       fun = target_stats,
                       append_cols = "id")
  r_name <- names(r)

  # Add a column labeling which raster these results are from
  res$raster_name <- r_name
  return(res)
},mc.cores = num.cores)

# Combine results from all rasters into one data frame
zonal_df <- do.call(rbind, results_list)

raster_name <- unique(zonal_df$raster_name)
service <- rep(c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination"), each = 1)
color <- rep(c("#9e9ac8", "#2c944c", "#08306b", "#A57C00", "#dd1c77"), each=1)
cd <- as_tibble(cbind(raster_name, service, color))

# Very important. This line adds the new attribnutes to the polygons. I could add all here.

zonal_wide <- pivot_wider(zonal_df, id_cols=id, names_from=c(service,year), values_from = c(mean, median))

poly <- left_join(poly, zonal_wide, by= "id")
st_write(poly, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/global_poly_means.gpkg') 

save(zonal_df, file=here('output_data', 'diffs_wb.RData'))
write.csv(zonal_df, here('output_data', "diffs_wb_.csv"))
# Show a snippet of the combined resultshead(zonal_df)
```


```{r load raster es, echo=FALSE}
# Vector of raster file paths
inpath <- '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/Downloaded_data_ES2'
tiffes <- file.path(inpath, list.files(paste0(inpath),pattern= 'tif$'))
tiffes <- tiffes[-c(1,10,13,14,17)]
tiffes <- tiffes[-c(7,8)]

filename<- basename(tiffes)

# Use terra::rast() to load each file as a SpatRaster object
rasters_list <- lapply(tiffes, rast)
service <- rep(c("Coastal Protection", "Nitrogen Export", "Sediment Export", "Nature Access", "Pollination"), each=2)
filename<- as_tibble(cbind(service,filename))
#colnames(prod) <- c("Service", "Filename")
print(filename)
```

We want separate bar plots (facets) for each raster, showing (for instance) mean or median ecosystem service values by country, sorted from high to low.
Here there are some aspects we need to take care eventually: 
1. Including all countries result in too many bars obviously and there are visualization issues so we need to do following (which of course is going to be easier said than done): Create "partial/interrupted barplots and include only the top and bottom values and leafe a space in the middle.
2. We would like to be able to filter by group (eg income level/region or any other atrribute, so we need to join them so we can apply a filter for charting)
## 4. Plot Mean ES Value for each year
```{r barplots, echo=FALSE, fig.height=8, fig.width=14}

# here need to add the names again. Need to think which column to use for that. 
# Data preparation: reorder countries by mean for each service
data_prepped <- zonal_df %>% filter(!is.na(mean)) %>%  # Remove rows with NA mean values
  mutate(name_long = reorder_within(name_long, -mean, service))  # Reorder within each service
# For each service, get min and max across *both* years (so 1992 and 2020)
service_range <- data_prepped %>%
  group_by(service) %>%
  summarize(
    min_val = min(mean, na.rm = TRUE),
    max_val = max(mean, na.rm = TRUE),
    .groups = "drop"
  )


# Pivot min_val and max_val to a "long" format so we can plot them
range_data_1992 <- service_range %>%
  pivot_longer(cols = c(min_val, max_val), 
               names_to = "range_type", 
               values_to = "mean") %>%
  mutate(
    # We include year=1992 so it will show up in the 1992 plot
    year = 1992,
    # We need some dummy country name so x won't break; 
    # "dummy" won't appear on your real bars
    name_long = "dummy"
  )

range_data_2020 <- service_range %>%
  pivot_longer(cols = c(min_val, max_val), 
               names_to = "range_type", 
               values_to = "mean") %>%
  mutate(
    year = 2020,
    name_long = "dummy"
  )


plot_data_1992 <- data_prepped %>% filter(year == 1992)

p_1992 <- ggplot() +
  # 1) Plot the real bars
  geom_bar(
    data = plot_data_1992,
    aes(x = name_long, y = mean, fill = color),
    stat = "identity", show.legend = FALSE
  ) +
  # 2) Add invisible “blank” geoms with min and max for each service
  geom_blank(
    data = range_data_1992,
    aes(x = name_long, y = mean)
  ) +
  # Reuse your original reordering scale & color approach
  scale_fill_identity() +
  facet_wrap(~ service, scales = "free") +
  scale_x_reordered() +
  labs(
    title = "Mean Ecosystem Service Values per World Bank Region, 1992",
    x = "Country",
    y = "Mean Value"
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1, size=5)
  )

# Combine the 1992 plots in a row (or multi-row if you prefer).
p_1992
```


```{r ggplot 2020, echo=FALSE, fig.height=8, fig.width=14}

plot_data_2020 <- data_prepped %>% filter(year == 2020)


p_2020 <- ggplot() +
  geom_bar(
    data = plot_data_2020,
    aes(x = name_long, y = mean, fill = color),
    stat = "identity", show.legend = FALSE
  ) +
  geom_blank(
    data = range_data_2020,
    aes(x = name_long, y = mean)
  ) +
  scale_fill_identity() +
  facet_wrap(~ service, scales = "free") +
  scale_x_reordered() +
  labs(
    title = "Mean Ecosystem Service Values per Country, 2020",
    x = "Country",
    y = "Mean Value"
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
p_2020
```



## 5. Generate  Maps

Use ggplot2 and the spatial geometry from our sf object to color each polygon by its statistic of interest. Faceting again by each raster is useful to compare spatial patterns.
```{r create maps, include=FALSE}

# Very important. This line adds the new attribnutes to the polygons. I could add all here.

zonal_wide <- pivot_wider(zonal_df, id_cols=id, names_from=c(service,year), values_from = c(mean, median))

poly <- left_join(poly, zonal_wide, by= "id")
st_write(poly, '/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/Global_ES_mapping/global_poly_means.gpkg') 
# First, merge the zonal_df results back into the polygon sf object.
# This ensures each polygon gets a corresponding mean/median value.

# We need a unique identifier. 
# Assuming 'country' is unique per polygon, we can do a left_join:
map_data <- poly %>%
  left_join(zonal_df, by = "id")

map_1992 <- map_data %>% filter(year == 1992)
services <- unique(map_1992$service)
map_2020 <-  map_data %>% filter(year == 2020)


# 2) Identify unique services
services <- unique(map_data$service)

# 3) For each service, find min/max across both years
service_range <- map_data %>%
  group_by(service) %>%
  summarize(
    min_val = min(mean, na.rm = TRUE),
    max_val = max(mean, na.rm = TRUE),
    unique_color = unique(color)  # we assume 1 color per service
  )

plot_list_1992 <- list()
plot_list_2020 <- list()

for (s in services) {
  # Extract global range (across both years) for service s
  row_s <- filter(service_range, service == s)
  s_min <- row_s$min_val
  s_max <- row_s$max_val
  s_color <- row_s$unique_color  # The single "unique" color for that service
  
  # Subset to year=1992, that service
  df_1992 <- map_data %>%
    filter(service == s, year == 1992)
  
  # Subset to year=2020, that service
  df_2020 <- map_data %>%
    filter(service == s, year == 2020)
  
  # Build the 1992 map
  p_1992 <- ggplot(df_1992) +
    geom_sf(aes(fill = mean), color = NA) +
    scale_fill_gradientn(
      colors   = c("white", s_color),  # White -> "unique_color"
      limits   = c(s_min, s_max),      # Force the same scale min/max
      na.value = "gray40"
    ) +
    labs(
      title = paste0(s, " (1992)"),
      fill  = "Mean"
    ) +
    theme_minimal()
  
  # Build the 2020 map
  p_2020 <- ggplot(df_2020) +
    geom_sf(aes(fill = mean), color = NA) +
    scale_fill_gradientn(
      colors   = c("white", s_color),
      limits   = c(s_min, s_max),
      na.value = "gray40"
    ) +
    labs(
      title = paste0(s, " (2020)"),
      fill  = "Mean"
    ) +
    theme_minimal()
  
  # Add them to the lists
  plot_list_1992[[s]] <- p_1992
  plot_list_2020[[s]] <- p_2020
}
```

```{r echo=FALSE, fig.height=12, fig.width=14}

# Combine the 1992 plots in a row (or multi-row if you prefer).
combined_1992 <- wrap_plots(plot_list_1992, ncol = 3)

# Combine the 2020 plots similarly.
combined_2020 <- wrap_plots(plot_list_2020, ncol = 3)

combined_1992
combined_2020
```

Next steps:  

Run for: Biomes, Continents, income group, fao_reg?

Land cover change -> assess change/area In other words what do i **exactly** need to do here?  What is my target variable? LC change? 

Will have to do it using DiffeR and iterate over all countries, then assemble the dataframes and get percentage of change distributed among the main components. This is not going to be easy but i can get it. However, keep in mind that with all the original classes this becomes unmanageable. Can we produce successive no change maps and check what happens among the change (alloation, swaps and shifts)

Take care of the beneficiaries. 









---
title: "NBS optimization- Yucatan Peninsula"
author: "Jeronimo Rodriguez Escobar"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r prep environment, eval=TRUE, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats', 'foreign',  'exactextractr', 'kableExtra', 'knitr', 'devtools')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)

load_all()

normalize_raster <- function(r) {
  min_val <- min(values(r), na.rm = TRUE)
  max_val <- max(values(r), na.rm = TRUE)
  (r - min_val) / (max_val - min_val)
}

process_intervention_area <- function(raster_list) {
  # Normalize each raster in the list
  normalized_rasters <- lapply(raster_list, normalize_raster)
  # Combine the normalized rasters by summing them
  combined_raster <- do.call(sum, normalized_rasters)
  return(combined_raster)}

```


# 1. Introduction

This report presents a preliminary approach to estimate potential ecosystem service provision gains derived from restoration across five study areas prioritized in WWF's Nature-Based Solutions Originating Platform (NbS-OP)and optimize restoration interventions. 


We evaluated following ecosystem services:
 
- Nitrogen Export 
- Sediment Retention 
- Pollination 
- Coastal Protection  
- Nature Access.

## 1.2 Objectives

- Identify high-value pixels for restoration purposes.
- Estimate potential gains in ecosystem service (ES) provision if a target surface (15.000ha) is restored to a natural vegetation state within the intervention area. 

## 1.3 Theoretical Assumptions


- All ecosystem services are equally valuable.
- Restoration likelihood is distributed uniformly across the pixels prioritized for restoration.
- The total area to restore is known in advance, the exact location of the restoration action within the potential restoration area is not determined yet, as it is contingent on variables such as: The willingness of landowners and managers to participate in the project and the ability to secure control over land during the intended intervention duration.


The use of stratified random sampling ensures that representative of the entire population (i.e., all pixels with restoration potential within the AOI). By assessing the results from multiple repetitions, the overall estimates are less susceptible to the influence of individual sample variations and provide a more accurate representation of the true population values.  
  
## Intervention Area 


The intervention area  is located in the Yucatan Peninsula, and encompasses biodiversity corridors and existing protected areas across key ecosystems including the northern shoreline.


![Yucatan 2020 ESa Land  Cover map (reclassified) and Restoration Priority Areas](/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/WWF_nbs_op/Interventions/Mex_intervention/yucatan_LC1.png)


# 2. Methods
## 2.1 Input Data

-   **Ecosystem Service Data:** Chaplin-Kramer et al. (2022) [Mapping the planetâ€™s critical natural assets](https://www.nature.com/articles/s41559-022-01934-5). Data derived using InVEST, a suite of spatially explicit models that map and quantify nature's contributions to people. InVEST was developed by the Natural Capital Project, a partnership among Stanford University, the University of Minnesota, The Nature Conservancy, and the World Wildlife Fund.

-   **Restoration Potential:** [Adjusted Griscom restoration grided data](https://zenodo.org/records/883444)

Two sets of data were used, the first one representing the baseline for the ecosystem service provision estimated for 2020, and a second one representing  potential increase in ecosystem service provision if an area is restored to its potential natural vegetation, except for urban/built-up areas. The following ecosystem services were considered:


1.  Nitrogen Export. Derived from the Nitrogen retention modeled using the [**InVEST Nutrient Delivery Ratio**](http://data.naturalcapitalproject.org/invest-releases/3.5.0/userguide/ndr.html).
    Expressed in kg/pixel/year

2.  Sediment Retention. Derived using [**InVEST SDR: Sediment Delivery Ration**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/sdr.html).
    Values in ton/pixel/year

3.  Pollination. Derived from [**InVEST SDR: Pollinator Abundance Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/croppollination.html).
    Units represent Polllination Change in people fed on HABitat. More information in
    [**Chaplin-Kramer, et al.
    2022**](https://static-content.springer.com/esm/art%3A10.1038%2Fs41559-022-01934-5/MediaObjects/41559_2022_1934_MOESM1_ESM.pdf)
    
4.  Coastal Protection. Unitless measure, refers to a derived vulnerability index. [**InVEST Coastal Vulnerability Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/coastal_vulnerability.html)

5.  Nature Access represented as [**the number of people within 1 hour travel of natural and semi-natural lands**](https://github.com/springinnovate/distance-to-hab-with-friction) (Chaplin-Kramer et al,
    2022).
    

## 2.2 Area of Interest Definition and Sampling

- Define the area of interest (AOI) within the intervention area extent. This AOI represents the
spatial extent from which the samples will be drawn. In this specific case, the AOI is set to 15.000 ha.


- Calculate the Sample Size: Based on the resolution of the raster and the
desired AOI, the required number of pixels to be sampled is calculated.
This ensures that the total area covered by the sampled pixels
corresponds to the defined AOI.

- Perform repeated stratified random sampling:  Random sampling of pixels within the defined AOI. The sampling process is repeated 30 times (Bootstrapping) to estimate the sampling distribution of a statistic.

Calculate Summary Statistics: For each repetition, the mean, standard
deviation, and 95% confidence intervals are calculated for each band in
the raster dataset. This provides a measure of the central tendency and
variability of the sampled data.

Synthesize Results: The results from all repetitions are combined to
calculate an overall mean and confidence interval for each band. This
provides a more robust estimate of the expected values, effectively
correcting for potential outliers and reducing the influence of
individual sample variations.


The model can be refined by incorporating additional parameters, such as minimum distance to boundaries, distance between points, topography, or inhabited areas. By definition, coastal risk protection only occurs at the coast; and this has to be kept in mind for example by setting sampling weights for the different  services or derived from population density data. This approach provides a distribution of possible values and allows for the calculation of confidence intervals. By synthesizing the results from multiple repetitions, the estimates are less susceptible to individual sample variations.

## 2.3 Processing Environment 

Spatial Raster Processing Tools: The `terra`, `dplyr`, and `sf` packages in **R** were used for analysis. Data visualization was performed using `ggplot2`.

## 3. Results


```{r select ES  data, include=FALSE}
wd <- here()
wd <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/WWF_nbs_op"

# Baseline
tiffes <- file.path(paste(wd, "cropped_raster_data", sep='/'),
  list.files(paste(wd,"cropped_raster_data", sep ='/'),
    pattern = "MEXICO,*\\.tif$"
  )
)
# Select and reorganize tiffes forthe baselines
tiffes_b <- tiffes[c(6,8,11,16,20)] #just keep pollination - AG,
# Reorder because freaking file names #Check this, but this should be easier
tiffes_b <- tiffes_b[c(2,3,5,1,4)]

#Restoration:
tiffes_rest <- tiffes[c(2,15,19,21,27)]
tiffes_rest <- tiffes_rest[c(3,5,4,1,2)]
# set the order of the services here:
serv <- c("Nitrogen Retention", "Sediment Retention", "Pollination", "Coastal Protection", "Nature Access")
```

## Obtain baseline values
```{r obtain baseline output, include=FALSE}

#wd <- here()
wd <- "/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/WWF_nbs_op"
# Load AOI (polygon or raster)
poly <- st_read(paste(wd, "Interventions", "Mexico_int_areas", "AREA_INTERVENCION2_CORREDORES.shp", sep ='/'))
#Reproject to the reference crs if necessary


# LOAD DATA TO ANALYZE # SOME OF THESE BECOEMS THE TEMPLATE FOR THE REST. 
baseES <- lapply(tiffes_b, rast)
# lOAD MASKS 
#1 Restoration Mask (fix direcotries, too complex as is)
griscom <- rast(paste(wd, "Restoration_Griscom", "recMEXICO.tif", sep='/'))
#2 Background mask (fix  this also, too complicated)
#bkg_ee <- rast(paste(wd, "Interventions", "Mex_intervention", "yucatan_bkg.tif", sep= "/"))
#Reprojections

poly <- st_transform(poly, crs=crs(griscom))
griscom <- trim(mask(griscom, poly))

rcl <- matrix(c(
  0, Inf, 0   # Any value from 0 to Infinity becomes 1
), ncol = 3, byrow = TRUE)
# Create background/mask
bkg <- trim(classify(griscom, rcl))
#bkg_proj <- trim(project(bkg, bkg_ee,threads=24))
#trim(bkg_proj)

# Get total area.
exp <- expanse(bkg, unit="ha")

# Aling and mask rasters for the target area.
baseES <- align_rasters(baseES, bkg)
baseES <- lapply(baseES, function(r){
  r <- mask(r,bkg)
})

#Export masked rasters for mapping # improve here. the path is not great, have to do by hand. 
serv. <- gsub(" ", "", serv, fixed = TRUE)
map(1:length(baseES), function(x) writeRaster(baseES[[x]], paste0(wd,"/",'Interventions',"/","Mexico_int_areas","/",  serv.[x], "_msk.tif"), overwrite=TRUE))

baseES <- lapply(baseES, function(r){
  r <- mask(r,griscom, maskvalue=0)
})

# Here i masked the baselines by the Giscom restoration priority pixels, but suddenly i am not sure anymore if this is the right apprach. wha <- t is more important? To know the increase in the total ecosystems service provisoon by the whole region? Whis is nothing. 15.000 ha from 3'626.809 ha are not a lot (0.41%) or if we say that this restoration would be a share of the total 
gmsk <- subst(griscom, from =c(0,1), to = c(NA,0))
exp_g <- expanse(gmsk, unit = "ha")
###################
```


# Summarize (get baseline )
```{r get summaries baseline, include=FALSE}

summary <- lapply(baseES, function(r) {
  list(
    sum = global(r, "sum", na.rm = TRUE),
    mean = global(r, "mean", na.rm = TRUE)
  )
})

summary<- lapply(summary, function(df){
  df <- do.call(cbind,df)
  })
summary <- do.call(rbind, summary)
band <- rownames(summary)  
summary <- cbind(band,summary)
summary <- cbind(serv,summary)
summary <- as_tibble(summary)
summary <- summary %>% rename(Sum = sum) %>% rename(Mean=mean)
```

## Load Restoration layers, align and prepare
```{r apply mask AOI AP, include=FALSE}

baseES_r <- lapply(tiffes_rest, rast)
baseES_r <- align_rasters(baseES_r, bkg) 

# crop mask for the intervention area . This works way better on geographic, not projected crs 
baseES_r <- lapply(baseES_r, function(r){
  r <- crop(r,gmsk)
  #ext(r) <- round(ext(r), 4)
  r <- mask(r,gmsk)
})
```



```{r sampling target areas Yucatan, include=FALSE}
# Convert the serviocsinto a single multilyr for sampling. 
baseES_r <- do.call(c,baseES_r) 
baseES_r <- merge(baseES_r, gmsk)

#Calculate the number of pixels needed for 15,000 hectares
pixel_area <- 300 * 300  # Area of a single pixel in square meters (30m resolution)
hectare_area <- 10000  # Area of one hectare in square meters
pix_tot <- round((15000 * hectare_area) / pixel_area)

# Count total pixels Griscom intervention area
tot <- freq(griscom)
# This is the share o the total potential area that is expected to be restored. 
perc_rest <- pix_tot/tot$count[2]*100

# Number of CPU cores to use
num_cores <- detectCores() - 2  # Leave 1 core free

# Define the number of repetitions
num_repetitions <- 30

# Run parallel sampling
results_list <- mclapply(1:num_repetitions, function(x) {
  sample_and_calculate(raster = baseES_r, pixels_needed = pix_tot, confidence = 0.975)
}, mc.cores = num_cores)

#Add product name
results_list <- lapply(results_list, function(df){
  df <- df %>% mutate(band=rownames(df))
})

# Combine results into a single data frame
final_results <- do.call(rbind, results_list)

# Add repetition index
final_results$repetition <- rep(1:num_repetitions, each = nrow(final_results) / num_repetitions)

final_results <- as_tibble(final_results)

# Add new columns with the service names and units.
final_results <- final_results %>%
  mutate(Service = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Coastal Protection",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "Nature Access",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Retention",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on Hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Retention",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

final_results <- final_results %>%
  mutate(units = case_when(
    Service == "Coastal Protection" ~ "Risk Reduction Index",
    Service == "Nature Access" ~ "People within 1 hour",
    Service == "Nitrogen Retention" ~ "Nitrogen Export (kg/ha/year)",
    Service == "Pollination" ~ "Pollination (equivalent people fed)",
    Service == "Sediment Retention" ~ "Sediment Export (ton/kg/year)",
    # ... add more cases for other Services ...
    TRUE ~ Service  # Keep the original Service name if no match
  ))

final_results <- final_results %>%
  mutate(color = case_when(
    Service == "Coastal Protection" ~ "#9e9ac8",
    Service == "Nature Access" ~ "#A57C00",
    Service == "Nitrogen Retention" ~ "#2c944c",
    Service == "Pollination" ~ "#dd1c77",
    Service == "Sediment Retention" ~ "#08306b",
    # ... add more cases for other Services ...
    TRUE ~ Service  # Keep the original Service name if no match
  ))


#drop the column with the raster name
final_results$band <- NULL
summary$band <- NULL
final_results <- left_join(final_results,summary, join_by(Service == serv))

#Get Percentages!!!

final_results <- final_results %>% mutate(perc_sum= (sum/Sum)*100) %>% mutate(perc_mean=(mean/Mean)*100)

# Get a final Column with the final percentages for services (move the access from mean)
# Add the "Final" column based on the rule
final_results <- final_results %>%
  mutate(Final = case_when(
    Service == "Nature Access" ~ perc_mean,  # If "Service" is "Nature Access", use "perc_mean"
    TRUE ~ perc_sum  # For all other cases, use "perc_sum"
  ))


#save(final_results, file= here("Interventions", "Mex_intervention", "all_res_mex3.RData"))

```
## 3.1 EStimated ES Gain through Restoration

The estimated gains in ecosystem service provision, (%) are synthetized in following table
```{r table outputs, echo=FALSE}
#load(here("Interventions", "Mex_intervention", "all_res_mex3.RData"))


sumy <- final_results %>% group_by(Service) %>% 
  summarise(mean_final = mean(perc_sum, na.rm = TRUE),        # Mean of Final column
    sd_final = sd(perc_sum, na.rm = TRUE),            # Standard deviation
    iqr_final = IQR(perc_sum, na.rm = TRUE),          # Interquartile range (optional)
    .groups = "drop"
  )

final_summary <- final_results %>%
  group_by(Service) %>%
  summarise(
    mean_final = mean(Final, na.rm = TRUE), 
    lower_ci = quantile(Final, probs = 0.025, na.rm = TRUE),  # 2.5th percentile
    upper_ci = quantile(Final, probs = 0.975, na.rm = TRUE),  # 97.5th percentile
    .groups = "drop"
  )

final_summary %>%
  kbl(
    col.names = c("Ecosystem Service", "Mean (%)", "Lower CI (%)", "Upper CI (%)"),
    caption = "Estimated ES Provision Gain in %  with Confidence Intervals",
    align = "c"
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  )

#write.csv(final_summary, file=here("Interventions", "Mex_intervention", "summary_interventionMex3.csv"))
```
These gains are achieved by restoring 15.000ha, or 5.41% of the potential restoration pixels inside the intervention area.
### 3.2 Result Visualization

```{r plot Yucatan outpus, echo=FALSE}

#load(here("Interventions", "Mex_intervention", "all_res_mex2.RData"))

# Generate the ggplot object
plot <- ggplot(final_results, aes(y = perc_sum, fill = color)) +
  geom_boxplot() +
  labs(title = str_wrap("Potential in ES gain for the target intervention area - Yucatan", width = 50), 
       y = "Estimated ES provision gain (%)") +
  theme_bw() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    legend.position = "none"  # Remove legend since color is already mapped
  ) +
  scale_fill_identity() + 
  facet_wrap(~ Service, scales = "free_y", labeller = labeller(Service = label_wrap_gen(width = 10))) +  
  scale_y_continuous(labels = function(x) {
    if (max(x, na.rm = TRUE) > 100000) {
      paste0(format(x / 1000, big.mark = ".", decimal.mark = ","), "k")
    } else {
      format(x, big.mark = ".", decimal.mark = ",")
    }
  }) 
plot
```



```{r synthesis data Surendra, eval=FALSE, include=FALSE}
###############################################################################
# Script:    process_ES_rasters.R
# Author:    Jeronimo Rodriguez E.
# Date:      2025-01-08
# Purpose:   Read and align multiple SpatRasters, convert them to binary masks,
#            merge and sum layers, then project and normalize the result.
#            Finally, write outputs at 30 m and 100 m resolutions (EPSG:32616).
###############################################################################


# -----------------------------------------------------------------------------
# 1. Load and Inspect Data
# -----------------------------------------------------------------------------
# 'tiffes' is a character vector of file paths (all .tif files).
# Each file is read into a SpatRaster and stored in a list.
#serv_1 is the potential resotration already cropped, aligned and stacked for all services
serv_1 <- subst(serv_1, from = 0, to =NA) 


# Apply the function to each layer It is not workin g yet. don't use. 
#top_10. <- select_top_percentile(serv_1, percentile=0.9) 

top_10 <- lapply(serv_1, select_top_10_percent)
 
top_10_2 <- do.call(c,top_10)
writeRaster(top_10_2, here('Interventions', "Mex_intervention", "top_10_mx.tif"), overwrite=TRUE)
top_10 <- lapply(top_10, function(x) ifel(x > 0, 1, NA))
top_10 <- do.call(c,top_10)

writeRaster(top_10, here('Interventions', "Mex_intervention", "top_10_bin_mx.tif"), overwrite=TRUE)

```


# 3.3 Maximum Return Potential

This maps shows, for each class, the pixles wit hthe highest 10% of the gain values

![Restoration Priority area per Service (top 10%)](/Users/rodriguez/Library/CloudStorage/OneDrive-WorldWildlifeFund,Inc/WWF_nbs_op/Interventions/Mex_intervention/Mex_fin.png)

```{r normalize, eval=FALSE, include=FALSE}
#writeRaster(baseES_st, here('Interventions', "Mex_intervention", "stack_normmskd_mx.tif"))
# -----------------------------------------------------------------------------
# 3. Convert to Binary Masks (Values > 0 => 1, Otherwise => 0)
# -----------------------------------------------------------------------------
baseES <- lapply(baseES, function(x) ifel(x > 0, 1, 0))

# Create an additional "background" raster by substituting 1 -> 0 
# in one of the existing layers (baseES[[2]] in this case).
tmp <- subst(baseES[[2]], from = 1, to = 0)

# Combine all SpatRasters into a single multi-layer SpatRaster
baseES <- do.call(c, baseES)

# -----------------------------------------------------------------------------
# 4. Trim and Crop Extent
# -----------------------------------------------------------------------------
# Use the 3rd layer to define a minimal bounding extent, then crop.
ext <- trim(baseES[[3]])             # determines the bounding box
baseES <- crop(baseES, ext)          # crop the entire stack to this extent

# Further crop and mask by 'tmp' (the zero-background)
baseES <- crop(baseES, tmp)
baseES <- mask(baseES, tmp)
tmp <- crop(tmp, baseES)             # ensure 'tmp' matches final extent

# -----------------------------------------------------------------------------
# 5. Additional Classification / Background Handling
# -----------------------------------------------------------------------------
# Remove the 2nd layer from 'baseES' for further processing
tt <- baseES[[-2]]

# Create a classification matrix:
#   0 to Inf => 0
# This might invert or zero out certain areas. (Double-check your logic here!)
rcl <- matrix(c(
  0, Inf, 0
), ncol = 3, byrow = TRUE)

# Apply classification to each layer in 'tt'
tt <- lapply(tt, function(r) classify(r, rcl))

# Merge all layers into one SpatRaster
tt <- do.call(merge, tt)

# Mask the original stack by 'tt'
baseES <- mask(baseES, tt)

# Merge the masked 'baseES' with 'tt'
baseES <- merge(baseES, tt)

# -----------------------------------------------------------------------------
# 6. Sum Layers, Project, and Round
# -----------------------------------------------------------------------------
# Sum all layers in 'baseES'
serv_bra <- app(baseES, sum)

# Reproject to EPSG:32616 at 30 m resolution (multi-threaded if available)
serv_bra <- project(serv_bra, "EPSG:32616", res = 30, threads = 30)

# Round values to integers
serv_bra <- round(serv_bra, digits = 0)

# Write the 30 m result
writeRaster(
  serv_bra,
  paste0(here("Interventions", "Mexico_int_areas"), "/", "ES_occurrence.tif"),
  overwrite = TRUE
)

# -----------------------------------------------------------------------------
# 7. Normalize the 30 m Raster
# -----------------------------------------------------------------------------
# 'normalize_raster()' is assumed to be a custom function that scales
# values (e.g., 0â€“1 or minâ€“max normalization). Make sure it's defined/loaded.
serv_norm <- normalize_raster(serv_bra)

# Write the normalized 30 m output
writeRaster(
  serv_norm,
  paste0(here("Interventions", "Mexico_int_areas"), "/", "ES_occ_norm_30m.tif"),
  overwrite = TRUE
)

# -----------------------------------------------------------------------------
# 8. Resample to 100 m Resolution
# -----------------------------------------------------------------------------
# Create a 100 m SpatRaster template from 'serv_norm'
r100_tmp <- rast(serv_norm)
res(r100_tmp) <- 100  # set desired 100 m resolution

# Resample (near) for categorical or masked data
r100 <- resample(serv_norm, r100_tmp, method = "near")

# Write the normalized 100 m output
writeRaster(
  r100,
  paste0(here("Interventions", "Mexico_int_areas"), "/", "ES_occ_norm_100m.tif"),
  overwrite = TRUE
)

###############################################################################
# End of script
###############################################################################


```


```{r norm, eval=FALSE, include=FALSE}
## 1.4 Normalization of Ecosystem Service Restoration Data 

We normalized the different ES for visualization and simplification purposes 
$$
\text{Normalized Value} = \frac{\text{Raster Value} - \text{Min Value}}{\text{Max Value} - \text{Min Value}}
$$

Where: 
- $\text{Raster Value}$ is the value of a given pixel. 
- $\text{Min Value}$ and $\text{Max Value}$ represent the minimum and maximum observed values across the raster dataset.




```{r normalize 2, eval=FALSE, include=FALSE}
# # ust assign this variable name, so we don't need to edit the whole thing 


serv_bra <- lapply(serv_bra,normalize_raster)
serv_bra <- do.call(c, serv_bra)

writeRaster(serv_bra, paste0(here("restoration_combined"),'/', 'MEX_ES_multiband.tif'), overwrite=T) #Add Access layer before exporting. 
# add mask with the place. 

# load polygon, not just the intevention area. or not?
# run bot hthings 

tmp <- rast(here("Interventions","Mex_intervention", "yucatan_bkg.tif"))
tmp <- trim(tmp)

serv_bra  <- merge(serv_bra , tmp)
    
    # Sum the layers of the merged raster
    serv_bra  <- app(serv_bra, sum)

writeRaster(serv_bra, paste0(here("restoration_combined"),'/', 'MEX_ES_sum2.tif'))

```




  








---
title: "Globale ES Extraction"
output: html_notebook
---

####################################################################### 

Second part with Justin.

# Explore Data.

The ecorregions Raster is not suited, 830 different values. Could be
done but does not make a lot of sense
# 1. Prepare Environemnt (Load necessary libraries)

```{r load libraries, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats', 'foreign')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
```

Key aspect:

Global NCP change maps: country breakdowns -> income groups; geographic regions (IPBES? Or just continents); individual countries that have changed the most
How much land cover change did you get, how much biophysical change did you get, how much service change?
Which specific transitions are service change attributable to? (e.g., forest to cropland vs. forest to grassland)

# Goal
Global NCP change maps: country breakdowns -> income groups; geographic regions (IPBES? Or just continents); individual countries that have changed the most
How much land cover change did you get, how much biophysical change did you get, how much service change?
Which specific transitions are service change attributable to? (e.g., forest to cropland vs. forest to grassland)





```{r explore ecoregions, eval=FALSE, include=FALSE}

ecoregions <- rast(here("Downloaded_data_ES", "Ecoregions2017_compressed_md5_316061.tif"))
ec_uniqe <- unique(ecoregions)
```

Use vector file, just the main biomes.

# Load Data

Countries
```{r filter out PAs}
poly <- st_read(here('natcap_data', 'cartographic_ee_ee_r264_correspondence.gpkg'))
nam <- sort(unique(poly$region_wb))
poly <- poly%>%split(.$region_wb)
nam <- gsub("[[:punct:]]", "", nam) # Remove punctuation
nam <- gsub(" ", "_", nam) # Replace spaces with underscores
```

```{r select data to process}

inpath <- "~/Documents/MN_Applied_Economics/Global_ES_mapping/Downloaded_data" 
outpath <- "~/Documents/MN_Applied_Economics/Global_ES_mapping/region_wb"
# do not include data with a differnet crs here, we will deal with that on the next chunk.

tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))
#Remove points, risk reduction baseline
tiffes <- tiffes[-c(2,4)]
tf <- basename(tiffes)
clean_filename <- function(filename){
  sub("_md5_*", "", filename)
  sub(".tif.*", "", filename)
}
tf <- clean_filename(basename(tf))
```

This runs the same process in batch. Memory was optimized to not load all the data at thesame time, but process and write each raster individually before going to the next one.  



```{r iterate cropMask, eval=FALSE, include=FALSE}
clean_filename <- function(filename){
  sub("_md5.*", "", filename)
  sub(".tif.*", "", filename)
}
# Loop through each geotiff file
for (i in seq_along(tiffes)) {
  # Get the current tiff file
  tiff_file <- tiffes[i]
  
  # Load the current geotiff (don't load all at once)
  access <- rast(tiff_file)
  
  # Apply crop and mask operations to each polygon
  cropped <- map(1:length(poly), function(x) crop(access, poly[[x]]))
  masked <- map(1:length(cropped), function(x) mask(cropped[[x]], poly[[x]]))
  
  # Extract the base filename before 'md5'
  base_filename <- clean_filename(basename(tiff_file))
  
  # Write the cropped and masked rasters to files
  map(1:length(masked), function(x) {
    writeRaster(masked[[x]], paste0(outpath,"/", base_filename, "_", nam[x], ".tif"), overwrite = TRUE)
  })
  # Optionally, clear the variable to free up memory
  rm(access, cropped, masked)
  gc()  # Garbage collection to clear memory
}
```


# By Income level
# Region Wb
```{r cropmask}
rs <- lapply(tiffes, rast) 
# Pending to add: run this in parallel!
# Apply cropping and masking recursively using lapply and map

num_cores <- 6
# Parallelize the outer loop using mclapply
rs_msk <- mclapply(rs, function(raster) {
  # Use lapply for the inner loop
  lapply(poly, function(polygon) {
    # Crop and mask the raster
    cropped_raster <- crop(raster, polygon)
    masked_raster <- mask(cropped_raster, polygon)
    return(masked_raster)
  })
}, mc.cores = num_cores) 

# Use lapply to iterate over each list in nested_list
lapply(seq_along(rs_msk), function(i) {
  # For each outer list, iterate through the inner list (rasters)
  lapply(seq_along(rs_msk[[i]]), function(j) {
    
    # Construct the filename using vector_1 and vector_2
    file_name <- paste0(outpath, tf[[i]], '_', nam[[j]], '.tif')
    
    # Write the raster to the file
    writeRaster(rs_msk[[i]][[j]], file_name, overwrite = TRUE)
  })
})
```


```{r}
summary_df <- map2_dfr(seq_along(rs_msk), rs_msk, function(i, inner_list) {
  
  # i corresponds to the index of the outer list (e.g., 1, 2, 3...)
  outer_label <- paste0("Outer_", i)  # Label for the outer list
  
  # Loop through each SpatRaster in the inner list
  map_dfr(inner_list, function(raster) {
    extract_raster_summary(raster, outer_label)
  })
})


for (i in seq_along(rs_msk)) {
  sublist <- rs_msk[[i]]  # Access each sublist
  
  for (j in seq_along(sublist)) {
    raster <- sublist[[j]]  # Access each SpatRaster
    raster_name <- names(raster)  # Get the name slot of the SpatRaster
    
    # Build the output filename
    output_filename <- paste0(outpath, '/', raster_name, "_", nam[j], ".tif")
    
    # Save the raster to a .tif file
    writeRaster(raster, filename = output_filename, overwrite = TRUE)
  }
}


```
ok I need you help drafting an email to my dissertation committee that I really struggle with 
# Split by Countries/continents


# mask by protected areas (all togeter, chategories I-VI)

# Reclassify Land Cover maps.

# Change Maps

```{r change_map_calc}
esat <- c('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2020-v2.1.1_md5_2ed6285e6f8ec1e7e0b75309cc6d6f9f.tif','/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992-v2.0.7cds_compressed_md5_60cf30.tif')


esas <- lapply(esat,rast)


nam <- c(2020,1992)

map(1:2, function(x) writeRaster(esas[[x]], paste0('ESA_LC_', nam[x],'.tif')))
cls <- lapply(esas,freq)


wd <- '~/Documents/Natural_capital/NBS_OP/data/ESA_LC'

tf <- file.path(wd, list.files(wd, pattern='tif'))

tf <- tf[c(2,3)]

esas <- lapply(tf,rast)



countries <- st_read('/Users/sputnik/Library/CloudStorage/OneDrive-TempleUniversity/personal files/PosDoc/DataOBS_op/global_products/cartographic_ee_ee_r264_correspondence.gpkg')

cont <- countries %>% filter(subregion=='Central America')

# Define the reclassification rules in one operation
reclass_table <- data.frame(
  from = c(10, 11, 12, 20, 30, 40, 50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100,
           110, 120, 121, 122, 130, 140, 150, 151, 152, 153, 160, 170, 180,
           190, 200, 201, 202, 210, 220),
  to = c(1, 1, 1, 1, 1, 1,   # Cultivated
         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,  # Forests
         3, 3, 3, 3, 3,   # Grasses and Shrubs
         4, 4, 4, 4, 4,   # Sparse Vegetation
         5, 5, 5,   # Mangroves
         6,   # Urban
         7, 7, 7,   # Bare
         8,   # Water
         9)   # Ice
)

esas <- lapply(esas, function(r) {
  subst(r, from = reclass_table$from, to = reclass_table$to)
})


map(1:2, function(x) writeRaster(esas[[x]], paste0('~/Documents/Natural_capital/NBS_OP/data/ESA_LC/', "Forests_esa", year[x], '.tif')))

# Extract forests
esas <- lapply(esas, function(r) {
  subst(r, from = 2, to =1, others=NA)
})


chg <- ch_mapR(esas[[1]],esas[[2]])
freq(chg)

mat_1 <- crosstabm(esas[[1]],esas[[2]])

diffs_p <-  diffTablej(mat, digits=3, analysis = "change")


```
git remote set-url origin git@github.com:springinnovate/nbs_op.git
# Get Change Maps 

```{r calculate 
change maps}

inpath <- '~/Documents/MN_Applied_Economics/Global_ES_mapping/ESA_LC'
tiffes <- file.path(here(inpath), list.files(paste0(here(inpath)),pattern= '.tif$'))

es <- lapply(tiffes, rast)
ch <- ch_mapR(es)
```
# Get Sankey Diagrams

```{r Sankey Diagram}
classes <- c("Cultivated", "Forests", "Grasses/shrubs", "Sparse", "Mangroves", "Urban", "Bare", "Water")
groupColor <- c("#dcf064","#00b809","#8ca000","#ffebaf",'#009678', "#c31400", "#fff5d7", "#0046c8")

years <- c(1992,2020)


nodeInfo <- nodeInfoR(years, classes, groupColor)

NodeCols<- sort(unique(nodeInfo$nodeCol))

#create paths to the rasters/bands
num_bands <- length(NodeCols)
tf <- list.files(wd, pattern= "Rec")
tf <- file.path(wd,tf)
fileInfo <- tibble(
  nodeCol = seq_along(tf),
  rast = tf,
  rasterBand = 1
)

# join path strings to nodeInfo
nodeInfo <- dplyr::left_join(nodeInfo, fileInfo, by='nodeCol') 
linkInfo <- linkInfoR(NodeCols, nodeInfo)


fontSize <- 0.5
nodeWidth <-30
fontFamily <- "sans-serif"
colorScaleJS <- sprintf("d3.scaleOrdinal().domain(%s).range(%s)",
                        jsonlite::toJSON(unique(nodeInfo$nodeGroup)),
                        jsonlite::toJSON(unique(nodeInfo$groupColor)))


sankeyNetwork(Links = linkInfo, Nodes = nodeInfo,
              Source = "source",
              Target = "target",
              Value = "value",
              NodeID = "nodeName",
              NodeGroup = "nodeGroup",
              LinkGroup = "LinkGroup",
              fontSize = fontSize,
              fontFamily = fontFamily,
              nodePadding = 10,
              margin=1,
              nodeWidth = nodeWidth,
              colourScale = JS(colorScaleJS))
```

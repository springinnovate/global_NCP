---
title: "Preliminary Intervention Assessment NBS for the state of Espírito Santo, Brazil"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

# 1. Introduction

We present a preliminary approach to estimate potential ecosystem service gains derived from 
restoration in the state of **Espírito Santo (Brazil)** with a spatial resolution of 30 meters.

## 1.2 Objectives

1.  Identify high-value pixels for restoration purposes.
2.  Estimate potential ecosystem service (ES) provision gains if a given/target surface (preliminarily 30.200 ha) are restored to natural vegetation within the State of Espirito Santo.

## 1.3 Theoretical Assumptions

-   All ecosystem services are equally valuable.
-   Restoration likelihood is distributed uniformly across the intervention areas.
-   Confidence intervals provide robust estimates under random
    distribution scenarios. -we need a better way to say this  but i have not been able to find the suited words, but basically it means thatat this point we only know that the are to restore is 30.200 ha, but we still don't know exactly where they are going to be located, as it depends on land availability (which depends on factoe like willingess to take take part on the project by owners and land managers, and the capability ot secure control over land during the duration of the project) help me adjust this here please.

------------------------------------------------------------------------

# 2. Materials and Methods

## 2.1 Study Area

This analysis encompasses the totality of the State of Espírito Santo State, on the Brazilian Atlantic coast.  
**Note** I still need to learn more about  the State of Espirito Santo ,  and can't say much about it, but a more detailed detailed contextualziation of the region, including geographic, environmental, demographic and economic aspects and how it integrates into the broader Symbiosis prokects can be included here.

For this exercise, the total area to restore is 30.200 ha

------------------------------------------------------------------------

## 2.2 Input Data

-   **Ecosystem Service Data:** Chaplin-Kramer et al. (2022) [Mapping the planet’s critical natural assets](https://www.nature.com/articles/s41559-022-01934-5), derived using InVEST. InVEST produces spatially explicit models to map and quantify nature’s contributions to people. Developed by the Natural Capital Project—a partnership among Stanford University, the University of Minnesota, The Nature Conservancy, and the World Wildlife Fund. 

-   **Restoration Potential:** [Adjusted Griscom restoration grided data](98. 


We used  spatial grids (raster data in GeoTIFF format) for which the pixel values reflects the estimated gain—measured in units of the specified ecosystem service—represent the amount of the ecosystem service provision gain if it is restored to its potential natural vegetation except for urban/built-up for following  variables:


1.  Coastal Protection. Unitless measure, refers to a derived vulnerability index. [**InVEST Coastal Vulnerability Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/coastal_vulnerability.html)

2.  Nitrogen Export. Derived from the Nitrogen retention modeled using the [**InVEST Nutrient Delivery Ratio**](http://data.naturalcapitalproject.org/invest-releases/3.5.0/userguide/ndr.html).
    Expressed in kg/pixel/year

3.  Sediment Retention. Derived using [**InVEST SDR: Sediment Delivery Ration**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/sdr.html).
    Values in ton/pixel/year

4.  Pollination. Derived from [**InVEST SDR: Pollinator Abundance Model**](https://storage.googleapis.com/releases.naturalcapitalproject.org/invest-userguide/latest/en/croppollination.html).
    Units represent Polllination Change in people fed on HABitat. More information in
    [**Chaplin-Kramer, et al.
    2022**](https://static-content.springer.com/esm/art%3A10.1038%2Fs41559-022-01934-5/MediaObjects/41559_2022_1934_MOESM1_ESM.pdf)

5.  Nature Access represented as [**the number of people within 1 hour travel of natural and semi-natural lands**](https://github.com/springinnovate/distance-to-hab-with-friction) (Chaplin-Kramer et al,
    2022).


## 2.3 Processing Environment 

-   **Spatial Raster Processing Tools:** `terra`, `dplyr`, and `sf` packages in R, visualization using `ggplot2`.


### 2.4 Normalization of Ecosystem Service Restoration Data 

The raster bands where nornmalized following using theis formula to bring them to a common standard:
$$
\text{Normalized Value} = \frac{\text{Raster Value} - \text{Min Value}}{\text{Max Value} - \text{Min Value}}
$$

Where: - $\text{Raster Value}$ is the value of a given pixel. -
$\text{Min Value}$ and $\text{Max Value}$ represent the minimum and maximum observed values across the raster dataset.

This step allows to aggregate services with different units (e.g., tons, kilograms, number of people, unitless) can be aggregated limiting bias toward any single
service.

Once normalized, the ecosystem service raster datasets are added to create a synthesis raster that represents the aggregated value of all services. This approach assumes equal importance of all ecosystem services, meaning no weighting was applied at this stage. **Note**. The next iteration will include population-based weights. 


# 2.5 Potential  ES gain estimation   

WWe run a stratified random sampling to estimate the the total amount in ES service provision gains given a total intervention area, in this case 30.200 ha. 
Following aspects are considered :

-   Spatial configuration effects ware incorporasted during the modeling of the ecosystem service gainlayers. 
-   The model can be refined by incorporating additional
    parameters (e.g minimum distance to boundaries,distance between 
    points, topography or inhabited areas)
-   Coastal risk protection only occurs at the coast, any random
    sampling performed will haveto consider this. This can be adjusted
    setting weights for the services or using population density data.

To reduce potential sampling bias and improve the
accuracy of the estimates, the random sampling process is repeated  at least 30  times . This  bootstrapping, approach, where repeated sampling with replacement is
used to estimate the sampling distribution of a statistic, in this case total ES gain for following ES.

Calculate Summary Statistics: For each repetition, the mean, standard
deviation, and 95% confidence intervals are calculated for each band in
the raster dataset. This provides a measure of the central tendency and
variability of the sampled data.

Synthesize Results: The results from all repetitions are combined to
calculate an overall mean and confidence interval for each band. This
provides a more robust estimate of the expected values, effectively
correcting for potential outliers and reducing the influence of
individual sample variations.

Theoretical Justification

The use of stratified random sampling ensures that
the sample is representative of the entire population (i.e., all pixels
within the AOI). By dividing the population into strata and sampling
from each stratum, this method reduces the variability of the estimates
compared to simple random sampling. This is particularly important when
dealing with spatial data, where there may be inherent spatial
autocorrelation or heterogeneity.

The repeated sampling approach further enhances the robustness of the
estimates by providing a distribution of possible values. This allows
for the calculation of confidence intervals, which provide a measure of
the uncertainty associated with the estimates. By synthesizing the
results from multiple repetitions, the overall estimates are less
susceptible to the influence of individual sample variations and provide
a more accurate representation of the true population values.



```{r prep environment, eval=TRUE, include=FALSE}
packs <- c('terra', 'purrr', 'landscapemetrics', 'sf','dplyr',
           'here', 'gdalUtilities', 'jsonlite', 'devtools', 'stringr',
           'parallel', 'dplyr', 'tidyr', 'ggplot2', 'janitor', 'forcats', 'foreign')
sapply(packs, require, character.only = TRUE, quietly=TRUE)
rm(packs)
align_rasters <- function(raster_list, template, resample_method = "bilinear") {
  lapply(raster_list, function(r) {
    if (!compareGeom(r, template, stopOnError = FALSE)) {
      message("Aligning raster: ", names(r))
      # Resample to align with the template
      resample(r, template, method = resample_method)
    } else {
      # Return raster as is if already aligned
      message("Raster already aligned: ", names(r))
      r
    }
  })
}

normalize_raster <- function(r) {
  min_val <- min(values(r), na.rm = TRUE)
  max_val <- max(values(r), na.rm = TRUE)
  (r - min_val) / (max_val - min_val)
}

process_intervention_area <- function(raster_list) {
  # Normalize each raster in the list
  normalized_rasters <- lapply(raster_list, normalize_raster)
  # Combine the normalized rasters by summing them
  combined_raster <- do.call(sum, normalized_rasters)
  return(combined_raster)
}
```

### 3.2.3 Prepare Templates to Align the Data

To perform the analysis, all the raster datasets need to be brought to the same spatial resolution, crs and aligned  to the same origin. We used the global ESA Land Cover Classification Raster for 2020 (CRS: WGS-84)

```{r create tmp, eval=FALSE, include=FALSE}
library(here)
# add backgrounds/templates to align 
path_lc <- here('ESA_LC') 
# load reclassified land cover map
tf <- file.path(path_lc, list.files(path_lc, pattern= "Rec"))
lc <- lapply(tf,rast)
lc <- lc[[1]]
# create rcl matrix
rcl <- matrix(c(
  0, Inf, 0   # Any value greater than 0 becomes 1
), ncol = 3, byrow = TRUE)

#create background pixels, subsitute all by 0
tmp <- lapply(lc, function(r){
  r <- classify(r[[1]], rcl)
})
tmp <- tmp[[1]]
rm(lc)
```

------------------------------------------------------------------------

# 4 Processing

## 4.1 List the target layers

```{r select ES diff  data}
#Restoration:

tiffes <- file.path(
  here("cropped_raster_data"),
  list.files(
    here("cropped_raster_data"),
    pattern = "BRAZIL.*\\.tif$"
  )
)
tiffes <- tiffes[c(1,11,15,18,22)] #just keep pollination - HAB,
tiffes <- tiffes[-2]
```

## 4.2 Normalize and combine the ES provision gains into a single raster 


The combined raster is a proxy for the total ES provided, assuming that all services are equally valuable, to identify the distribution of ecosystem sercie gains. 
On asecond run, we extracted pixels for 30.200 ha that have the highestaggregated value, running an optimization. 
It does not include Nature Access, as the pixel values repesent the amount of people that gain access to each individual pixel, which would result in counting the same people multiple times. I am dealing with that now. 

```{r apply mask AOI AP, eval=FALSE, include=FALSE}

# Step 1: Extract file names, product names, and country names
file_names <- basename(tiffes)  # Extract file names from paths

# Extract product names and country names
product_names <- sub("_[A-Z]+\\.tif$", "", file_names)  # Remove "_COUNTRY.tif" to get product name
country_names <- sub(".*_(.*?)\\.tif$", "\\1", file_names)  # Extract country name from file name

# Step 2: Create a dataframe to organize the information
file_info <- data.frame(
  FilePath = tiffes,
  Product = product_names,
  Country = country_names,
  stringsAsFactors = FALSE
)

# Step 3: Sort the dataframe by Country first, then by Product
file_info <- file_info %>%
  arrange(Country, Product)

# Step 4: Extract the sorted file paths
tiffes <- file_info$FilePath

baseES <- lapply(tiffes, rast)


tmp <- crop(tmp, baseES[[1]])
 

baseES <- lapply(baseES, function(r) {
  if (!compareGeom(r, tmp, stopOnError = FALSE)) {
    message("Aligning raster: ", names(r))
    resample(r, tmp, method = "bilinear")
    } else {
      message("Raster already aligned: ", names(r))
      r
      }
    })

poly <- st_read(here("Interventions", "Brazil_int_areas", "Espirito_Santo_Albers.shp"))
pol_wgs <- st_transform(poly, crs=crs(baseES[[1]]))
serv_bra <- lapply(baseES, function(r){
  r <- crop(r,pol_wgs)
  r <- mask(r, pol_wgs)
  }) 

serv_bra <- lapply(serv_bra,normalize_raster)
serv_bra <- do.call(c, serv_bra)

tmp <- mask(tmp, pol_wgs)
tmp <- trim(tmp)

serv_bra  <- merge(serv_bra , tmp)
    
    # Sum the layers of the merged raster
    serv_bra  <- app(serv_bra, sum)

writeRaster(serv_bra, paste0(here("restoration_combined"),'/', 'BRA_ES_sum2.tif'))
```



## 4.3 Run Sampling and Syntetize results - Espirito Santo

Finally, we are sampled  the data for obtain estimate of the expected
restoration gains assuming randomly selected pixels extracted form the
potential restoration areas. Again, some assumptions will have to be
reviewed, but this is an initial assessment. The targeted intervention
area is 30.200 ha.

```{r sampling target areas brazil, eval=FALSE, include=FALSE}
serv_1 <- rast(here("Interventions", "Brazil_intervention",'/', 'serv_BRAZIL.tif'))
rest_m <- rast(here('Interventions', 'Brazil_intervention', 'SS_griscom.tif'))
rcl <- matrix(c(
  -Inf, 0, 0,  # Any value from -Infinity to 0 remains 0
  0, Inf, 1   # Any value from 0 to Infinity becomes 1
), ncol = 3, byrow = TRUE)
rest_m <- classify(rest_m, rcl)

rest_m <-project(rest_m, serv_1)
rest_m <- terra::resample(rest_m, serv_1)
serv_1 <- mask(serv_1,rest_m, maskvalues=0)
#Calculate the number of pixels needed for 30,000 hectares
pixel_area <- 30 * 30  # Area of a single pixel in square meters (30m resolution)
hectare_area <- 10000  # Area of one hectare in square meters
pixels_needed <- round((30200 * hectare_area) / pixel_area)

# Number of repetitions
n_repetitions <- 30
# Function to perform the sampling and calculations
sample_and_calculate <- function(i, raster, pixels_needed) {
  # Sample pixels and directly extract values (without na.rm)
  sample_values <- spatSample(raster, size = pixels_needed, 
                              method = "random", 
                              na.rm = FALSE,  
                              as.points = FALSE, 
                              xy = FALSE,
                              values = TRUE) 

  # Remove rows where ALL values are NA
  sample_values <- sample_values[rowSums(is.na(sample_values)) != ncol(sample_values), ]

  # If not enough samples after removing NAs, resample
  if (nrow(sample_values) < pixels_needed) {
    sample_values <- rbind(
      sample_values,
      spatSample(raster, size = pixels_needed - nrow(sample_values),
                 method = "random", na.rm = TRUE, 
                 as.points = FALSE, xy = FALSE, values = TRUE)
    )
  }

  # Calculate summary statistics
  band_stats <- apply(sample_values, 2, function(x) { 
    mean_val <- mean(x)
    sd_val <- sd(x)
    n_val <- length(x)
    se_val <- sd_val / sqrt(n_val)
    margin_error <- qt(0.975, df = n_val - 1) * se_val
    lower_ci <- mean_val - margin_error
    upper_ci <- mean_val + margin_error
    return(c(mean = mean_val, lower_ci = lower_ci, upper_ci = upper_ci))
  })

  band_stats_df <- as.data.frame(t(band_stats))
  band_stats_df$repetition <- i

  # Calculate sum of pixel values for each band
  band_sums <- colSums(sample_values) 
  band_stats_df$sum <- band_sums

  return(band_stats_df)
}

# Using mclapply (parallel processing)
num_cores <- 15 
results_list <- mclapply(1:n_repetitions, sample_and_calculate, 
                         raster = serv_1, 
                         pixels_needed = pixels_needed,
                         mc.cores = num_cores) 

results_list <- lapply(results_list, function(df){
  df <- df %>% mutate(band=rownames(df))
})
# Combine all results into a single data frame
all_results <- as_tibble(do.call(rbind, results_list))


# Add new columns with the service names and units.
all_results <- all_results %>%
  mutate(Service = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Coastal Protection",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "Nature Access",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Export",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on Hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Export",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

all_results <- all_results %>%
  mutate(units = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "Risk Reduction Index",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "People within 1 hour",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "Nitrogen Export (kg/ha/year)",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "Pollination (equivalent people fed)",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "Pollination (people fed on hab)",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "Sediment Export (ton/kg/year)",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))

all_results <- all_results %>%
  mutate(color = case_when(
    band == "cv_habitat_value_Sc3v1-ESAmod2_v2_md5_64082b" ~ "#7a0177",
    band == "nature_access_diff_Sc3v1_PNVnoag-esa2020" ~ "#A57C00",
    band == "nitrogen_ESAmod2-Sc3v1_md5_024a36" ~ "#2c944c",
    band == "pollination_ppl_fed_on_ag_10s_Sc3v1_PNVnoag-esa2020_md5_405c88" ~ "#dd1c77",
    band == "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790" ~ "#dd1b56",
    band == "sediment_ESAmod2-Sc3v1_md5_149078" ~ "#08306b",
    # ... add more cases for other bands ...
    TRUE ~ band  # Keep the original band name if no match
  ))
save(all_results, file= here("Interventions", "Brazil_intervention", "all_res_bra.RData"))
```

### 6.1.3 Plot Results Espirito Santo

```{r plot brazil outpus, echo=FALSE}
load(here("Interventions", "Brazil_intervention", "all_res_bra.RData"))
df <-all_results %>%  filter(band!="nature_access_diff_Sc3v1_PNVnoag-esa2020") %>% filter(band!= "pollination_ppl_fed_on_hab_Sc3v1_PNV_no_ag-ESA_md5_576790")
# Assuming your 'all_results' data frame has the columns 'Service', 'units', and 'color'
# Assuming your 'all_results' data frame has the columns 'Service', 'units', and 'color'

# Generate the ggplot object
plot <- ggplot(df, aes(y = sum, fill = color)) +
  geom_boxplot() +
  labs(title = str_wrap("Total estimated service change in units for the target intervention area - Espirito Santo", width = 50), 
       y = "Total Sum Value") +
  theme_bw() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    strip.background = element_blank(),
    strip.text = element_text(face = "bold"),
    legend.position = "none"  # Remove legend since color is already mapped
  ) +
  scale_fill_identity() + 
  facet_wrap(~ Service, scales = "free_y", labeller = labeller(Service = label_wrap_gen(width = 10))) +  
  scale_y_continuous(labels = function(x) {
    if (max(x, na.rm = TRUE) > 100000) {
      paste0(format(x / 1000, big.mark = ".", decimal.mark = ","), "k")
    } else {
      format(x, big.mark = ".", decimal.mark = ",")
    }
  }) 
plot
```

------------------------------------------------------------------------


References

Cochran, W. G. (1977). Sampling techniques (3rd ed.). John Wiley & Sons.
Lohr, S. L. (2010). Sampling: Design and analysis (2nd ed.).
Brooks/Cole. 1\
1. Bivand, R. S., Pebesma, E. J., & Gómez-Rubio, V. (2013). Applied
spatial data analysis with R (2nd ed.). Springer.

